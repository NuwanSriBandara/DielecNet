{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVEVLyrwZeUX"
      },
      "source": [
        "# Arrangement\n",
        "\n",
        "1) Only with experimental (ie real) data \\\\\n",
        "2) Only with simulation data \\\\\n",
        "3) With all data \\\\\n",
        "4) Transfer learning from simulation to experimental models and vice versa \\\\\n",
        "5) All possible methods? few-shot learning?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZBCAiadPLN9",
        "outputId": "e8aa2ea0-dbd1-4025-ceb2-291f70a1b917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbvCaWKKeSSf"
      },
      "source": [
        "#Supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDKkcCkG4Y8o"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EBnkY1D4knf"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "epochs = 500\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lcw8qrE_2ZV4"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "mat_data = scipy.io.loadmat('/content/drive/MyDrive/IEEE EMBS SMP 2023/datanew.mat') # Martina: Pls add the path of the dataset file here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQLwq7DzwL7Y",
        "outputId": "910be6ee-560e-4d74-dde5-bc5ae04cab6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2,  8,  7],\n",
              "       [ 2, 10,  7],\n",
              "       [ 2, 20,  7],\n",
              "       [ 2, 30,  7],\n",
              "       [ 2, 40,  7],\n",
              "       [ 2, 50,  7],\n",
              "       [ 2, 60,  7],\n",
              "       [ 2, 70,  7],\n",
              "       [ 3, 10,  7],\n",
              "       [ 3, 20,  7],\n",
              "       [ 3, 30,  7],\n",
              "       [ 3, 40,  7],\n",
              "       [ 3, 50,  7],\n",
              "       [ 3, 60,  7],\n",
              "       [ 3, 70,  7],\n",
              "       [ 4, 10,  7],\n",
              "       [ 4, 20,  7],\n",
              "       [ 4, 30,  7],\n",
              "       [ 4, 40,  7],\n",
              "       [ 4, 50,  7],\n",
              "       [ 4, 60,  7],\n",
              "       [ 4, 70,  7],\n",
              "       [ 2, 10,  8],\n",
              "       [ 2, 20,  8],\n",
              "       [ 2, 30,  8],\n",
              "       [ 2, 40,  8],\n",
              "       [ 2, 50,  8],\n",
              "       [ 2, 60,  8],\n",
              "       [ 2, 70,  8],\n",
              "       [ 3, 10,  8],\n",
              "       [ 3, 20,  8],\n",
              "       [ 3, 30,  8],\n",
              "       [ 3, 40,  8],\n",
              "       [ 3, 50,  8],\n",
              "       [ 3, 60,  8],\n",
              "       [ 3, 70,  8],\n",
              "       [ 4, 10,  8],\n",
              "       [ 4, 20,  8],\n",
              "       [ 4, 30,  8],\n",
              "       [ 4, 40,  8],\n",
              "       [ 4, 50,  8],\n",
              "       [ 4, 60,  8],\n",
              "       [ 4, 70,  8],\n",
              "       [ 2, 10,  9],\n",
              "       [ 2, 20,  9],\n",
              "       [ 2, 30,  9],\n",
              "       [ 2, 40,  9],\n",
              "       [ 2, 50,  9],\n",
              "       [ 2, 60,  9],\n",
              "       [ 2, 70,  9],\n",
              "       [ 3, 10,  9],\n",
              "       [ 3, 20,  9],\n",
              "       [ 3, 30,  9],\n",
              "       [ 3, 40,  9],\n",
              "       [ 3, 50,  9],\n",
              "       [ 3, 60,  9],\n",
              "       [ 3, 70,  9],\n",
              "       [ 4, 10,  9],\n",
              "       [ 4, 20,  9],\n",
              "       [ 4, 30,  9],\n",
              "       [ 4, 40,  9],\n",
              "       [ 4, 50,  9],\n",
              "       [ 4, 60,  9],\n",
              "       [ 4, 70,  9],\n",
              "       [ 2, 10, 10],\n",
              "       [ 2, 20, 10],\n",
              "       [ 2, 30, 10],\n",
              "       [ 2, 40, 10],\n",
              "       [ 2, 50, 10],\n",
              "       [ 2, 60, 10],\n",
              "       [ 2, 70, 10],\n",
              "       [ 3, 10, 10],\n",
              "       [ 3, 20, 10],\n",
              "       [ 3, 30, 10],\n",
              "       [ 3, 40, 10],\n",
              "       [ 3, 50, 10],\n",
              "       [ 3, 60, 10],\n",
              "       [ 3, 70, 10],\n",
              "       [ 4, 10, 10],\n",
              "       [ 4, 20, 10],\n",
              "       [ 4, 30, 10],\n",
              "       [ 4, 40, 10],\n",
              "       [ 4, 50, 10],\n",
              "       [ 4, 60, 10],\n",
              "       [ 4, 70, 10],\n",
              "       [ 2, 10, 11],\n",
              "       [ 2, 20, 11],\n",
              "       [ 2, 30, 11],\n",
              "       [ 2, 40, 11],\n",
              "       [ 2, 50, 11],\n",
              "       [ 2, 60, 11],\n",
              "       [ 2, 70, 11],\n",
              "       [ 3, 10, 11],\n",
              "       [ 3, 20, 11],\n",
              "       [ 3, 30, 11],\n",
              "       [ 3, 40, 11],\n",
              "       [ 3, 50, 11],\n",
              "       [ 3, 60, 11],\n",
              "       [ 3, 70, 11],\n",
              "       [ 4, 10, 11],\n",
              "       [ 4, 20, 11],\n",
              "       [ 4, 30, 11],\n",
              "       [ 4, 40, 11],\n",
              "       [ 4, 50, 11],\n",
              "       [ 4, 60, 11],\n",
              "       [ 4, 70, 11],\n",
              "       [ 2, 10, 12],\n",
              "       [ 2, 20, 12],\n",
              "       [ 2, 30, 12],\n",
              "       [ 2, 40, 12],\n",
              "       [ 2, 50, 12],\n",
              "       [ 2, 60, 12],\n",
              "       [ 2, 70, 12],\n",
              "       [ 3, 10, 12],\n",
              "       [ 3, 20, 12],\n",
              "       [ 3, 30, 12],\n",
              "       [ 3, 40, 12],\n",
              "       [ 3, 50, 12],\n",
              "       [ 3, 60, 12],\n",
              "       [ 3, 70, 12],\n",
              "       [ 4, 10, 12],\n",
              "       [ 4, 20, 12],\n",
              "       [ 4, 30, 12],\n",
              "       [ 4, 40, 12],\n",
              "       [ 4, 50, 12],\n",
              "       [ 4, 60, 12],\n",
              "       [ 4, 70, 12],\n",
              "       [ 2, 10, 13],\n",
              "       [ 2, 20, 13],\n",
              "       [ 2, 30, 13],\n",
              "       [ 2, 40, 13],\n",
              "       [ 2, 50, 13],\n",
              "       [ 2, 60, 13],\n",
              "       [ 2, 70, 13],\n",
              "       [ 3, 10, 13],\n",
              "       [ 3, 20, 13],\n",
              "       [ 3, 30, 13],\n",
              "       [ 3, 40, 13],\n",
              "       [ 3, 50, 13],\n",
              "       [ 3, 60, 13],\n",
              "       [ 3, 70, 13],\n",
              "       [ 4, 10, 13],\n",
              "       [ 4, 20, 13],\n",
              "       [ 4, 30, 13],\n",
              "       [ 4, 40, 13],\n",
              "       [ 4, 50, 13],\n",
              "       [ 4, 60, 13],\n",
              "       [ 4, 70, 13],\n",
              "       [ 2, 10, 14],\n",
              "       [ 2, 20, 14],\n",
              "       [ 2, 30, 14],\n",
              "       [ 2, 40, 14],\n",
              "       [ 2, 50, 14],\n",
              "       [ 2, 60, 14],\n",
              "       [ 2, 70, 14],\n",
              "       [ 3, 10, 14],\n",
              "       [ 3, 20, 14],\n",
              "       [ 3, 30, 14],\n",
              "       [ 3, 40, 14],\n",
              "       [ 3, 50, 14],\n",
              "       [ 3, 60, 14],\n",
              "       [ 3, 70, 14],\n",
              "       [ 4, 10, 14],\n",
              "       [ 4, 20, 14],\n",
              "       [ 4, 30, 14],\n",
              "       [ 4, 40, 14],\n",
              "       [ 4, 50, 14],\n",
              "       [ 4, 60, 14],\n",
              "       [ 4, 70, 14]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "mat_data['datanew'][0][0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA0Ms7oPSO-E",
        "outputId": "4716a29c-5700-4598-ba16-db27ad9f202f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(169, 3)\n"
          ]
        }
      ],
      "source": [
        "# Print the content\n",
        "print(mat_data['datanew'][0][0][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPJmCPGciMxV",
        "outputId": "372551ed-98b3-4188-9ac0-e831934aaa87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 8 7]\n",
            "[ 2 10  7]\n",
            "[ 2 20  7]\n",
            "[ 2 30  7]\n",
            "[ 2 40  7]\n",
            "[ 2 50  7]\n",
            "[ 2 60  7]\n",
            "[ 2 70  7]\n",
            "[ 3 10  7]\n",
            "[ 3 20  7]\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print(mat_data['datanew'][0][0][0][i, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfqlmUIP02rd",
        "outputId": "322dca88-ff5f-4cdf-bdbd-be4ce1aac002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2, 1001, 169)\n"
          ]
        }
      ],
      "source": [
        "print(mat_data['datanew'][0][0][1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l_ZscHvicl0",
        "outputId": "ecdadf28-16ff-4d44-bb12-d4e8d62c19a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-0.86570132+0.38310367j -0.85496521+0.39368257j\n",
            "   -0.84368217+0.40362477j ...  0.68890667+0.20858736j\n",
            "    0.68945938+0.20640016j  0.68999785+0.20420378j]\n",
            "  [-0.08941916-0.19664524j -0.09756447-0.20212676j\n",
            "   -0.10612392-0.20723958j ...  0.05783085-0.05409621j\n",
            "    0.05795807-0.05366111j  0.05814043-0.05321635j]]\n",
            "\n",
            " [[-0.11723218-0.21020404j -0.12625068-0.21349844j\n",
            "   -0.13552417-0.21636043j ...  0.0677823 -0.05339449j\n",
            "    0.06802986-0.05392028j  0.06823816-0.05445299j]\n",
            "  [-0.95659131+0.17724706j -0.95084208+0.18872538j\n",
            "   -0.94416237+0.20000231j ... -0.65921265+0.28089011j\n",
            "   -0.6590097 +0.28165576j -0.65876347+0.2824344j ]]]\n",
            "[[[-0.92876697+0.12952596j -0.94073367+0.12644033j\n",
            "   -0.95452535+0.1298309j  ...  0.52643734+0.28387567j\n",
            "    0.53042942+0.28261378j  0.53452176+0.28033039j]\n",
            "  [ 0.01849695-0.20674758j  0.02065374-0.2266801j\n",
            "    0.01357647-0.24516264j ... -0.2463669 -0.18781553j\n",
            "   -0.24549405-0.18456404j -0.24426453-0.18218859j]]\n",
            "\n",
            " [[ 0.02891231-0.25573584j  0.00987105-0.27041402j\n",
            "   -0.0116756 -0.27629361j ... -0.24741386-0.1835992j\n",
            "   -0.2450684 -0.18043444j -0.24256085-0.17851314j]\n",
            "  [-0.9349677 -0.02778731j -0.93138754-0.04720177j\n",
            "   -0.93898606-0.06437288j ... -0.58612835+0.13762163j\n",
            "   -0.5848937 +0.14014567j -0.58324623+0.14162338j]]]\n",
            "[[[-0.95905274+0.02824961j -0.92092037+0.04756998j\n",
            "   -0.88035125+0.04509032j ...  0.52162528+0.07724459j\n",
            "    0.52214289+0.07274652j  0.52155417+0.06795375j]\n",
            "  [ 0.04396933-0.44814023j  0.00869328-0.42897657j\n",
            "   -0.00484176-0.39399132j ...  0.52836359-0.10459976j\n",
            "    0.52977139-0.10761185j  0.53091162-0.11169627j]]\n",
            "\n",
            " [[ 0.00277825-0.31891015j  0.04604351-0.30422154j\n",
            "    0.09225398-0.31539449j ...  0.53218937-0.1042591j\n",
            "    0.53303641-0.10938107j  0.53259861-0.11485632j]\n",
            "  [-0.90698361-0.29062125j -0.94656318-0.26262039j\n",
            "   -0.96038038-0.2161725j  ... -0.45801249+0.2469461j\n",
            "   -0.45460185+0.24780571j -0.4514851 +0.24740027j]]]\n",
            "[[[-9.59994078e-01+0.19922058j -9.63501632e-01+0.20796461j\n",
            "   -9.66184974e-01+0.21785016j ...  6.11596644e-01+0.17428261j\n",
            "    6.12606347e-01+0.17114913j  6.13351285e-01+0.16786405j]\n",
            "  [-9.92274843e-03-0.14940247j -2.75614508e-03-0.1590756j\n",
            "    5.49987773e-04-0.17155156j ...  1.82826687e-02+0.14869618j\n",
            "    1.85880419e-02+0.14882801j  1.91690456e-02+0.14964674j]]\n",
            "\n",
            " [[-3.59597616e-02-0.21235965j -4.56547961e-02-0.20996583j\n",
            "   -5.37536405e-02-0.20580916j ...  2.35973466e-02+0.1565913j\n",
            "    2.64135264e-02+0.15512048j  2.89045926e-02+0.15345158j]\n",
            "  [-9.68209982e-01+0.05376352j -9.56071258e-01+0.04726541j\n",
            "   -9.48946595e-01+0.03675347j ... -5.50243497e-01+0.26179776j\n",
            "   -5.51464438e-01+0.26239139j -5.52338421e-01+0.26384887j]]]\n",
            "[[[-0.97467053+0.21793568j -0.97570455+0.22899899j\n",
            "   -0.97623622+0.24016269j ...  0.62273347+0.20896044j\n",
            "    0.62313789+0.20675646j  0.62349784+0.20456783j]\n",
            "  [-0.03392941-0.12915352j -0.02705127-0.13142614j\n",
            "   -0.02048958-0.13636327j ...  0.21234071-0.02591865j\n",
            "    0.21130452-0.02875009j  0.20993002-0.031232j  ]]\n",
            "\n",
            " [[-0.05312784-0.15575491j -0.05439771-0.15660527j\n",
            "   -0.05549411-0.15740274j ...  0.21361902-0.0250391j\n",
            "    0.21334317-0.0272294j   0.21301116-0.02939853j]\n",
            "  [-0.98909342+0.09416399j -0.97923934+0.09832506j\n",
            "   -0.96947384+0.0994852j  ... -0.60236305+0.21721306j\n",
            "   -0.60343939+0.21628703j -0.60490948+0.21577001j]]]\n",
            "[[[-0.94033355+0.32026863j -0.9346025 +0.32944134j\n",
            "   -0.92865807+0.33818197j ...  0.67271775+0.18614331j\n",
            "    0.67257267+0.18361361j  0.67240292+0.18110216j]\n",
            "  [-0.04069763-0.16301012j -0.04385636-0.16572405j\n",
            "   -0.04692511-0.16823959j ...  0.1381979 -0.11540183j\n",
            "    0.13816004-0.11662426j  0.13807322-0.11790803j]]\n",
            "\n",
            " [[-0.00635452-0.13978161j -0.00768054-0.14682809j\n",
            "   -0.00943958-0.15379062j ...  0.14067268-0.12034091j\n",
            "    0.14005166-0.121769j    0.13936664-0.12318894j]\n",
            "  [-1.00270033+0.07558639j -1.00417471+0.08342931j\n",
            "   -1.00508249+0.09149642j ... -0.62911135+0.28858504j\n",
            "   -0.62775612+0.28861722j -0.62643915+0.28858608j]]]\n",
            "[[[-0.91262239+0.29588497j -0.91184193+0.30605805j\n",
            "   -0.9107492 +0.31644997j ...  0.64417821+0.21977267j\n",
            "    0.64480448+0.21843883j  0.64545155+0.2170984j ]\n",
            "  [-0.04510674-0.15953226j -0.04783972-0.16512823j\n",
            "   -0.0508355 -0.17066458j ...  0.03312061-0.10471155j\n",
            "    0.0329042 -0.10471374j  0.03262509-0.10472765j]]\n",
            "\n",
            " [[-0.03716701-0.12977596j -0.03693527-0.13608515j\n",
            "   -0.03701155-0.14264248j ...  0.02181556-0.10452884j\n",
            "    0.02158453-0.10321377j  0.02144632-0.10190491j]\n",
            "  [-0.94845295+0.17396398j -0.94246799+0.17727593j\n",
            "   -0.93670875+0.18012634j ... -0.63203245+0.24342903j\n",
            "   -0.63219458+0.24322079j -0.63240218+0.24302167j]]]\n",
            "[[[-0.9175359 +0.39765313j -0.90634531+0.41536194j\n",
            "   -0.89386439+0.43243083j ...  0.69720483+0.23608291j\n",
            "    0.69778007+0.23347472j  0.6983158 +0.23088053j]\n",
            "  [-0.06768756-0.20444579j -0.0746536 -0.21402255j\n",
            "   -0.08247595-0.22349128j ...  0.02104645-0.03689695j\n",
            "    0.02117985-0.03684304j  0.02127385-0.03675759j]]\n",
            "\n",
            " [[-0.05473843-0.23637407j -0.06545895-0.24688017j\n",
            "   -0.07708131-0.25684714j ...  0.02055781-0.03927626j\n",
            "    0.02043703-0.03915422j  0.02028613-0.03897364j]\n",
            "  [-0.99175847+0.21792233j -0.98384023+0.23374075j\n",
            "   -0.97460538+0.24922213j ... -0.68355614+0.26750869j\n",
            "   -0.68329895+0.26819399j -0.68304443+0.26889259j]]]\n",
            "[[[-0.91837883-0.09398656j -0.9545756 -0.07239757j\n",
            "   -0.97166687-0.03252735j ...  0.52952343+0.07806008j\n",
            "    0.53018183+0.07551628j  0.53084683+0.0722084j ]\n",
            "  [ 0.06107018-0.45610026j  0.0134871 -0.44184744j\n",
            "   -0.0087255 -0.40157065j ...  0.55542409-0.11199784j\n",
            "    0.55293369-0.11586437j  0.55054796-0.11814237j]]\n",
            "\n",
            " [[ 0.05267007-0.45964265j  0.00976439-0.44378841j\n",
            "   -0.01168851-0.40704912j ...  0.5521844 -0.10757258j\n",
            "    0.55320835-0.11035027j  0.55425364-0.11401408j]\n",
            "  [-0.88773263-0.297492j   -0.94004518-0.27521485j\n",
            "   -0.96276695-0.22359589j ... -0.46584588+0.24574636j\n",
            "   -0.46698168+0.24586712j -0.46795383+0.24783729j]]]\n",
            "[[[-0.94209009+0.19016539j -0.9266901 +0.18487085j\n",
            "   -0.91747385+0.17480078j ...  0.52810919+0.28343388j\n",
            "    0.5279417 +0.28194955j  0.52819502+0.28132105j]\n",
            "  [-0.0337542 -0.21124174j -0.02363776-0.20189585j\n",
            "   -0.00824745-0.20115599j ... -0.25592709-0.19772452j\n",
            "   -0.25767756-0.19466186j -0.25877085-0.19096781j]]\n",
            "\n",
            " [[ 0.00995625-0.18333746j  0.02387177-0.19924287j\n",
            "    0.03064438-0.2200495j  ... -0.25561139-0.20073546j\n",
            "   -0.25849923-0.19775721j -0.26084229-0.19373278j]\n",
            "  [-0.99932438-0.03354575j -0.98609072-0.01684163j\n",
            "   -0.96610022-0.01020113j ... -0.58920646+0.12578358j\n",
            "   -0.59114623+0.12804724j -0.59231347+0.1310759j ]]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print(mat_data['datanew'][0][0][1][:,:, :, i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVuqYCJoYC0C"
      },
      "outputs": [],
      "source": [
        "# # @title\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Load the CSV file into a pandas DataFrame\n",
        "# df = pd.read_csv('/content/drive/MyDrive/IEEE EMBS SMP Project/Data_Simulation/new_simulations/data.csv')\n",
        "\n",
        "# # Convert the DataFrame to a NumPy array\n",
        "# full_array = df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI5MOfwnbtn5"
      },
      "outputs": [],
      "source": [
        "# full_array.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj1X16A_EKui"
      },
      "outputs": [],
      "source": [
        "# full_array[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgkWPQQ2hWNq"
      },
      "outputs": [],
      "source": [
        "# full_array[:504, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phaZlG2uZKiW"
      },
      "outputs": [],
      "source": [
        "# print(df.apply(np.mean))\n",
        "# print(df.apply(np.std))\n",
        "# print(df.apply(np.max))\n",
        "# print(df.apply(np.min))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8iu_1zma5lH"
      },
      "outputs": [],
      "source": [
        "# threshold_value = 13.10305 # 0.666192*3+11.104474\n",
        "\n",
        "# column_to_analyze = data_vector[:, 3]\n",
        "# pass_threshold_mask = column_to_analyze > threshold_value\n",
        "# count_passing_samples = np.count_nonzero(pass_threshold_mask)\n",
        "# print(f\"Number of samples passing the threshold: {count_passing_samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mMPuvwfrUSp"
      },
      "outputs": [],
      "source": [
        "# # Identify and remove duplicates\n",
        "# df_no_duplicates = df.drop_duplicates(keep='first')\n",
        "\n",
        "# # Save the result to a new CSV file if needed\n",
        "# df_no_duplicates.to_csv('cleaned_file.csv', index=False)\n",
        "\n",
        "# # Convert the DataFrame to a NumPy array\n",
        "# full_array_no_duplicates = df_no_duplicates.values\n",
        "# full_array_no_duplicates.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8KT_lMftoRu"
      },
      "outputs": [],
      "source": [
        "X = mat_data['datanew'][0][0][1].reshape((2 * 2 * 1001, 169)).T\n",
        "y = mat_data['datanew'][0][0][0] #.reshape((3, 169)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7u_YAsSqAuv"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "\n",
        "# sum_array_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRaN_p82dayL",
        "outputId": "01fdf070-b363-4353-b6c3-d03d0d0990e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(169, 4004)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oy_Lqll35J8",
        "outputId": "dd0f1370-f6d4-4261-825d-d6ff054edddc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(169, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNrMntMw4Eux"
      },
      "source": [
        "Just testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxSrGTL581_t",
        "outputId": "de5c9f05-8858-43b4-b22e-09cbe1a39887"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.98668396+0.15292013j, -0.98272109+0.17032465j,\n",
              "        -0.97419804+0.18568896j, ..., -0.5715071 +0.13318332j,\n",
              "        -0.57307142+0.13581447j, -0.57372415+0.13888747j],\n",
              "       [-0.9849968 +0.14966434j, -0.98222739+0.1668717j ,\n",
              "        -0.97485989+0.18251653j, ..., -0.55620766+0.13806093j,\n",
              "        -0.55629551+0.13762562j, -0.55716437+0.13724133j],\n",
              "       [-0.98252726+0.25978017j, -0.97940505+0.27196094j,\n",
              "        -0.97560352+0.28380808j, ..., -0.59939891+0.22856458j,\n",
              "        -0.59770006+0.22868243j, -0.59616703+0.22825265j],\n",
              "       ...,\n",
              "       [-0.83965832+0.00436371j, -0.83329397-0.02315836j,\n",
              "        -0.8396337 -0.04876202j, ..., -0.44136316+0.22152416j,\n",
              "        -0.43936554+0.22017017j, -0.43848956+0.21860319j],\n",
              "       [-0.83871245-0.00141003j, -0.83463699-0.02090368j,\n",
              "        -0.83800453-0.03900377j, ..., -0.42152199+0.20386253j,\n",
              "        -0.42066926+0.20592488j, -0.41921106+0.20786965j],\n",
              "       [-0.83694053-0.00367127j, -0.83397377-0.02376966j,\n",
              "        -0.83856368-0.04206602j, ..., -0.41830894+0.21151507j,\n",
              "        -0.41671702+0.21229269j, -0.41506571+0.21276523j]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = np.unique(X, axis=0)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRj7ZwXdSu78",
        "outputId": "23bd3d93-e841-4499-db14-bbe3e9f0e07e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(168, 4004)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZLoBfpHQjPk",
        "outputId": "ba13ee62-00bc-4af6-d147-650d92e8cc75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.968198  +0.2501851j , -0.95635635+0.26174176j,\n",
              "       -0.94342089+0.26923579j, ..., -0.43986645+0.2250226j ,\n",
              "       -0.43792567+0.22085951j, -0.43794703+0.21612328j])"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkyYCzzbRCjE",
        "outputId": "fb28b9f9-9436-4742-b241-17ad697dd30d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_complex = np.all(np.iscomplex(X))\n",
        "all_complex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsUKzb48Qtb5"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy8aeZC9eTbM"
      },
      "source": [
        "# Real-valued model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVuXuoGVqyWt"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.245, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soNgkEiut9AR",
        "outputId": "5efee282-8483-4c73-b28d-a16c86fd4dca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(127, 4004)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQQ1NIw0C2fa"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(full_array[:,:5], full_array[:,5], test_size=0.245, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXxTSWz59NbZ",
        "outputId": "f6c14075-782f-40c6-bdd1-0e3004e2e843"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.95045483+0.29827434j, -0.94611484+0.3087866j ,\n",
              "       -0.94140643+0.31898522j, ..., -0.61546588+0.27329239j,\n",
              "       -0.61506802+0.2731314j , -0.61472493+0.27296954j])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_k6FfzX-YRG"
      },
      "outputs": [],
      "source": [
        "# file_path = 'X_test_withFreq.csv' #withStandardizing\n",
        "\n",
        "# # Save the NumPy array to a CSV file\n",
        "# np.savetxt(file_path, X_test, delimiter=',', fmt='%.15f')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK-Xmcn9ONYL"
      },
      "outputs": [],
      "source": [
        "# file_path = 'y_test.csv' #withStandardizing\n",
        "\n",
        "# # Save the NumPy array to a CSV file\n",
        "# np.savetxt(file_path, y_test, delimiter=',', fmt='%.15f')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8dCvJ3FrSKz",
        "outputId": "28a10e91-f838-4666-9a2c-694a5b9e49d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               2050560   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2126659 (8.11 MB)\n",
            "Trainable params: 2126659 (8.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r1/1 [==============================] - ETA: 0s - loss: 715.3400"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 658.11487, saving model to best_model_1_treating_as_real.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 2s 2s/step - loss: 715.3400 - val_loss: 658.1149 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 655.5114\n",
            "Epoch 2: val_loss improved from 658.11487 to 553.76453, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 655.5114 - val_loss: 553.7645 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 551.5958\n",
            "Epoch 3: val_loss improved from 553.76453 to 438.39633, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 551.5958 - val_loss: 438.3963 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 437.3190\n",
            "Epoch 4: val_loss improved from 438.39633 to 329.92365, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 437.3190 - val_loss: 329.9236 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 330.2986\n",
            "Epoch 5: val_loss improved from 329.92365 to 260.52921, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 330.2986 - val_loss: 260.5292 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 262.9115\n",
            "Epoch 6: val_loss improved from 260.52921 to 234.69508, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 262.9115 - val_loss: 234.6951 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 239.6488\n",
            "Epoch 7: val_loss did not improve from 234.69508\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 239.6488 - val_loss: 234.9787 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 242.0654\n",
            "Epoch 8: val_loss improved from 234.69508 to 220.91263, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 242.0654 - val_loss: 220.9126 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 228.7816\n",
            "Epoch 9: val_loss improved from 220.91263 to 181.42322, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 228.7816 - val_loss: 181.4232 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 188.9801\n",
            "Epoch 10: val_loss improved from 181.42322 to 132.07301, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 188.9801 - val_loss: 132.0730 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 138.8832\n",
            "Epoch 11: val_loss improved from 132.07301 to 93.87969, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 138.8832 - val_loss: 93.8797 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 99.9122\n",
            "Epoch 12: val_loss improved from 93.87969 to 73.90728, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 99.9122 - val_loss: 73.9073 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 79.2234\n",
            "Epoch 13: val_loss improved from 73.90728 to 68.04948, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 79.2234 - val_loss: 68.0495 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 72.9808\n",
            "Epoch 14: val_loss improved from 68.04948 to 64.08684, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 72.9808 - val_loss: 64.0868 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 69.0854\n",
            "Epoch 15: val_loss improved from 64.08684 to 58.66795, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 69.0854 - val_loss: 58.6679 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 63.9613\n",
            "Epoch 16: val_loss improved from 58.66795 to 52.90010, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 63.9613 - val_loss: 52.9001 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 58.7534\n",
            "Epoch 17: val_loss improved from 52.90010 to 47.73368, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 58.7534 - val_loss: 47.7337 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 54.4447\n",
            "Epoch 18: val_loss improved from 47.73368 to 45.78275, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 54.4447 - val_loss: 45.7828 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 53.4409\n",
            "Epoch 19: val_loss did not improve from 45.78275\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 53.4409 - val_loss: 45.8371 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 54.3328\n",
            "Epoch 20: val_loss improved from 45.78275 to 44.54718, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 54.3328 - val_loss: 44.5472 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 53.5316\n",
            "Epoch 21: val_loss improved from 44.54718 to 39.30132, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 53.5316 - val_loss: 39.3013 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 48.2120\n",
            "Epoch 22: val_loss improved from 39.30132 to 32.40415, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 48.2120 - val_loss: 32.4041 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.7056\n",
            "Epoch 23: val_loss improved from 32.40415 to 28.67787, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 40.7056 - val_loss: 28.6779 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 35.9567\n",
            "Epoch 24: val_loss did not improve from 28.67787\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 35.9567 - val_loss: 29.3891 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 35.6739\n",
            "Epoch 25: val_loss did not improve from 28.67787\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 35.6739 - val_loss: 30.2107 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 36.0589\n",
            "Epoch 26: val_loss improved from 28.67787 to 28.19043, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 36.0589 - val_loss: 28.1904 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 34.2037\n",
            "Epoch 27: val_loss improved from 28.19043 to 24.89121, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 34.2037 - val_loss: 24.8912 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 31.5791\n",
            "Epoch 28: val_loss improved from 24.89121 to 23.16272, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 31.5791 - val_loss: 23.1627 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 30.7223\n",
            "Epoch 29: val_loss improved from 23.16272 to 23.04593, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 30.7223 - val_loss: 23.0459 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 31.2978\n",
            "Epoch 30: val_loss improved from 23.04593 to 22.39866, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 31.2978 - val_loss: 22.3987 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 30.8798\n",
            "Epoch 31: val_loss improved from 22.39866 to 20.40585, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 30.8798 - val_loss: 20.4058 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 28.7061\n",
            "Epoch 32: val_loss improved from 20.40585 to 19.00856, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 28.7061 - val_loss: 19.0086 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 26.7588\n",
            "Epoch 33: val_loss improved from 19.00856 to 18.55556, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 26.7588 - val_loss: 18.5556 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 25.9873\n",
            "Epoch 34: val_loss improved from 18.55556 to 18.21102, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 25.9873 - val_loss: 18.2110 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 25.5109\n",
            "Epoch 35: val_loss improved from 18.21102 to 17.27725, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 25.5109 - val_loss: 17.2773 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 24.6654\n",
            "Epoch 36: val_loss improved from 17.27725 to 16.06531, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 24.6654 - val_loss: 16.0653 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 23.6662\n",
            "Epoch 37: val_loss improved from 16.06531 to 15.22811, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 23.6662 - val_loss: 15.2281 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 23.1222\n",
            "Epoch 38: val_loss improved from 15.22811 to 14.89004, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 23.1222 - val_loss: 14.8900 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 22.9810\n",
            "Epoch 39: val_loss improved from 14.89004 to 14.65197, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 22.9810 - val_loss: 14.6520 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 22.6498\n",
            "Epoch 40: val_loss improved from 14.65197 to 14.45222, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 22.6498 - val_loss: 14.4522 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 22.1364\n",
            "Epoch 41: val_loss did not improve from 14.45222\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 22.1364 - val_loss: 14.6086 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 21.8292\n",
            "Epoch 42: val_loss did not improve from 14.45222\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 21.8292 - val_loss: 15.0808 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 21.8741\n",
            "Epoch 43: val_loss did not improve from 14.45222\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 21.8741 - val_loss: 15.1304 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 21.8444\n",
            "Epoch 44: val_loss did not improve from 14.45222\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 21.8444 - val_loss: 14.7373 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 21.6305\n",
            "Epoch 45: val_loss improved from 14.45222 to 14.31167, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 21.6305 - val_loss: 14.3117 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 21.4218\n",
            "Epoch 46: val_loss improved from 14.31167 to 14.09227, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 21.4218 - val_loss: 14.0923 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 21.2796\n",
            "Epoch 47: val_loss did not improve from 14.09227\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 21.2796 - val_loss: 14.1121 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 21.2408\n",
            "Epoch 48: val_loss improved from 14.09227 to 14.04045, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 335ms/step - loss: 21.2408 - val_loss: 14.0405 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 21.0735\n",
            "Epoch 49: val_loss improved from 14.04045 to 13.80536, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 21.0735 - val_loss: 13.8054 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.8268\n",
            "Epoch 50: val_loss improved from 13.80536 to 13.63308, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 20.8268 - val_loss: 13.6331 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.7420\n",
            "Epoch 51: val_loss improved from 13.63308 to 13.56812, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 20.7420 - val_loss: 13.5681 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.7587\n",
            "Epoch 52: val_loss improved from 13.56812 to 13.44185, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 20.7587 - val_loss: 13.4419 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.6337\n",
            "Epoch 53: val_loss improved from 13.44185 to 13.32570, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 20.6337 - val_loss: 13.3257 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.5143\n",
            "Epoch 54: val_loss improved from 13.32570 to 13.19685, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 20.5143 - val_loss: 13.1969 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.4856\n",
            "Epoch 55: val_loss improved from 13.19685 to 12.97381, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 20.4856 - val_loss: 12.9738 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.4568\n",
            "Epoch 56: val_loss improved from 12.97381 to 12.76507, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 20.4568 - val_loss: 12.7651 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.3944\n",
            "Epoch 57: val_loss improved from 12.76507 to 12.67806, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 20.3944 - val_loss: 12.6781 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.2879\n",
            "Epoch 58: val_loss did not improve from 12.67806\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 20.2879 - val_loss: 12.7313 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.2255\n",
            "Epoch 59: val_loss did not improve from 12.67806\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 20.2255 - val_loss: 12.7478 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.1949\n",
            "Epoch 60: val_loss improved from 12.67806 to 12.57633, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 20.1949 - val_loss: 12.5763 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.1141\n",
            "Epoch 61: val_loss improved from 12.57633 to 12.36059, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 20.1141 - val_loss: 12.3606 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.0397\n",
            "Epoch 62: val_loss improved from 12.36059 to 12.28125, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 20.0397 - val_loss: 12.2812 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.9913\n",
            "Epoch 63: val_loss did not improve from 12.28125\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 19.9913 - val_loss: 12.3320 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.9514\n",
            "Epoch 64: val_loss did not improve from 12.28125\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 19.9514 - val_loss: 12.3594 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.9004\n",
            "Epoch 65: val_loss did not improve from 12.28125\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 19.9004 - val_loss: 12.2931 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.8329\n",
            "Epoch 66: val_loss improved from 12.28125 to 12.24672, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 19.8329 - val_loss: 12.2467 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.7970\n",
            "Epoch 67: val_loss did not improve from 12.24672\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 19.7970 - val_loss: 12.2589 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.7518\n",
            "Epoch 68: val_loss did not improve from 12.24672\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 19.7518 - val_loss: 12.2791 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.7000\n",
            "Epoch 69: val_loss improved from 12.24672 to 12.21777, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 19.7000 - val_loss: 12.2178 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.6528\n",
            "Epoch 70: val_loss improved from 12.21777 to 12.08359, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 19.6528 - val_loss: 12.0836 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.6149\n",
            "Epoch 71: val_loss improved from 12.08359 to 12.02434, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 19.6149 - val_loss: 12.0243 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.5723\n",
            "Epoch 72: val_loss did not improve from 12.02434\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 19.5723 - val_loss: 12.0353 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.5224\n",
            "Epoch 73: val_loss did not improve from 12.02434\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 19.5224 - val_loss: 12.0474 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.4863\n",
            "Epoch 74: val_loss improved from 12.02434 to 11.94863, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 19.4863 - val_loss: 11.9486 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.4425\n",
            "Epoch 75: val_loss improved from 11.94863 to 11.85876, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 19.4425 - val_loss: 11.8588 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.3962\n",
            "Epoch 76: val_loss improved from 11.85876 to 11.82778, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 19.3962 - val_loss: 11.8278 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.3505\n",
            "Epoch 77: val_loss improved from 11.82778 to 11.79897, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 19.3505 - val_loss: 11.7990 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.3128\n",
            "Epoch 78: val_loss improved from 11.79897 to 11.70699, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 19.3128 - val_loss: 11.7070 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.2657\n",
            "Epoch 79: val_loss improved from 11.70699 to 11.65577, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 19.2657 - val_loss: 11.6558 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.2200\n",
            "Epoch 80: val_loss did not improve from 11.65577\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 19.2200 - val_loss: 11.6653 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.1779\n",
            "Epoch 81: val_loss improved from 11.65577 to 11.65416, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 19.1779 - val_loss: 11.6542 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.1360\n",
            "Epoch 82: val_loss improved from 11.65416 to 11.58422, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 19.1360 - val_loss: 11.5842 - lr: 0.0010\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.0923\n",
            "Epoch 83: val_loss improved from 11.58422 to 11.54423, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 19.0923 - val_loss: 11.5442 - lr: 0.0010\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.0572\n",
            "Epoch 84: val_loss did not improve from 11.54423\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 19.0572 - val_loss: 11.5554 - lr: 0.0010\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.0168\n",
            "Epoch 85: val_loss improved from 11.54423 to 11.54014, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 19.0168 - val_loss: 11.5401 - lr: 0.0010\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.9741\n",
            "Epoch 86: val_loss improved from 11.54014 to 11.50002, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 18.9741 - val_loss: 11.5000 - lr: 0.0010\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.9341\n",
            "Epoch 87: val_loss improved from 11.50002 to 11.47756, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 18.9341 - val_loss: 11.4776 - lr: 0.0010\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.8938\n",
            "Epoch 88: val_loss improved from 11.47756 to 11.43523, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 18.8938 - val_loss: 11.4352 - lr: 0.0010\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.8553\n",
            "Epoch 89: val_loss improved from 11.43523 to 11.34113, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 18.8553 - val_loss: 11.3411 - lr: 0.0010\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.8136\n",
            "Epoch 90: val_loss improved from 11.34113 to 11.30534, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 18.8136 - val_loss: 11.3053 - lr: 0.0010\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.7711\n",
            "Epoch 91: val_loss improved from 11.30534 to 11.28362, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 18.7711 - val_loss: 11.2836 - lr: 0.0010\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.7307\n",
            "Epoch 92: val_loss improved from 11.28362 to 11.19627, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 18.7307 - val_loss: 11.1963 - lr: 0.0010\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.6865\n",
            "Epoch 93: val_loss improved from 11.19627 to 11.11162, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 18.6865 - val_loss: 11.1116 - lr: 0.0010\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.6407\n",
            "Epoch 94: val_loss improved from 11.11162 to 11.05920, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 18.6407 - val_loss: 11.0592 - lr: 0.0010\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.6026\n",
            "Epoch 95: val_loss improved from 11.05920 to 11.00934, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 18.6026 - val_loss: 11.0093 - lr: 0.0010\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.5590\n",
            "Epoch 96: val_loss did not improve from 11.00934\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 18.5590 - val_loss: 11.0317 - lr: 0.0010\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.5143\n",
            "Epoch 97: val_loss improved from 11.00934 to 11.00821, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 18.5143 - val_loss: 11.0082 - lr: 0.0010\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.4704\n",
            "Epoch 98: val_loss improved from 11.00821 to 10.99823, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 18.4704 - val_loss: 10.9982 - lr: 0.0010\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.4284\n",
            "Epoch 99: val_loss improved from 10.99823 to 10.97139, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 18.4284 - val_loss: 10.9714 - lr: 0.0010\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.3829\n",
            "Epoch 100: val_loss improved from 10.97139 to 10.90477, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 18.3829 - val_loss: 10.9048 - lr: 0.0010\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.3350\n",
            "Epoch 101: val_loss improved from 10.90477 to 10.85735, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 18.3350 - val_loss: 10.8573 - lr: 0.0010\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.2919\n",
            "Epoch 102: val_loss improved from 10.85735 to 10.75355, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 18.2919 - val_loss: 10.7536 - lr: 0.0010\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.2519\n",
            "Epoch 103: val_loss did not improve from 10.75355\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 18.2519 - val_loss: 10.9029 - lr: 0.0010\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.2449\n",
            "Epoch 104: val_loss improved from 10.75355 to 10.63685, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 18.2449 - val_loss: 10.6368 - lr: 0.0010\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.2467\n",
            "Epoch 105: val_loss did not improve from 10.63685\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 18.2467 - val_loss: 10.8722 - lr: 0.0010\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.1576\n",
            "Epoch 106: val_loss did not improve from 10.63685\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 18.1576 - val_loss: 10.7125 - lr: 0.0010\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.0760\n",
            "Epoch 107: val_loss improved from 10.63685 to 10.54694, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 18.0760 - val_loss: 10.5469 - lr: 0.0010\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.0910\n",
            "Epoch 108: val_loss did not improve from 10.54694\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 18.0910 - val_loss: 10.6849 - lr: 0.0010\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.0074\n",
            "Epoch 109: val_loss did not improve from 10.54694\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 18.0074 - val_loss: 10.6498 - lr: 0.0010\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.9623\n",
            "Epoch 110: val_loss improved from 10.54694 to 10.42812, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 17.9623 - val_loss: 10.4281 - lr: 0.0010\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.9439\n",
            "Epoch 111: val_loss did not improve from 10.42812\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 17.9439 - val_loss: 10.4681 - lr: 0.0010\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.8608\n",
            "Epoch 112: val_loss did not improve from 10.42812\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 17.8608 - val_loss: 10.5347 - lr: 0.0010\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.8434\n",
            "Epoch 113: val_loss improved from 10.42812 to 10.32395, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 17.8434 - val_loss: 10.3239 - lr: 0.0010\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.7970\n",
            "Epoch 114: val_loss improved from 10.32395 to 10.29618, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 17.7970 - val_loss: 10.2962 - lr: 0.0010\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.7332\n",
            "Epoch 115: val_loss did not improve from 10.29618\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 17.7332 - val_loss: 10.3648 - lr: 0.0010\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.7244\n",
            "Epoch 116: val_loss improved from 10.29618 to 10.24022, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 17.7244 - val_loss: 10.2402 - lr: 0.0010\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.6669\n",
            "Epoch 117: val_loss improved from 10.24022 to 10.19825, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 17.6669 - val_loss: 10.1982 - lr: 0.0010\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.6150\n",
            "Epoch 118: val_loss did not improve from 10.19825\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 17.6150 - val_loss: 10.2560 - lr: 0.0010\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.5958\n",
            "Epoch 119: val_loss improved from 10.19825 to 10.16650, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 17.5958 - val_loss: 10.1665 - lr: 0.0010\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.5355\n",
            "Epoch 120: val_loss improved from 10.16650 to 10.07808, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 17.5355 - val_loss: 10.0781 - lr: 0.0010\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.4964\n",
            "Epoch 121: val_loss did not improve from 10.07808\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 17.4964 - val_loss: 10.1385 - lr: 0.0010\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.4685\n",
            "Epoch 122: val_loss improved from 10.07808 to 10.07042, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 17.4685 - val_loss: 10.0704 - lr: 0.0010\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.4158\n",
            "Epoch 123: val_loss improved from 10.07042 to 9.93880, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 17.4158 - val_loss: 9.9388 - lr: 0.0010\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.3710\n",
            "Epoch 124: val_loss did not improve from 9.93880\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 17.3710 - val_loss: 9.9703 - lr: 0.0010\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.3325\n",
            "Epoch 125: val_loss did not improve from 9.93880\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 17.3325 - val_loss: 9.9638 - lr: 0.0010\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.3022\n",
            "Epoch 126: val_loss improved from 9.93880 to 9.86583, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 17.3022 - val_loss: 9.8658 - lr: 0.0010\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.2663\n",
            "Epoch 127: val_loss did not improve from 9.86583\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 17.2663 - val_loss: 9.8921 - lr: 0.0010\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.2239\n",
            "Epoch 128: val_loss improved from 9.86583 to 9.79316, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 17.2239 - val_loss: 9.7932 - lr: 0.0010\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.1829\n",
            "Epoch 129: val_loss improved from 9.79316 to 9.78604, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 17.1829 - val_loss: 9.7860 - lr: 0.0010\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.1474\n",
            "Epoch 130: val_loss improved from 9.78604 to 9.73728, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 17.1474 - val_loss: 9.7373 - lr: 0.0010\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.1136\n",
            "Epoch 131: val_loss improved from 9.73728 to 9.71855, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 17.1136 - val_loss: 9.7185 - lr: 0.0010\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.0842\n",
            "Epoch 132: val_loss improved from 9.71855 to 9.67462, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 17.0842 - val_loss: 9.6746 - lr: 0.0010\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.0587\n",
            "Epoch 133: val_loss did not improve from 9.67462\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 17.0587 - val_loss: 9.7124 - lr: 0.0010\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.0322\n",
            "Epoch 134: val_loss improved from 9.67462 to 9.63226, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 17.0322 - val_loss: 9.6323 - lr: 0.0010\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.9945\n",
            "Epoch 135: val_loss improved from 9.63226 to 9.61085, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 16.9945 - val_loss: 9.6109 - lr: 0.0010\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.9510\n",
            "Epoch 136: val_loss improved from 9.61085 to 9.58492, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 16.9510 - val_loss: 9.5849 - lr: 0.0010\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.9045\n",
            "Epoch 137: val_loss improved from 9.58492 to 9.54927, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 16.9045 - val_loss: 9.5493 - lr: 0.0010\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.8703\n",
            "Epoch 138: val_loss improved from 9.54927 to 9.47320, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 16.8703 - val_loss: 9.4732 - lr: 0.0010\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.8370\n",
            "Epoch 139: val_loss improved from 9.47320 to 9.46591, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 16.8370 - val_loss: 9.4659 - lr: 0.0010\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.8032\n",
            "Epoch 140: val_loss improved from 9.46591 to 9.39853, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 1s 746ms/step - loss: 16.8032 - val_loss: 9.3985 - lr: 0.0010\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.7704\n",
            "Epoch 141: val_loss improved from 9.39853 to 9.37523, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 16.7704 - val_loss: 9.3752 - lr: 0.0010\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.7401\n",
            "Epoch 142: val_loss improved from 9.37523 to 9.30184, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 16.7401 - val_loss: 9.3018 - lr: 0.0010\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.7091\n",
            "Epoch 143: val_loss did not improve from 9.30184\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 16.7091 - val_loss: 9.3029 - lr: 0.0010\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.6845\n",
            "Epoch 144: val_loss did not improve from 9.30184\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 16.6845 - val_loss: 9.3566 - lr: 0.0010\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.6765\n",
            "Epoch 145: val_loss did not improve from 9.30184\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 16.6765 - val_loss: 9.3489 - lr: 0.0010\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.7623\n",
            "Epoch 146: val_loss did not improve from 9.30184\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 16.7623 - val_loss: 9.7133 - lr: 0.0010\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.0317\n",
            "Epoch 147: val_loss did not improve from 9.30184\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 17.0317 - val_loss: 9.6554 - lr: 0.0010\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.0094\n",
            "Epoch 148: val_loss improved from 9.30184 to 9.11087, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 17.0094 - val_loss: 9.1109 - lr: 0.0010\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.5543\n",
            "Epoch 149: val_loss did not improve from 9.11087\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 16.5543 - val_loss: 9.5284 - lr: 0.0010\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.8304\n",
            "Epoch 150: val_loss did not improve from 9.11087\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 16.8304 - val_loss: 9.3985 - lr: 0.0010\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.6839\n",
            "Epoch 151: val_loss improved from 9.11087 to 9.05894, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 1s 825ms/step - loss: 16.6839 - val_loss: 9.0589 - lr: 0.0010\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.5395\n",
            "Epoch 152: val_loss did not improve from 9.05894\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 16.5395 - val_loss: 9.3003 - lr: 0.0010\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.7030\n",
            "Epoch 153: val_loss did not improve from 9.05894\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 16.7030 - val_loss: 9.2375 - lr: 0.0010\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.4424\n",
            "Epoch 154: val_loss did not improve from 9.05894\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 16.4424 - val_loss: 9.2398 - lr: 0.0010\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.5853\n",
            "Epoch 155: val_loss improved from 9.05894 to 8.98100, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 1s 893ms/step - loss: 16.5853 - val_loss: 8.9810 - lr: 0.0010\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.4409\n",
            "Epoch 156: val_loss did not improve from 8.98100\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 16.4409 - val_loss: 9.2077 - lr: 0.0010\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.4365\n",
            "Epoch 157: val_loss did not improve from 8.98100\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 16.4365 - val_loss: 9.2272 - lr: 0.0010\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.4251\n",
            "Epoch 158: val_loss improved from 8.98100 to 8.90010, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 1s 603ms/step - loss: 16.4251 - val_loss: 8.9001 - lr: 0.0010\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.3377\n",
            "Epoch 159: val_loss did not improve from 8.90010\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 16.3377 - val_loss: 9.0168 - lr: 0.0010\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.3776\n",
            "Epoch 160: val_loss did not improve from 8.90010\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 16.3776 - val_loss: 9.1083 - lr: 0.0010\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.2730\n",
            "Epoch 161: val_loss did not improve from 8.90010\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 16.2730 - val_loss: 8.9936 - lr: 0.0010\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.3014\n",
            "Epoch 162: val_loss improved from 8.90010 to 8.78134, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 1s 752ms/step - loss: 16.3014 - val_loss: 8.7813 - lr: 0.0010\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.2466\n",
            "Epoch 163: val_loss did not improve from 8.78134\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 16.2466 - val_loss: 8.9185 - lr: 0.0010\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.2238\n",
            "Epoch 164: val_loss did not improve from 8.78134\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 16.2238 - val_loss: 8.9908 - lr: 0.0010\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.2170\n",
            "Epoch 165: val_loss improved from 8.78134 to 8.73157, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 16.2170 - val_loss: 8.7316 - lr: 0.0010\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.1575\n",
            "Epoch 166: val_loss did not improve from 8.73157\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 16.1575 - val_loss: 8.7317 - lr: 0.0010\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.1642\n",
            "Epoch 167: val_loss did not improve from 8.73157\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 16.1642 - val_loss: 8.8657 - lr: 0.0010\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.1008\n",
            "Epoch 168: val_loss did not improve from 8.73157\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 16.1008 - val_loss: 8.8687 - lr: 0.0010\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.1017\n",
            "Epoch 169: val_loss improved from 8.73157 to 8.67293, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 16.1017 - val_loss: 8.6729 - lr: 0.0010\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.0622\n",
            "Epoch 170: val_loss did not improve from 8.67293\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 16.0622 - val_loss: 8.7208 - lr: 0.0010\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.0376\n",
            "Epoch 171: val_loss did not improve from 8.67293\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 16.0376 - val_loss: 8.8182 - lr: 0.0010\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.0355\n",
            "Epoch 172: val_loss improved from 8.67293 to 8.61338, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 16.0355 - val_loss: 8.6134 - lr: 0.0010\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.9842\n",
            "Epoch 173: val_loss improved from 8.61338 to 8.55900, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 15.9842 - val_loss: 8.5590 - lr: 0.0010\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.9934\n",
            "Epoch 174: val_loss did not improve from 8.55900\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 15.9934 - val_loss: 8.6246 - lr: 0.0010\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.9456\n",
            "Epoch 175: val_loss did not improve from 8.55900\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 15.9456 - val_loss: 8.6554 - lr: 0.0010\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.9370\n",
            "Epoch 176: val_loss improved from 8.55900 to 8.54856, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 15.9370 - val_loss: 8.5486 - lr: 0.0010\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.9149\n",
            "Epoch 177: val_loss improved from 8.54856 to 8.53539, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 15.9149 - val_loss: 8.5354 - lr: 0.0010\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.8785\n",
            "Epoch 178: val_loss did not improve from 8.53539\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 15.8785 - val_loss: 8.6517 - lr: 0.0010\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.8748\n",
            "Epoch 179: val_loss did not improve from 8.53539\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 15.8748 - val_loss: 8.5965 - lr: 0.0010\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.8398\n",
            "Epoch 180: val_loss improved from 8.53539 to 8.48252, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 15.8398 - val_loss: 8.4825 - lr: 0.0010\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.8240\n",
            "Epoch 181: val_loss improved from 8.48252 to 8.46523, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 15.8240 - val_loss: 8.4652 - lr: 0.0010\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.8064\n",
            "Epoch 182: val_loss did not improve from 8.46523\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 15.8064 - val_loss: 8.5375 - lr: 0.0010\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.7866\n",
            "Epoch 183: val_loss improved from 8.46523 to 8.36571, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 15.7866 - val_loss: 8.3657 - lr: 0.0010\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.7607\n",
            "Epoch 184: val_loss improved from 8.36571 to 8.33943, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 15.7607 - val_loss: 8.3394 - lr: 0.0010\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.7476\n",
            "Epoch 185: val_loss did not improve from 8.33943\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 15.7476 - val_loss: 8.4978 - lr: 0.0010\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.7270\n",
            "Epoch 186: val_loss did not improve from 8.33943\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 15.7270 - val_loss: 8.3813 - lr: 0.0010\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.6953\n",
            "Epoch 187: val_loss improved from 8.33943 to 8.31944, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 15.6953 - val_loss: 8.3194 - lr: 0.0010\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.6840\n",
            "Epoch 188: val_loss did not improve from 8.31944\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 15.6840 - val_loss: 8.4691 - lr: 0.0010\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.6650\n",
            "Epoch 189: val_loss did not improve from 8.31944\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 15.6650 - val_loss: 8.3533 - lr: 0.0010\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.6350\n",
            "Epoch 190: val_loss improved from 8.31944 to 8.25417, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 15.6350 - val_loss: 8.2542 - lr: 0.0010\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.6208\n",
            "Epoch 191: val_loss did not improve from 8.25417\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 15.6208 - val_loss: 8.3547 - lr: 0.0010\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.6013\n",
            "Epoch 192: val_loss did not improve from 8.25417\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 15.6013 - val_loss: 8.3069 - lr: 0.0010\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.5798\n",
            "Epoch 193: val_loss improved from 8.25417 to 8.24536, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 15.5798 - val_loss: 8.2454 - lr: 0.0010\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.5587\n",
            "Epoch 194: val_loss did not improve from 8.24536\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 15.5587 - val_loss: 8.2982 - lr: 0.0010\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.5476\n",
            "Epoch 195: val_loss improved from 8.24536 to 8.19192, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 15.5476 - val_loss: 8.1919 - lr: 0.0010\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.5292\n",
            "Epoch 196: val_loss did not improve from 8.19192\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 15.5292 - val_loss: 8.2263 - lr: 0.0010\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.5170\n",
            "Epoch 197: val_loss improved from 8.19192 to 8.18141, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 15.5170 - val_loss: 8.1814 - lr: 0.0010\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.4937\n",
            "Epoch 198: val_loss improved from 8.18141 to 8.17610, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 15.4937 - val_loss: 8.1761 - lr: 0.0010\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.4694\n",
            "Epoch 199: val_loss improved from 8.17610 to 8.16911, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 15.4694 - val_loss: 8.1691 - lr: 0.0010\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.4507\n",
            "Epoch 200: val_loss improved from 8.16911 to 8.15949, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 15.4507 - val_loss: 8.1595 - lr: 0.0010\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.4373\n",
            "Epoch 201: val_loss improved from 8.15949 to 8.15200, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 15.4373 - val_loss: 8.1520 - lr: 0.0010\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.4172\n",
            "Epoch 202: val_loss improved from 8.15200 to 8.08680, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 15.4172 - val_loss: 8.0868 - lr: 0.0010\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.3959\n",
            "Epoch 203: val_loss did not improve from 8.08680\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 15.3959 - val_loss: 8.1219 - lr: 0.0010\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.3732\n",
            "Epoch 204: val_loss did not improve from 8.08680\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 15.3732 - val_loss: 8.1539 - lr: 0.0010\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.3910\n",
            "Epoch 205: val_loss improved from 8.08680 to 8.01010, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 15.3910 - val_loss: 8.0101 - lr: 0.0010\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.3771\n",
            "Epoch 206: val_loss did not improve from 8.01010\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 15.3771 - val_loss: 8.0581 - lr: 0.0010\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.3509\n",
            "Epoch 207: val_loss improved from 8.01010 to 7.99806, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 15.3509 - val_loss: 7.9981 - lr: 0.0010\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.3313\n",
            "Epoch 208: val_loss improved from 7.99806 to 7.91455, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 15.3313 - val_loss: 7.9146 - lr: 0.0010\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.3106\n",
            "Epoch 209: val_loss did not improve from 7.91455\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 15.3106 - val_loss: 7.9758 - lr: 0.0010\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.2929\n",
            "Epoch 210: val_loss did not improve from 7.91455\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 15.2929 - val_loss: 7.9905 - lr: 0.0010\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.2843\n",
            "Epoch 211: val_loss did not improve from 7.91455\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 15.2843 - val_loss: 7.9951 - lr: 0.0010\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.2743\n",
            "Epoch 212: val_loss did not improve from 7.91455\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 15.2743 - val_loss: 7.9577 - lr: 0.0010\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.2561\n",
            "Epoch 213: val_loss did not improve from 7.91455\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 15.2561 - val_loss: 8.0516 - lr: 0.0010\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.2386\n",
            "Epoch 214: val_loss improved from 7.91455 to 7.83750, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 15.2386 - val_loss: 7.8375 - lr: 0.0010\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.2243\n",
            "Epoch 215: val_loss did not improve from 7.83750\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 15.2243 - val_loss: 7.9552 - lr: 0.0010\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.1907\n",
            "Epoch 216: val_loss did not improve from 7.83750\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 15.1907 - val_loss: 7.9618 - lr: 0.0010\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.1717\n",
            "Epoch 217: val_loss improved from 7.83750 to 7.79737, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 15.1717 - val_loss: 7.7974 - lr: 0.0010\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.1572\n",
            "Epoch 218: val_loss did not improve from 7.79737\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 15.1572 - val_loss: 7.9570 - lr: 0.0010\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.1507\n",
            "Epoch 219: val_loss did not improve from 7.79737\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 15.1507 - val_loss: 7.9020 - lr: 0.0010\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.1627\n",
            "Epoch 220: val_loss did not improve from 7.79737\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 15.1627 - val_loss: 7.8367 - lr: 0.0010\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.1717\n",
            "Epoch 221: val_loss did not improve from 7.79737\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 15.1717 - val_loss: 7.8371 - lr: 0.0010\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.1579\n",
            "Epoch 222: val_loss did not improve from 7.79737\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 15.1579 - val_loss: 7.8785 - lr: 0.0010\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.1196\n",
            "Epoch 223: val_loss improved from 7.79737 to 7.70488, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 15.1196 - val_loss: 7.7049 - lr: 0.0010\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.0814\n",
            "Epoch 224: val_loss did not improve from 7.70488\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 15.0814 - val_loss: 7.8551 - lr: 0.0010\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.0525\n",
            "Epoch 225: val_loss did not improve from 7.70488\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 15.0525 - val_loss: 7.7695 - lr: 0.0010\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.0262\n",
            "Epoch 226: val_loss improved from 7.70488 to 7.68321, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 15.0262 - val_loss: 7.6832 - lr: 0.0010\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.0156\n",
            "Epoch 227: val_loss did not improve from 7.68321\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 15.0156 - val_loss: 7.8494 - lr: 0.0010\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.0167\n",
            "Epoch 228: val_loss did not improve from 7.68321\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 15.0167 - val_loss: 7.6836 - lr: 0.0010\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.0197\n",
            "Epoch 229: val_loss did not improve from 7.68321\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 15.0197 - val_loss: 7.7752 - lr: 0.0010\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.0071\n",
            "Epoch 230: val_loss did not improve from 7.68321\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 15.0071 - val_loss: 7.7601 - lr: 0.0010\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.9980\n",
            "Epoch 231: val_loss did not improve from 7.68321\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 14.9980 - val_loss: 7.8182 - lr: 0.0010\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.9888\n",
            "Epoch 232: val_loss improved from 7.68321 to 7.59064, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 14.9888 - val_loss: 7.5906 - lr: 0.0010\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.9403\n",
            "Epoch 233: val_loss did not improve from 7.59064\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 14.9403 - val_loss: 7.6667 - lr: 0.0010\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.9069\n",
            "Epoch 234: val_loss did not improve from 7.59064\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 14.9069 - val_loss: 7.6224 - lr: 0.0010\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.8901\n",
            "Epoch 235: val_loss improved from 7.59064 to 7.53721, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 341ms/step - loss: 14.8901 - val_loss: 7.5372 - lr: 0.0010\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.8870\n",
            "Epoch 236: val_loss did not improve from 7.53721\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 14.8870 - val_loss: 7.6861 - lr: 0.0010\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.8819\n",
            "Epoch 237: val_loss did not improve from 7.53721\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 14.8819 - val_loss: 7.6526 - lr: 0.0010\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.8729\n",
            "Epoch 238: val_loss did not improve from 7.53721\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 14.8729 - val_loss: 7.6403 - lr: 0.0010\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.8567\n",
            "Epoch 239: val_loss did not improve from 7.53721\n",
            "1/1 [==============================] - 0s 384ms/step - loss: 14.8567 - val_loss: 7.5991 - lr: 0.0010\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.8292\n",
            "Epoch 240: val_loss did not improve from 7.53721\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 14.8292 - val_loss: 7.6135 - lr: 0.0010\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.8108\n",
            "Epoch 241: val_loss improved from 7.53721 to 7.43688, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 14.8108 - val_loss: 7.4369 - lr: 0.0010\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7995\n",
            "Epoch 242: val_loss did not improve from 7.43688\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 14.7995 - val_loss: 7.5531 - lr: 0.0010\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7803\n",
            "Epoch 243: val_loss did not improve from 7.43688\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 14.7803 - val_loss: 7.4946 - lr: 0.0010\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7617\n",
            "Epoch 244: val_loss did not improve from 7.43688\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 14.7617 - val_loss: 7.4463 - lr: 0.0010\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7524\n",
            "Epoch 245: val_loss did not improve from 7.43688\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 14.7524 - val_loss: 7.5875 - lr: 0.0010\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7387\n",
            "Epoch 246: val_loss did not improve from 7.43688\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 14.7387 - val_loss: 7.4927 - lr: 0.0010\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7219\n",
            "Epoch 247: val_loss did not improve from 7.43688\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 14.7219 - val_loss: 7.4578 - lr: 0.0010\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7102\n",
            "Epoch 248: val_loss did not improve from 7.43688\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 14.7102 - val_loss: 7.5147 - lr: 0.0010\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.6998\n",
            "Epoch 249: val_loss did not improve from 7.43688\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 14.6998 - val_loss: 7.5169 - lr: 0.0010\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.6946\n",
            "Epoch 250: val_loss improved from 7.43688 to 7.35365, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 14.6946 - val_loss: 7.3536 - lr: 0.0010\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.6857\n",
            "Epoch 251: val_loss did not improve from 7.35365\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 14.6857 - val_loss: 7.5512 - lr: 0.0010\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.6959\n",
            "Epoch 252: val_loss did not improve from 7.35365\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 14.6959 - val_loss: 7.3798 - lr: 0.0010\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7169\n",
            "Epoch 253: val_loss did not improve from 7.35365\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 14.7169 - val_loss: 7.4878 - lr: 0.0010\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7411\n",
            "Epoch 254: val_loss did not improve from 7.35365\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 14.7411 - val_loss: 7.5627 - lr: 0.0010\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7914\n",
            "Epoch 255: val_loss did not improve from 7.35365\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 14.7914 - val_loss: 7.5912 - lr: 0.0010\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7798\n",
            "Epoch 256: val_loss did not improve from 7.35365\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 14.7798 - val_loss: 7.4255 - lr: 0.0010\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7173\n",
            "Epoch 257: val_loss did not improve from 7.35365\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 14.7173 - val_loss: 7.5180 - lr: 0.0010\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.5989\n",
            "Epoch 258: val_loss did not improve from 7.35365\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 14.5989 - val_loss: 7.3537 - lr: 0.0010\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.5634\n",
            "Epoch 259: val_loss improved from 7.35365 to 7.34394, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 14.5634 - val_loss: 7.3439 - lr: 0.0010\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.6102\n",
            "Epoch 260: val_loss did not improve from 7.34394\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 14.6102 - val_loss: 7.5138 - lr: 0.0010\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.6240\n",
            "Epoch 261: val_loss did not improve from 7.34394\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 14.6240 - val_loss: 7.4119 - lr: 0.0010\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.6000\n",
            "Epoch 262: val_loss improved from 7.34394 to 7.18971, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 14.6000 - val_loss: 7.1897 - lr: 0.0010\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.5197\n",
            "Epoch 263: val_loss did not improve from 7.18971\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 14.5197 - val_loss: 7.2591 - lr: 0.0010\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.5129\n",
            "Epoch 264: val_loss did not improve from 7.18971\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 14.5129 - val_loss: 7.2402 - lr: 0.0010\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.5551\n",
            "Epoch 265: val_loss improved from 7.18971 to 7.18096, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 14.5551 - val_loss: 7.1810 - lr: 0.0010\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.5660\n",
            "Epoch 266: val_loss did not improve from 7.18096\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 14.5660 - val_loss: 7.2836 - lr: 0.0010\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.5354\n",
            "Epoch 267: val_loss did not improve from 7.18096\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 14.5354 - val_loss: 7.3899 - lr: 0.0010\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.4869\n",
            "Epoch 268: val_loss did not improve from 7.18096\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 14.4869 - val_loss: 7.2270 - lr: 0.0010\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.4603\n",
            "Epoch 269: val_loss did not improve from 7.18096\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 14.4603 - val_loss: 7.3184 - lr: 0.0010\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.4405\n",
            "Epoch 270: val_loss did not improve from 7.18096\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 14.4405 - val_loss: 7.2809 - lr: 0.0010\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.4376\n",
            "Epoch 271: val_loss improved from 7.18096 to 7.08612, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 14.4376 - val_loss: 7.0861 - lr: 0.0010\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.4416\n",
            "Epoch 272: val_loss did not improve from 7.08612\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 14.4416 - val_loss: 7.2266 - lr: 0.0010\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.4113\n",
            "Epoch 273: val_loss did not improve from 7.08612\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 14.4113 - val_loss: 7.2118 - lr: 0.0010\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.3821\n",
            "Epoch 274: val_loss did not improve from 7.08612\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 14.3821 - val_loss: 7.0930 - lr: 0.0010\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.3716\n",
            "Epoch 275: val_loss did not improve from 7.08612\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 14.3716 - val_loss: 7.2538 - lr: 0.0010\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.3589\n",
            "Epoch 276: val_loss did not improve from 7.08612\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 14.3589 - val_loss: 7.2130 - lr: 0.0010\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.3596\n",
            "Epoch 277: val_loss did not improve from 7.08612\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 14.3596 - val_loss: 7.1280 - lr: 0.0010\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.3851\n",
            "Epoch 278: val_loss did not improve from 7.08612\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 14.3851 - val_loss: 7.3094 - lr: 0.0010\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.4118\n",
            "Epoch 279: val_loss did not improve from 7.08612\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 14.4118 - val_loss: 7.1883 - lr: 0.0010\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.4161\n",
            "Epoch 280: val_loss did not improve from 7.08612\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 14.4161 - val_loss: 7.2185 - lr: 0.0010\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.3730\n",
            "Epoch 281: val_loss did not improve from 7.08612\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 14.3730 - val_loss: 7.3356 - lr: 0.0010\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.3926\n",
            "Epoch 282: val_loss improved from 7.08612 to 7.07958, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 14.3926 - val_loss: 7.0796 - lr: 0.0010\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.3325\n",
            "Epoch 283: val_loss improved from 7.07958 to 7.02924, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 14.3325 - val_loss: 7.0292 - lr: 0.0010\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.2560\n",
            "Epoch 284: val_loss improved from 7.02924 to 7.01774, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 14.2560 - val_loss: 7.0177 - lr: 0.0010\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.2632\n",
            "Epoch 285: val_loss improved from 7.01774 to 6.91600, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 14.2632 - val_loss: 6.9160 - lr: 0.0010\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.2579\n",
            "Epoch 286: val_loss did not improve from 6.91600\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 14.2579 - val_loss: 7.0198 - lr: 0.0010\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.2462\n",
            "Epoch 287: val_loss did not improve from 6.91600\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 14.2462 - val_loss: 7.1050 - lr: 0.0010\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.2415\n",
            "Epoch 288: val_loss did not improve from 6.91600\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 14.2415 - val_loss: 7.0838 - lr: 0.0010\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.2300\n",
            "Epoch 289: val_loss did not improve from 6.91600\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 14.2300 - val_loss: 7.0566 - lr: 0.0010\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.2027\n",
            "Epoch 290: val_loss did not improve from 6.91600\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 14.2027 - val_loss: 7.0343 - lr: 0.0010\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1844\n",
            "Epoch 291: val_loss improved from 6.91600 to 6.86671, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 14.1844 - val_loss: 6.8667 - lr: 0.0010\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1726\n",
            "Epoch 292: val_loss did not improve from 6.86671\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 14.1726 - val_loss: 6.9539 - lr: 0.0010\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1553\n",
            "Epoch 293: val_loss did not improve from 6.86671\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 14.1553 - val_loss: 6.9711 - lr: 0.0010\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1409\n",
            "Epoch 294: val_loss did not improve from 6.86671\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 14.1409 - val_loss: 6.9489 - lr: 0.0010\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1274\n",
            "Epoch 295: val_loss did not improve from 6.86671\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 14.1274 - val_loss: 7.0296 - lr: 0.0010\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1593\n",
            "Epoch 296: val_loss did not improve from 6.86671\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 14.1593 - val_loss: 6.9437 - lr: 0.0010\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1175\n",
            "Epoch 297: val_loss improved from 6.86671 to 6.83137, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 14.1175 - val_loss: 6.8314 - lr: 0.0010\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1122\n",
            "Epoch 298: val_loss improved from 6.83137 to 6.79646, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 14.1122 - val_loss: 6.7965 - lr: 0.0010\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1156\n",
            "Epoch 299: val_loss improved from 6.79646 to 6.76389, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 14.1156 - val_loss: 6.7639 - lr: 0.0010\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0994\n",
            "Epoch 300: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 14.0994 - val_loss: 6.8720 - lr: 0.0010\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0792\n",
            "Epoch 301: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 14.0792 - val_loss: 6.9722 - lr: 0.0010\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0708\n",
            "Epoch 302: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 14.0708 - val_loss: 6.9290 - lr: 0.0010\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0597\n",
            "Epoch 303: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 14.0597 - val_loss: 6.9665 - lr: 0.0010\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0395\n",
            "Epoch 304: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 14.0395 - val_loss: 6.8469 - lr: 0.0010\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0303\n",
            "Epoch 305: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 14.0303 - val_loss: 6.7781 - lr: 0.0010\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0236\n",
            "Epoch 306: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 14.0236 - val_loss: 6.8127 - lr: 0.0010\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0147\n",
            "Epoch 307: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 14.0147 - val_loss: 6.7744 - lr: 0.0010\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0201\n",
            "Epoch 308: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 14.0201 - val_loss: 6.8291 - lr: 0.0010\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0448\n",
            "Epoch 309: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 14.0448 - val_loss: 6.9383 - lr: 0.0010\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0771\n",
            "Epoch 310: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 14.0771 - val_loss: 6.9698 - lr: 0.0010\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1428\n",
            "Epoch 311: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 14.1428 - val_loss: 7.0395 - lr: 0.0010\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1823\n",
            "Epoch 312: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 14.1823 - val_loss: 7.0247 - lr: 0.0010\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.2057\n",
            "Epoch 313: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 14.2057 - val_loss: 6.9211 - lr: 0.0010\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0958\n",
            "Epoch 314: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 14.0958 - val_loss: 6.7794 - lr: 0.0010\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.9772\n",
            "Epoch 315: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 13.9772 - val_loss: 6.7980 - lr: 0.0010\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.9055\n",
            "Epoch 316: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 13.9055 - val_loss: 6.8929 - lr: 0.0010\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.9706\n",
            "Epoch 317: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 13.9706 - val_loss: 6.7969 - lr: 0.0010\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.0568\n",
            "Epoch 318: val_loss did not improve from 6.76389\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 14.0568 - val_loss: 6.7974 - lr: 0.0010\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.9774\n",
            "Epoch 319: val_loss improved from 6.76389 to 6.75662, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 13.9774 - val_loss: 6.7566 - lr: 0.0010\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.8864\n",
            "Epoch 320: val_loss improved from 6.75662 to 6.61119, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 13.8864 - val_loss: 6.6112 - lr: 0.0010\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.8816\n",
            "Epoch 321: val_loss did not improve from 6.61119\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 13.8816 - val_loss: 6.7820 - lr: 0.0010\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.9104\n",
            "Epoch 322: val_loss did not improve from 6.61119\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 13.9104 - val_loss: 6.8530 - lr: 0.0010\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.9114\n",
            "Epoch 323: val_loss did not improve from 6.61119\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 13.9114 - val_loss: 6.6194 - lr: 0.0010\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.8658\n",
            "Epoch 324: val_loss did not improve from 6.61119\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 13.8658 - val_loss: 6.8176 - lr: 0.0010\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.8272\n",
            "Epoch 325: val_loss did not improve from 6.61119\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 13.8272 - val_loss: 6.7031 - lr: 0.0010\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.8348\n",
            "Epoch 326: val_loss did not improve from 6.61119\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 13.8348 - val_loss: 6.6273 - lr: 0.0010\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.8574\n",
            "Epoch 327: val_loss did not improve from 6.61119\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 13.8574 - val_loss: 6.6962 - lr: 0.0010\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.8222\n",
            "Epoch 328: val_loss improved from 6.61119 to 6.60345, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 13.8222 - val_loss: 6.6034 - lr: 0.0010\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.7823\n",
            "Epoch 329: val_loss improved from 6.60345 to 6.51620, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 13.7823 - val_loss: 6.5162 - lr: 0.0010\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.7701\n",
            "Epoch 330: val_loss did not improve from 6.51620\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 13.7701 - val_loss: 6.7227 - lr: 0.0010\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.7763\n",
            "Epoch 331: val_loss did not improve from 6.51620\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 13.7763 - val_loss: 6.6842 - lr: 0.0010\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.7745\n",
            "Epoch 332: val_loss did not improve from 6.51620\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 13.7745 - val_loss: 6.6379 - lr: 0.0010\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.7640\n",
            "Epoch 333: val_loss did not improve from 6.51620\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 13.7640 - val_loss: 6.7082 - lr: 0.0010\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.7390\n",
            "Epoch 334: val_loss improved from 6.51620 to 6.50803, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 13.7390 - val_loss: 6.5080 - lr: 0.0010\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.7236\n",
            "Epoch 335: val_loss did not improve from 6.50803\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 13.7236 - val_loss: 6.5884 - lr: 0.0010\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.7085\n",
            "Epoch 336: val_loss did not improve from 6.50803\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 13.7085 - val_loss: 6.5346 - lr: 0.0010\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.7021\n",
            "Epoch 337: val_loss improved from 6.50803 to 6.49173, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 13.7021 - val_loss: 6.4917 - lr: 0.0010\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6934\n",
            "Epoch 338: val_loss did not improve from 6.49173\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 13.6934 - val_loss: 6.6285 - lr: 0.0010\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6755\n",
            "Epoch 339: val_loss did not improve from 6.49173\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 13.6755 - val_loss: 6.6275 - lr: 0.0010\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6997\n",
            "Epoch 340: val_loss did not improve from 6.49173\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 13.6997 - val_loss: 6.5080 - lr: 0.0010\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6618\n",
            "Epoch 341: val_loss improved from 6.49173 to 6.44463, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 13.6618 - val_loss: 6.4446 - lr: 0.0010\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6630\n",
            "Epoch 342: val_loss improved from 6.44463 to 6.38577, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 13.6630 - val_loss: 6.3858 - lr: 0.0010\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6663\n",
            "Epoch 343: val_loss improved from 6.38577 to 6.37074, saving model to best_model_1_treating_as_real.h5\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 13.6663 - val_loss: 6.3707 - lr: 0.0010\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6546\n",
            "Epoch 344: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 13.6546 - val_loss: 6.4826 - lr: 0.0010\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6437\n",
            "Epoch 345: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 13.6437 - val_loss: 6.5160 - lr: 0.0010\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6310\n",
            "Epoch 346: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 13.6310 - val_loss: 6.5329 - lr: 0.0010\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6154\n",
            "Epoch 347: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 13.6154 - val_loss: 6.5332 - lr: 0.0010\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6015\n",
            "Epoch 348: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 13.6015 - val_loss: 6.4304 - lr: 0.0010\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5929\n",
            "Epoch 349: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 13.5929 - val_loss: 6.4103 - lr: 0.0010\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5823\n",
            "Epoch 350: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 13.5823 - val_loss: 6.4937 - lr: 0.0010\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5764\n",
            "Epoch 351: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 13.5764 - val_loss: 6.4842 - lr: 1.0000e-04\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5717\n",
            "Epoch 352: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 13.5717 - val_loss: 6.4624 - lr: 1.0000e-04\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5646\n",
            "Epoch 353: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 13.5646 - val_loss: 6.4417 - lr: 1.0000e-04\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5609\n",
            "Epoch 354: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 13.5609 - val_loss: 6.4310 - lr: 1.0000e-04\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5603\n",
            "Epoch 355: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 13.5603 - val_loss: 6.4313 - lr: 1.0000e-04\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5598\n",
            "Epoch 356: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 13.5598 - val_loss: 6.4345 - lr: 1.0000e-04\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5587\n",
            "Epoch 357: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 13.5587 - val_loss: 6.4374 - lr: 1.0000e-04\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5562\n",
            "Epoch 358: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 13.5562 - val_loss: 6.4402 - lr: 1.0000e-04\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5531\n",
            "Epoch 359: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 13.5531 - val_loss: 6.4411 - lr: 1.0000e-04\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5507\n",
            "Epoch 360: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 13.5507 - val_loss: 6.4397 - lr: 1.0000e-04\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5504\n",
            "Epoch 361: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 13.5504 - val_loss: 6.4462 - lr: 1.0000e-04\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5501\n",
            "Epoch 362: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 13.5501 - val_loss: 6.4566 - lr: 1.0000e-04\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5493\n",
            "Epoch 363: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 13.5493 - val_loss: 6.4660 - lr: 1.0000e-04\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5481\n",
            "Epoch 364: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 13.5481 - val_loss: 6.4709 - lr: 1.0000e-04\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5465\n",
            "Epoch 365: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 13.5465 - val_loss: 6.4704 - lr: 1.0000e-04\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5442\n",
            "Epoch 366: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 13.5442 - val_loss: 6.4661 - lr: 1.0000e-04\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5418\n",
            "Epoch 367: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 13.5418 - val_loss: 6.4590 - lr: 1.0000e-04\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5415\n",
            "Epoch 368: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 13.5415 - val_loss: 6.4413 - lr: 1.0000e-04\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5406\n",
            "Epoch 369: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 13.5406 - val_loss: 6.4199 - lr: 1.0000e-04\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5387\n",
            "Epoch 370: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 13.5387 - val_loss: 6.4007 - lr: 1.0000e-04\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5388\n",
            "Epoch 371: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 13.5388 - val_loss: 6.4005 - lr: 1.0000e-04\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5379\n",
            "Epoch 372: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 13.5379 - val_loss: 6.4162 - lr: 1.0000e-04\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5355\n",
            "Epoch 373: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 13.5355 - val_loss: 6.4412 - lr: 1.0000e-04\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5354\n",
            "Epoch 374: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 13.5354 - val_loss: 6.4537 - lr: 1.0000e-04\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5349\n",
            "Epoch 375: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 13.5349 - val_loss: 6.4497 - lr: 1.0000e-04\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5325\n",
            "Epoch 376: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 13.5325 - val_loss: 6.4323 - lr: 1.0000e-04\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5314\n",
            "Epoch 377: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 13.5314 - val_loss: 6.4204 - lr: 1.0000e-04\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5309\n",
            "Epoch 378: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 13.5309 - val_loss: 6.4175 - lr: 1.0000e-04\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5295\n",
            "Epoch 379: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 13.5295 - val_loss: 6.4239 - lr: 1.0000e-04\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5288\n",
            "Epoch 380: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 13.5288 - val_loss: 6.4362 - lr: 1.0000e-04\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5275\n",
            "Epoch 381: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 13.5275 - val_loss: 6.4505 - lr: 1.0000e-04\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5258\n",
            "Epoch 382: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 13.5258 - val_loss: 6.4550 - lr: 1.0000e-04\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5250\n",
            "Epoch 383: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 13.5250 - val_loss: 6.4440 - lr: 1.0000e-04\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5225\n",
            "Epoch 384: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 13.5225 - val_loss: 6.4293 - lr: 1.0000e-04\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5221\n",
            "Epoch 385: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 13.5221 - val_loss: 6.4211 - lr: 1.0000e-04\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5214\n",
            "Epoch 386: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 13.5214 - val_loss: 6.4206 - lr: 1.0000e-04\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5200\n",
            "Epoch 387: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 13.5200 - val_loss: 6.4161 - lr: 1.0000e-04\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5189\n",
            "Epoch 388: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 387ms/step - loss: 13.5189 - val_loss: 6.4205 - lr: 1.0000e-04\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5182\n",
            "Epoch 389: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 13.5182 - val_loss: 6.4192 - lr: 1.0000e-04\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5167\n",
            "Epoch 390: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 13.5167 - val_loss: 6.4180 - lr: 1.0000e-04\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5156\n",
            "Epoch 391: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 13.5156 - val_loss: 6.4241 - lr: 1.0000e-04\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5145\n",
            "Epoch 392: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 13.5145 - val_loss: 6.4255 - lr: 1.0000e-04\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5138\n",
            "Epoch 393: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 13.5138 - val_loss: 6.4261 - lr: 1.0000e-04\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5126\n",
            "Epoch 394: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 13.5126 - val_loss: 6.4207 - lr: 1.0000e-04\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5112\n",
            "Epoch 395: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 13.5112 - val_loss: 6.4173 - lr: 1.0000e-04\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5103\n",
            "Epoch 396: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 13.5103 - val_loss: 6.4096 - lr: 1.0000e-04\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5096\n",
            "Epoch 397: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 13.5096 - val_loss: 6.4050 - lr: 1.0000e-04\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5084\n",
            "Epoch 398: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 13.5084 - val_loss: 6.4045 - lr: 1.0000e-04\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5077\n",
            "Epoch 399: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 13.5077 - val_loss: 6.4097 - lr: 1.0000e-04\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5067\n",
            "Epoch 400: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 376ms/step - loss: 13.5067 - val_loss: 6.4182 - lr: 1.0000e-04\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5054\n",
            "Epoch 401: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 13.5054 - val_loss: 6.4191 - lr: 1.0000e-05\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5053\n",
            "Epoch 402: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 13.5053 - val_loss: 6.4200 - lr: 1.0000e-05\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5051\n",
            "Epoch 403: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 13.5051 - val_loss: 6.4208 - lr: 1.0000e-05\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5048\n",
            "Epoch 404: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 13.5048 - val_loss: 6.4216 - lr: 1.0000e-05\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5045\n",
            "Epoch 405: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 13.5045 - val_loss: 6.4223 - lr: 1.0000e-05\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5042\n",
            "Epoch 406: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 13.5042 - val_loss: 6.4233 - lr: 1.0000e-05\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5040\n",
            "Epoch 407: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 13.5040 - val_loss: 6.4246 - lr: 1.0000e-05\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5037\n",
            "Epoch 408: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 13.5037 - val_loss: 6.4262 - lr: 1.0000e-05\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5037\n",
            "Epoch 409: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 372ms/step - loss: 13.5037 - val_loss: 6.4273 - lr: 1.0000e-05\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5036\n",
            "Epoch 410: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 13.5036 - val_loss: 6.4278 - lr: 1.0000e-05\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5034\n",
            "Epoch 411: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 13.5034 - val_loss: 6.4279 - lr: 1.0000e-05\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5032\n",
            "Epoch 412: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 13.5032 - val_loss: 6.4271 - lr: 1.0000e-05\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5029\n",
            "Epoch 413: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 13.5029 - val_loss: 6.4257 - lr: 1.0000e-05\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5026\n",
            "Epoch 414: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 13.5026 - val_loss: 6.4238 - lr: 1.0000e-05\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5023\n",
            "Epoch 415: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 13.5023 - val_loss: 6.4216 - lr: 1.0000e-05\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5021\n",
            "Epoch 416: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 13.5021 - val_loss: 6.4197 - lr: 1.0000e-05\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5019\n",
            "Epoch 417: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 13.5019 - val_loss: 6.4184 - lr: 1.0000e-05\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5017\n",
            "Epoch 418: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 13.5017 - val_loss: 6.4174 - lr: 1.0000e-05\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5016\n",
            "Epoch 419: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 13.5016 - val_loss: 6.4173 - lr: 1.0000e-05\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5017\n",
            "Epoch 420: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 13.5017 - val_loss: 6.4178 - lr: 1.0000e-05\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5016\n",
            "Epoch 421: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 13.5016 - val_loss: 6.4184 - lr: 1.0000e-05\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5015\n",
            "Epoch 422: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 13.5015 - val_loss: 6.4191 - lr: 1.0000e-05\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5014\n",
            "Epoch 423: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 13.5014 - val_loss: 6.4197 - lr: 1.0000e-05\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5011\n",
            "Epoch 424: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 13.5011 - val_loss: 6.4199 - lr: 1.0000e-05\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5008\n",
            "Epoch 425: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 13.5008 - val_loss: 6.4196 - lr: 1.0000e-05\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5006\n",
            "Epoch 426: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 13.5006 - val_loss: 6.4184 - lr: 1.0000e-05\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5006\n",
            "Epoch 427: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 13.5006 - val_loss: 6.4172 - lr: 1.0000e-05\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5006\n",
            "Epoch 428: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 13.5006 - val_loss: 6.4169 - lr: 1.0000e-05\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5005\n",
            "Epoch 429: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 13.5005 - val_loss: 6.4174 - lr: 1.0000e-05\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5004\n",
            "Epoch 430: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 13.5004 - val_loss: 6.4179 - lr: 1.0000e-05\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5003\n",
            "Epoch 431: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 13.5003 - val_loss: 6.4182 - lr: 1.0000e-05\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.5001\n",
            "Epoch 432: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 13.5001 - val_loss: 6.4180 - lr: 1.0000e-05\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4999\n",
            "Epoch 433: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 13.4999 - val_loss: 6.4182 - lr: 1.0000e-05\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4997\n",
            "Epoch 434: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 13.4997 - val_loss: 6.4183 - lr: 1.0000e-05\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4996\n",
            "Epoch 435: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 13.4996 - val_loss: 6.4186 - lr: 1.0000e-05\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4995\n",
            "Epoch 436: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 13.4995 - val_loss: 6.4189 - lr: 1.0000e-05\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4994\n",
            "Epoch 437: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 13.4994 - val_loss: 6.4183 - lr: 1.0000e-05\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4994\n",
            "Epoch 438: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 13.4994 - val_loss: 6.4180 - lr: 1.0000e-05\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4993\n",
            "Epoch 439: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 13.4993 - val_loss: 6.4179 - lr: 1.0000e-05\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4991\n",
            "Epoch 440: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 13.4991 - val_loss: 6.4186 - lr: 1.0000e-05\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4991\n",
            "Epoch 441: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 13.4991 - val_loss: 6.4191 - lr: 1.0000e-05\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4990\n",
            "Epoch 442: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 13.4990 - val_loss: 6.4190 - lr: 1.0000e-05\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4989\n",
            "Epoch 443: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 13.4989 - val_loss: 6.4184 - lr: 1.0000e-05\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4987\n",
            "Epoch 444: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 13.4987 - val_loss: 6.4172 - lr: 1.0000e-05\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4986\n",
            "Epoch 445: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 13.4986 - val_loss: 6.4162 - lr: 1.0000e-05\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4985\n",
            "Epoch 446: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 13.4985 - val_loss: 6.4160 - lr: 1.0000e-05\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4985\n",
            "Epoch 447: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 13.4985 - val_loss: 6.4160 - lr: 1.0000e-05\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4984\n",
            "Epoch 448: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 13.4984 - val_loss: 6.4162 - lr: 1.0000e-05\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4983\n",
            "Epoch 449: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 13.4983 - val_loss: 6.4167 - lr: 1.0000e-05\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4981\n",
            "Epoch 450: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 13.4981 - val_loss: 6.4174 - lr: 1.0000e-05\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4979\n",
            "Epoch 451: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 13.4979 - val_loss: 6.4174 - lr: 1.0000e-06\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4979\n",
            "Epoch 452: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 13.4979 - val_loss: 6.4175 - lr: 1.0000e-06\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4979\n",
            "Epoch 453: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 13.4979 - val_loss: 6.4174 - lr: 1.0000e-06\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4978\n",
            "Epoch 454: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 13.4978 - val_loss: 6.4174 - lr: 1.0000e-06\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4978\n",
            "Epoch 455: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 13.4978 - val_loss: 6.4173 - lr: 1.0000e-06\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4978\n",
            "Epoch 456: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 13.4978 - val_loss: 6.4172 - lr: 1.0000e-06\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4978\n",
            "Epoch 457: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 13.4978 - val_loss: 6.4172 - lr: 1.0000e-06\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4978\n",
            "Epoch 458: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 13.4978 - val_loss: 6.4171 - lr: 1.0000e-06\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4978\n",
            "Epoch 459: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 13.4978 - val_loss: 6.4171 - lr: 1.0000e-06\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4978\n",
            "Epoch 460: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 13.4978 - val_loss: 6.4170 - lr: 1.0000e-06\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4977\n",
            "Epoch 461: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 13.4977 - val_loss: 6.4170 - lr: 1.0000e-06\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4977\n",
            "Epoch 462: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 13.4977 - val_loss: 6.4171 - lr: 1.0000e-06\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4977\n",
            "Epoch 463: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 13.4977 - val_loss: 6.4172 - lr: 1.0000e-06\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4977\n",
            "Epoch 464: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 13.4977 - val_loss: 6.4173 - lr: 1.0000e-06\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4977\n",
            "Epoch 465: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 13.4977 - val_loss: 6.4173 - lr: 1.0000e-06\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4976\n",
            "Epoch 466: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 13.4976 - val_loss: 6.4173 - lr: 1.0000e-06\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4976\n",
            "Epoch 467: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 13.4976 - val_loss: 6.4172 - lr: 1.0000e-06\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4976\n",
            "Epoch 468: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 13.4976 - val_loss: 6.4172 - lr: 1.0000e-06\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4976\n",
            "Epoch 469: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 13.4976 - val_loss: 6.4171 - lr: 1.0000e-06\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4976\n",
            "Epoch 470: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 13.4976 - val_loss: 6.4171 - lr: 1.0000e-06\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4976\n",
            "Epoch 471: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 13.4976 - val_loss: 6.4171 - lr: 1.0000e-06\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4976\n",
            "Epoch 472: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 13.4976 - val_loss: 6.4171 - lr: 1.0000e-06\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4976\n",
            "Epoch 473: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 13.4976 - val_loss: 6.4171 - lr: 1.0000e-06\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4976\n",
            "Epoch 474: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 13.4976 - val_loss: 6.4171 - lr: 1.0000e-06\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4975\n",
            "Epoch 475: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 13.4975 - val_loss: 6.4170 - lr: 1.0000e-06\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4975\n",
            "Epoch 476: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 13.4975 - val_loss: 6.4169 - lr: 1.0000e-06\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4975\n",
            "Epoch 477: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 13.4975 - val_loss: 6.4168 - lr: 1.0000e-06\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4975\n",
            "Epoch 478: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 13.4975 - val_loss: 6.4169 - lr: 1.0000e-06\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4975\n",
            "Epoch 479: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 13.4975 - val_loss: 6.4170 - lr: 1.0000e-06\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4975\n",
            "Epoch 480: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 13.4975 - val_loss: 6.4170 - lr: 1.0000e-06\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4975\n",
            "Epoch 481: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 13.4975 - val_loss: 6.4168 - lr: 1.0000e-06\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4975\n",
            "Epoch 482: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 13.4975 - val_loss: 6.4167 - lr: 1.0000e-06\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4974\n",
            "Epoch 483: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 13.4974 - val_loss: 6.4167 - lr: 1.0000e-06\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4974\n",
            "Epoch 484: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 13.4974 - val_loss: 6.4168 - lr: 1.0000e-06\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4974\n",
            "Epoch 485: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 13.4974 - val_loss: 6.4169 - lr: 1.0000e-06\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4974\n",
            "Epoch 486: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 13.4974 - val_loss: 6.4169 - lr: 1.0000e-06\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4974\n",
            "Epoch 487: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 13.4974 - val_loss: 6.4167 - lr: 1.0000e-06\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4974\n",
            "Epoch 488: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 13.4974 - val_loss: 6.4167 - lr: 1.0000e-06\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4974\n",
            "Epoch 489: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 13.4974 - val_loss: 6.4167 - lr: 1.0000e-06\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4974\n",
            "Epoch 490: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 13.4974 - val_loss: 6.4168 - lr: 1.0000e-06\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4974\n",
            "Epoch 491: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 13.4974 - val_loss: 6.4168 - lr: 1.0000e-06\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4973\n",
            "Epoch 492: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 13.4973 - val_loss: 6.4168 - lr: 1.0000e-06\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4973\n",
            "Epoch 493: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 13.4973 - val_loss: 6.4167 - lr: 1.0000e-06\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4973\n",
            "Epoch 494: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 13.4973 - val_loss: 6.4166 - lr: 1.0000e-06\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4973\n",
            "Epoch 495: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 13.4973 - val_loss: 6.4166 - lr: 1.0000e-06\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4973\n",
            "Epoch 496: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 13.4973 - val_loss: 6.4166 - lr: 1.0000e-06\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4973\n",
            "Epoch 497: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 13.4973 - val_loss: 6.4166 - lr: 1.0000e-06\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4973\n",
            "Epoch 498: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 13.4973 - val_loss: 6.4167 - lr: 1.0000e-06\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4973\n",
            "Epoch 499: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 13.4973 - val_loss: 6.4168 - lr: 1.0000e-06\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4973\n",
            "Epoch 500: val_loss did not improve from 6.37074\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 13.4973 - val_loss: 6.4167 - lr: 1.0000e-06\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6.4167\n",
            "Test Loss: 6.416733741760254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 14ms/step - loss: 6.3707\n",
            "Test Loss (MSE) with Best Model: 6.370741844177246\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Build the model\n",
        "model_1 = Sequential([\n",
        "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(4004,)),\n",
        "    # Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    # Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    Dense(3)  # Martina: Pls change here as per the number of outputs\n",
        "])\n",
        "\n",
        "def learning_rate_schedule(epoch, initial_lr=0.001):\n",
        "    \"\"\"\n",
        "    Custom learning rate schedule. Adjust the learning rate based on the current epoch.\n",
        "    You can customize this function to fit your needs.\n",
        "    \"\"\"\n",
        "    if epoch < 300 or epoch == 300:\n",
        "        return initial_lr  # Keep the initial learning rate for the first 50 epochs\n",
        "    elif epoch > 300 and epoch%50 == 0:\n",
        "        initial_lr = initial_lr * 0.1  # Reduce the learning rate by a factor of 10 after epoch 50\n",
        "        return initial_lr\n",
        "    else:\n",
        "        return initial_lr\n",
        "\n",
        "\n",
        "# Create the Adam optimizer with the initial learning rate\n",
        "initial_learning_rate = 0.001\n",
        "adam_optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(optimizer=adam_optimizer, loss='mean_squared_error')\n",
        "model_1.summary()\n",
        "\n",
        "checkpoint_1 = ModelCheckpoint('best_model_1_treating_as_real.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "model_1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[checkpoint_1, lr_scheduler])\n",
        "\n",
        "loss_1 = model_1.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss_1)\n",
        "\n",
        "predictions_1 = model_1.predict(X_test)\n",
        "\n",
        "errors_1 = np.abs(predictions_1 - y_test)\n",
        "\n",
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model('best_model_1_treating_as_real.h5')\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "test_loss = best_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss (MSE) with Best Model: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb3j4vLY5bDf"
      },
      "outputs": [],
      "source": [
        "#0.05585065856575966"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BIq-q2KA59B",
        "outputId": "1c5a1983-4a8b-4b4e-e308-4a96c2c6c1ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 11ms/step\n"
          ]
        }
      ],
      "source": [
        "preds = model_1.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjEQz3v1BB3p",
        "outputId": "313128f5-c814-4048-9b34-8b9cca429a7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 3.0545292 50.082233  12.90647  ]   [ 3 50 13]\n",
            "[ 2.6711228 18.014008   8.156568 ]   [ 3 20  8]\n",
            "[ 3.1834273 64.82521   11.386971 ]   [ 3 70 12]\n",
            "[ 3.4290955 10.055168   8.769251 ]   [ 3 10  8]\n",
            "[ 3.135311 30.03865  12.290965]   [ 4 30 13]\n",
            "[ 2.790952  9.810348 11.741399]   [ 4 10 14]\n",
            "[ 2.8508835 39.744324  12.951585 ]   [ 4 40 14]\n",
            "[ 2.9536612 18.89312    9.174626 ]   [ 3 20  9]\n",
            "[ 3.3316994 66.66545   10.235998 ]   [ 4 70 11]\n",
            "[ 2.8741784 39.91478    8.949702 ]   [ 4 40  9]\n",
            "[3.7829468 9.987406  7.100745 ]   [ 4 10  7]\n",
            "[ 3.1114898 30.13899   13.266742 ]   [ 3 30 14]\n",
            "[ 3.0118828 62.783222  12.626582 ]   [ 2 70 13]\n",
            "[ 3.1851735 30.051321   9.552537 ]   [ 2 30  9]\n",
            "[ 2.9984326 50.0824    10.451828 ]   [ 2 50 10]\n",
            "[ 2.6830993  9.982358  12.150462 ]   [ 2 10 11]\n",
            "[ 3.199751 29.99982   8.53543 ]   [ 2 30  8]\n",
            "[ 2.8600533 39.935562  12.015869 ]   [ 2 40 12]\n",
            "[ 2.9853578 49.87052   10.188657 ]   [ 3 50 10]\n",
            "[ 3.1311662 30.110714  12.150926 ]   [ 2 30 12]\n",
            "[ 2.7322798 49.72092    7.143811 ]   [ 4 50  7]\n",
            "[ 2.2890131 16.788605   6.7875624]   [ 4 20  7]\n",
            "[ 3.2114697 29.957792   8.3630905]   [ 3 30  8]\n",
            "[ 2.8290281 39.998913   7.121045 ]   [ 4 40  7]\n",
            "[ 2.7630353 49.85157    7.229025 ]   [ 3 50  7]\n",
            "[ 2.3549695 17.093681   7.0714893]   [ 3 20  7]\n",
            "[ 2.9797392 49.802624   9.99322  ]   [ 4 50 10]\n",
            "[ 3.3116312 66.42301   10.41697  ]   [ 3 70 11]\n",
            "[ 3.113863 60.08431  10.045245]   [ 3 60 10]\n",
            "[ 3.072095 60.39407   8.953318]   [ 3 60  9]\n",
            "[ 2.9233966 70.33008    8.0254545]   [ 4 70  8]\n",
            "[ 3.1788836 69.172035   8.867574 ]   [ 3 70  9]\n",
            "[ 3.238946 59.03963  13.711754]   [ 2 60 14]\n",
            "[ 3.191496 59.516624 12.156914]   [ 2 60 12]\n",
            "[ 3.1250353 60.21676   10.229373 ]   [ 2 60 10]\n",
            "[ 2.8549762 39.831512  12.522486 ]   [ 3 40 13]\n",
            "[ 3.4339023 10.146792   8.746171 ]   [ 2 20  7]\n",
            "[ 3.0198035 19.685247  11.160787 ]   [ 3 20 11]\n",
            "[ 2.9692597 20.176743  12.913416 ]   [ 3 20 13]\n",
            "[ 3.1016636 30.210367  13.52573  ]   [ 2 30 14]\n",
            "[ 3.1541748 59.79085   11.155721 ]   [ 2 60 11]\n",
            "[ 2.9947534 19.948471  12.061156 ]   [ 3 20 12]\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(preds)):\n",
        "  print(preds[i], \" \", y_test[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gM32VW6TuJnT",
        "outputId": "cc8d4fd2-e85b-48bc-8ef0-4a88cb08eb63"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/IEEE EMBS SMP Project/best_model_1.h5'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.copy(\"/content/best_model_1_treating_as_real.h5\", \"/content/drive/MyDrive/IEEE EMBS SMP Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mU-8-qINPvS"
      },
      "source": [
        "Real-valued 3D-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRTQXG3rRb5N"
      },
      "outputs": [],
      "source": [
        "X = mat_data['datanew'][0][0][1]\n",
        "y = mat_data['datanew'][0][0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdgvpO4dWKeS",
        "outputId": "607c008f-6028-4ea2-bd17-cb7ee07a39a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 2, 1001, 169)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRbiNq2TWMxp",
        "outputId": "e3bcfeb1-3022-4357-b217-c6020a2d0c48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(169, 3)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYKzWKf9S--4",
        "outputId": "d2e1c1d3-c238-4edb-bb31-835afbccd6f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"3D-1D-CNN-Regression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_18 (InputLayer)       [(None, 2, 2, 1001)]      0         \n",
            "                                                                 \n",
            " tf.expand_dims_3 (TFOpLamb  (None, 2, 2, 1001, 1)     0         \n",
            " da)                                                             \n",
            "                                                                 \n",
            " conv3d_5 (Conv3D)           (None, 1, 1, 999, 32)     416       \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 1, 1, 999, 32)     128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 31968)             0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1024)              32736256  \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 128)               131200    \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32872227 (125.40 MB)\n",
            "Trainable params: 32872163 (125.40 MB)\n",
            "Non-trainable params: 64 (256.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Expand dimension to make it compatible with convolutional layers\n",
        "    x = tf.expand_dims(inputs, axis=-1)\n",
        "\n",
        "    # 3D Convolutional Layer\n",
        "    x = layers.Conv3D(filters=32, kernel_size=(2, 2, 3), strides=(2, 2, 1), activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Flatten the output of the 3D convolutional layer\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    # 1D Convolutional Layers\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    # x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    # x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    # Fully connected layers for regression\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    outputs = layers.Dense(3, activation=None)(x)  # No activation for regression task\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name=\"3D-1D-CNN-Regression\")\n",
        "    return model\n",
        "\n",
        "input_shape = (2, 2, 1001)\n",
        "\n",
        "model_cnn = create_cnn_model(input_shape)\n",
        "\n",
        "model_cnn.compile(optimizer='adam', loss='mse')\n",
        "model_cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or3ugvc1WBX3",
        "outputId": "dbda0e8e-08ed-470d-af27-5b713acf59e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(169, 2, 2, 1001)\n",
            "(169, 3)\n",
            "X_train shape: (127, 2, 2, 1001)\n",
            "X_test shape: (42, 2, 2, 1001)\n",
            "y_train shape: (127, 3)\n",
            "y_test shape: (42, 3)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = X.transpose(3, 0, 1, 2).reshape(169, 2, 2, 1001)\n",
        "print(X.shape)\n",
        "y = y.transpose()\n",
        "print(y.shape)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.245, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utAJRRZqNWT6"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, Model\n",
        "\n",
        "# def create_cnn_model(input_shape):\n",
        "#     # Define input layer\n",
        "#     inputs = layers.Input(shape=input_shape)\n",
        "#     # print(inputs.shape)\n",
        "\n",
        "#     # Reshape input data to have the correct shape for the first 2D CNN layer\n",
        "#     x = layers.Reshape((2, 2, 1001, 1))(inputs)\n",
        "#     # print(x.shape)\n",
        "\n",
        "#     # First 2D CNN layer\n",
        "#     x = layers.Conv2D(filters=32, kernel_size=2, strides=2, activation='relu')(x)\n",
        "#     print(x.shape)\n",
        "\n",
        "#     # Flatten the output of the first CNN layer\n",
        "#     # x = layers.Flatten()(x)\n",
        "#     # print(x.shape)\n",
        "\n",
        "#     # Add 1D CNN layers\n",
        "#     x = layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
        "#     print(x.shape)\n",
        "#     x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "#     x = layers.Conv1D(filters=128, kernel_size=4, activation='relu')(x)\n",
        "#     x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "#     # Add fully connected layers\n",
        "#     x = layers.Flatten()(x)\n",
        "#     x = layers.Dense(256, activation='relu')(x)\n",
        "#     x = layers.Dropout(0.5)(x)\n",
        "#     x = layers.Dense(128, activation='relu')(x)\n",
        "#     x = layers.Dropout(0.5)(x)\n",
        "\n",
        "#     # Output layer for regression\n",
        "#     outputs = layers.Dense(3, activation='linear')(x)\n",
        "\n",
        "#     # Create the model\n",
        "#     model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "#     return model\n",
        "\n",
        "# # Define input shape\n",
        "# input_shape = (2, 2, 1001)\n",
        "\n",
        "# # Create the model\n",
        "# model = create_cnn_model(input_shape)\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# # Print model summary\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4nGfWDMPO8L",
        "outputId": "5e284ae1-68df-4124-a2c8-7a77b6d59972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r1/1 [==============================] - ETA: 0s - loss: 681.7905"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 544.29767, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 681.7905 - val_loss: 544.2977 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 4559.6914\n",
            "Epoch 2: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4559.6914 - val_loss: 664.8451 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 338.2005\n",
            "Epoch 3: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 338.2005 - val_loss: 680.2263 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 552.3527\n",
            "Epoch 4: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 552.3527 - val_loss: 659.2339 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 286.8929\n",
            "Epoch 5: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 286.8929 - val_loss: 635.6104 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 221.0981\n",
            "Epoch 6: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 221.0981 - val_loss: 623.6511 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 305.7722\n",
            "Epoch 7: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 305.7722 - val_loss: 626.7435 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 231.1437\n",
            "Epoch 8: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 231.1437 - val_loss: 637.5788 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 146.6893\n",
            "Epoch 9: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 146.6893 - val_loss: 649.6378 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 182.0007\n",
            "Epoch 10: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 182.0007 - val_loss: 656.8052 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 229.8079\n",
            "Epoch 11: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 229.8079 - val_loss: 656.5974 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 192.3605\n",
            "Epoch 12: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 192.3605 - val_loss: 650.5043 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 137.6219\n",
            "Epoch 13: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 137.6219 - val_loss: 642.5741 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 144.3787\n",
            "Epoch 14: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 144.3787 - val_loss: 637.4716 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 176.0562\n",
            "Epoch 15: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 176.0562 - val_loss: 637.3774 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 159.9258\n",
            "Epoch 16: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 159.9258 - val_loss: 641.3548 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 116.9012\n",
            "Epoch 17: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 116.9012 - val_loss: 646.6708 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 104.7462\n",
            "Epoch 18: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 104.7462 - val_loss: 650.2567 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 124.2366\n",
            "Epoch 19: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 124.2366 - val_loss: 650.0125 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 131.9106\n",
            "Epoch 20: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 131.9106 - val_loss: 646.1136 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 113.4585\n",
            "Epoch 21: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 113.4585 - val_loss: 640.6989 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 96.4535\n",
            "Epoch 22: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 96.4535 - val_loss: 636.3819 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 100.5115\n",
            "Epoch 23: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 100.5115 - val_loss: 634.9177 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 108.7269\n",
            "Epoch 24: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 108.7269 - val_loss: 636.7875 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 100.6867\n",
            "Epoch 25: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 100.6867 - val_loss: 641.0234 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 86.2997\n",
            "Epoch 26: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 86.2997 - val_loss: 645.6485 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 83.5447\n",
            "Epoch 27: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 83.5447 - val_loss: 648.2842 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 88.2306\n",
            "Epoch 28: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 88.2306 - val_loss: 647.6636 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 85.6013\n",
            "Epoch 29: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 85.6013 - val_loss: 644.1276 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 75.7069\n",
            "Epoch 30: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 75.7069 - val_loss: 639.3276 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 71.1614\n",
            "Epoch 31: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 71.1614 - val_loss: 635.6665 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 74.7838\n",
            "Epoch 32: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 74.7838 - val_loss: 634.5536 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 75.1477\n",
            "Epoch 33: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 75.1477 - val_loss: 635.9477 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 67.8208\n",
            "Epoch 34: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 67.8208 - val_loss: 638.7275 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 61.7535\n",
            "Epoch 35: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 61.7535 - val_loss: 641.2753 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 62.3428\n",
            "Epoch 36: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 62.3428 - val_loss: 642.1603 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 63.3508\n",
            "Epoch 37: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 63.3508 - val_loss: 640.4968 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 59.1698\n",
            "Epoch 38: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 59.1698 - val_loss: 636.6144 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 54.0650\n",
            "Epoch 39: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 54.0650 - val_loss: 632.3741 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 53.5618\n",
            "Epoch 40: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 53.5618 - val_loss: 630.1296 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 54.2394\n",
            "Epoch 41: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 54.2394 - val_loss: 630.5823 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 51.1154\n",
            "Epoch 42: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 51.1154 - val_loss: 632.7300 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 47.2561\n",
            "Epoch 43: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 47.2561 - val_loss: 634.8869 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 46.6106\n",
            "Epoch 44: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 46.6106 - val_loss: 635.6982 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 46.3462\n",
            "Epoch 45: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 46.3462 - val_loss: 634.6044 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 43.8357\n",
            "Epoch 46: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 43.8357 - val_loss: 632.2935 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.2750\n",
            "Epoch 47: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 41.2750 - val_loss: 629.9398 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.7555\n",
            "Epoch 48: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 40.7555 - val_loss: 628.6264 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.1833\n",
            "Epoch 49: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 40.1833 - val_loss: 628.7150 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 37.9597\n",
            "Epoch 50: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 37.9597 - val_loss: 629.7610 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 36.0934\n",
            "Epoch 51: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 36.0934 - val_loss: 630.5436 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 35.5657\n",
            "Epoch 52: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 35.5657 - val_loss: 630.1111 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 34.6505\n",
            "Epoch 53: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 34.6505 - val_loss: 628.5823 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 32.7546\n",
            "Epoch 54: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 32.7546 - val_loss: 626.3818 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 31.4140\n",
            "Epoch 55: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 31.4140 - val_loss: 624.6355 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 30.8795\n",
            "Epoch 56: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 30.8795 - val_loss: 624.0649 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 29.7579\n",
            "Epoch 57: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 29.7579 - val_loss: 624.5493 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 28.2934\n",
            "Epoch 58: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 28.2934 - val_loss: 625.0535 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 27.4038\n",
            "Epoch 59: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 27.4038 - val_loss: 625.1499 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 26.6493\n",
            "Epoch 60: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 26.6493 - val_loss: 624.5444 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 25.4487\n",
            "Epoch 61: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 25.4487 - val_loss: 623.2512 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 24.3682\n",
            "Epoch 62: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 24.3682 - val_loss: 621.9598 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 23.6547\n",
            "Epoch 63: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 23.6547 - val_loss: 621.5648 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 22.7603\n",
            "Epoch 64: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 22.7603 - val_loss: 622.0612 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 21.7360\n",
            "Epoch 65: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 21.7360 - val_loss: 622.5335 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.9788\n",
            "Epoch 66: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 20.9788 - val_loss: 621.9701 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.2433\n",
            "Epoch 67: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 20.2433 - val_loss: 620.9296 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.4031\n",
            "Epoch 68: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 19.4031 - val_loss: 620.0284 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.6632\n",
            "Epoch 69: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 18.6632 - val_loss: 619.2211 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.0166\n",
            "Epoch 70: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 18.0166 - val_loss: 618.6782 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 17.3036\n",
            "Epoch 71: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 17.3036 - val_loss: 618.7670 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.6353\n",
            "Epoch 72: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 16.6353 - val_loss: 618.8955 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.0137\n",
            "Epoch 73: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 16.0137 - val_loss: 618.3091 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.3871\n",
            "Epoch 74: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 15.3871 - val_loss: 617.3470 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.7834\n",
            "Epoch 75: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 14.7834 - val_loss: 616.9507 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.2405\n",
            "Epoch 76: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 14.2405 - val_loss: 616.4979 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.6743\n",
            "Epoch 77: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 13.6743 - val_loss: 615.9716 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.1208\n",
            "Epoch 78: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 13.1208 - val_loss: 615.8597 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 12.6160\n",
            "Epoch 79: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 12.6160 - val_loss: 615.5628 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 12.1580\n",
            "Epoch 80: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 12.1580 - val_loss: 614.3522 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.6504\n",
            "Epoch 81: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 11.6504 - val_loss: 613.7081 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.2243\n",
            "Epoch 82: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 11.2243 - val_loss: 613.9849 - lr: 0.0010\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.7880\n",
            "Epoch 83: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 10.7880 - val_loss: 613.4651 - lr: 0.0010\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.3428\n",
            "Epoch 84: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 10.3428 - val_loss: 612.7198 - lr: 0.0010\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9701\n",
            "Epoch 85: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9.9701 - val_loss: 612.5725 - lr: 0.0010\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5372\n",
            "Epoch 86: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9.5372 - val_loss: 611.8409 - lr: 0.0010\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1910\n",
            "Epoch 87: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 9.1910 - val_loss: 610.7829 - lr: 0.0010\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 8.8296\n",
            "Epoch 88: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 8.8296 - val_loss: 610.8561 - lr: 0.0010\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 8.4714\n",
            "Epoch 89: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 8.4714 - val_loss: 610.8054 - lr: 0.0010\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 8.1485\n",
            "Epoch 90: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 8.1485 - val_loss: 609.7894 - lr: 0.0010\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 7.8599\n",
            "Epoch 91: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 7.8599 - val_loss: 609.3700 - lr: 0.0010\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 7.4797\n",
            "Epoch 92: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 7.4797 - val_loss: 608.8083 - lr: 0.0010\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 7.2159\n",
            "Epoch 93: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 7.2159 - val_loss: 607.6796 - lr: 0.0010\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 6.9261\n",
            "Epoch 94: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 6.9261 - val_loss: 607.5804 - lr: 0.0010\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 6.6631\n",
            "Epoch 95: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 6.6631 - val_loss: 607.5249 - lr: 0.0010\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 6.3820\n",
            "Epoch 96: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 6.3820 - val_loss: 606.4330 - lr: 0.0010\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 6.1321\n",
            "Epoch 97: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 6.1321 - val_loss: 606.0024 - lr: 0.0010\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.8550\n",
            "Epoch 98: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.8550 - val_loss: 605.3516 - lr: 0.0010\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.6628\n",
            "Epoch 99: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.6628 - val_loss: 604.6792 - lr: 0.0010\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.4214\n",
            "Epoch 100: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.4214 - val_loss: 604.1484 - lr: 0.0010\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.2361\n",
            "Epoch 101: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.2361 - val_loss: 603.7731 - lr: 0.0010\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 4.9925\n",
            "Epoch 102: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.9925 - val_loss: 603.0607 - lr: 0.0010\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 4.8240\n",
            "Epoch 103: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.8240 - val_loss: 602.7635 - lr: 0.0010\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 4.6115\n",
            "Epoch 104: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.6115 - val_loss: 602.0533 - lr: 0.0010\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 4.4469\n",
            "Epoch 105: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.4469 - val_loss: 601.1622 - lr: 0.0010\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 4.2558\n",
            "Epoch 106: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.2558 - val_loss: 600.7483 - lr: 0.0010\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 4.0684\n",
            "Epoch 107: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.0684 - val_loss: 600.3808 - lr: 0.0010\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.8962\n",
            "Epoch 108: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.8962 - val_loss: 599.3150 - lr: 0.0010\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.7682\n",
            "Epoch 109: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7682 - val_loss: 599.4921 - lr: 0.0010\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.6282\n",
            "Epoch 110: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6282 - val_loss: 598.1540 - lr: 0.0010\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.4279\n",
            "Epoch 111: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.4279 - val_loss: 597.7667 - lr: 0.0010\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.2744\n",
            "Epoch 112: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.2744 - val_loss: 597.2108 - lr: 0.0010\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.1429\n",
            "Epoch 113: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.1429 - val_loss: 596.0536 - lr: 0.0010\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.0463\n",
            "Epoch 114: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.0463 - val_loss: 596.2725 - lr: 0.0010\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9310\n",
            "Epoch 115: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.9310 - val_loss: 595.0789 - lr: 0.0010\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.8036\n",
            "Epoch 116: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.8036 - val_loss: 594.5197 - lr: 0.0010\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.6896\n",
            "Epoch 117: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6896 - val_loss: 594.4969 - lr: 0.0010\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.6066\n",
            "Epoch 118: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.6066 - val_loss: 592.8590 - lr: 0.0010\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.5056\n",
            "Epoch 119: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.5056 - val_loss: 593.3738 - lr: 0.0010\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.4100\n",
            "Epoch 120: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.4100 - val_loss: 592.2356 - lr: 0.0010\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2958\n",
            "Epoch 121: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.2958 - val_loss: 591.5651 - lr: 0.0010\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1882\n",
            "Epoch 122: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.1882 - val_loss: 591.8633 - lr: 0.0010\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1390\n",
            "Epoch 123: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.1390 - val_loss: 590.2821 - lr: 0.0010\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0573\n",
            "Epoch 124: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0573 - val_loss: 590.2239 - lr: 0.0010\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.9564\n",
            "Epoch 125: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9564 - val_loss: 589.7629 - lr: 0.0010\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8781\n",
            "Epoch 126: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8781 - val_loss: 588.7166 - lr: 0.0010\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8336\n",
            "Epoch 127: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.8336 - val_loss: 589.1826 - lr: 0.0010\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7598\n",
            "Epoch 128: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7598 - val_loss: 587.5175 - lr: 0.0010\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7041\n",
            "Epoch 129: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7041 - val_loss: 587.6058 - lr: 0.0010\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.6376\n",
            "Epoch 130: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6376 - val_loss: 586.6538 - lr: 0.0010\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5652\n",
            "Epoch 131: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5652 - val_loss: 586.0842 - lr: 0.0010\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4996\n",
            "Epoch 132: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4996 - val_loss: 586.1461 - lr: 0.0010\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4633\n",
            "Epoch 133: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4633 - val_loss: 585.0939 - lr: 0.0010\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3982\n",
            "Epoch 134: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3982 - val_loss: 584.9572 - lr: 0.0010\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3458\n",
            "Epoch 135: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3458 - val_loss: 584.2770 - lr: 0.0010\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3100\n",
            "Epoch 136: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3100 - val_loss: 583.4625 - lr: 0.0010\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2466\n",
            "Epoch 137: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2466 - val_loss: 583.2823 - lr: 0.0010\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2094\n",
            "Epoch 138: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2094 - val_loss: 582.3794 - lr: 0.0010\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1659\n",
            "Epoch 139: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1659 - val_loss: 582.1261 - lr: 0.0010\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1262\n",
            "Epoch 140: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1262 - val_loss: 581.4628 - lr: 0.0010\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0738\n",
            "Epoch 141: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0738 - val_loss: 580.6034 - lr: 0.0010\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0424\n",
            "Epoch 142: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0424 - val_loss: 580.3110 - lr: 0.0010\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0048\n",
            "Epoch 143: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0048 - val_loss: 579.1756 - lr: 0.0010\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9620\n",
            "Epoch 144: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9620 - val_loss: 579.3239 - lr: 0.0010\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9381\n",
            "Epoch 145: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9381 - val_loss: 578.2704 - lr: 0.0010\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9029\n",
            "Epoch 146: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9029 - val_loss: 578.4995 - lr: 0.0010\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8719\n",
            "Epoch 147: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8719 - val_loss: 576.9327 - lr: 0.0010\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8456\n",
            "Epoch 148: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8456 - val_loss: 577.3978 - lr: 0.0010\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8260\n",
            "Epoch 149: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8260 - val_loss: 575.3827 - lr: 0.0010\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7996\n",
            "Epoch 150: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7996 - val_loss: 576.3732 - lr: 0.0010\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7696\n",
            "Epoch 151: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7696 - val_loss: 574.5711 - lr: 0.0010\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7482\n",
            "Epoch 152: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7482 - val_loss: 575.5360 - lr: 0.0010\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7304\n",
            "Epoch 153: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7304 - val_loss: 573.0753 - lr: 0.0010\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7291\n",
            "Epoch 154: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7291 - val_loss: 574.3319 - lr: 0.0010\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7197\n",
            "Epoch 155: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7197 - val_loss: 571.5796 - lr: 0.0010\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6958\n",
            "Epoch 156: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6958 - val_loss: 573.3962 - lr: 0.0010\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6856\n",
            "Epoch 157: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6856 - val_loss: 570.4333 - lr: 0.0010\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6742\n",
            "Epoch 158: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6742 - val_loss: 572.4689 - lr: 0.0010\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6642\n",
            "Epoch 159: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6642 - val_loss: 568.9647 - lr: 0.0010\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6561\n",
            "Epoch 160: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6561 - val_loss: 571.1291 - lr: 0.0010\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6750\n",
            "Epoch 161: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6750 - val_loss: 567.3446 - lr: 0.0010\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6579\n",
            "Epoch 162: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6579 - val_loss: 570.1403 - lr: 0.0010\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6355\n",
            "Epoch 163: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6355 - val_loss: 566.4569 - lr: 0.0010\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6084\n",
            "Epoch 164: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6084 - val_loss: 568.8165 - lr: 0.0010\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5754\n",
            "Epoch 165: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5754 - val_loss: 565.3665 - lr: 0.0010\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5234\n",
            "Epoch 166: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5234 - val_loss: 566.9871 - lr: 0.0010\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4898\n",
            "Epoch 167: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4898 - val_loss: 564.2516 - lr: 0.0010\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4683\n",
            "Epoch 168: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4683 - val_loss: 565.4427 - lr: 0.0010\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4341\n",
            "Epoch 169: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4341 - val_loss: 563.4299 - lr: 0.0010\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4006\n",
            "Epoch 170: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4006 - val_loss: 563.9461 - lr: 0.0010\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3850\n",
            "Epoch 171: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3850 - val_loss: 562.0372 - lr: 0.0010\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3717\n",
            "Epoch 172: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3717 - val_loss: 562.6063 - lr: 0.0010\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3616\n",
            "Epoch 173: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3616 - val_loss: 560.6373 - lr: 0.0010\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3525\n",
            "Epoch 174: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3525 - val_loss: 561.5127 - lr: 0.0010\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3543\n",
            "Epoch 175: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3543 - val_loss: 559.1188 - lr: 0.0010\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3610\n",
            "Epoch 176: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3610 - val_loss: 560.7845 - lr: 0.0010\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3931\n",
            "Epoch 177: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3931 - val_loss: 556.9355 - lr: 0.0010\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4413\n",
            "Epoch 178: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4413 - val_loss: 560.4263 - lr: 0.0010\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5779\n",
            "Epoch 179: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5779 - val_loss: 554.1978 - lr: 0.0010\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7626\n",
            "Epoch 180: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7626 - val_loss: 560.9759 - lr: 0.0010\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1298\n",
            "Epoch 181: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.1298 - val_loss: 551.0386 - lr: 0.0010\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4429\n",
            "Epoch 182: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.4429 - val_loss: 561.2163 - lr: 0.0010\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8951\n",
            "Epoch 183: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8951 - val_loss: 548.9272 - lr: 0.0010\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.6330\n",
            "Epoch 184: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6330 - val_loss: 557.9386 - lr: 0.0010\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0271\n",
            "Epoch 185: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0271 - val_loss: 551.5976 - lr: 0.0010\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3693\n",
            "Epoch 186: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3693 - val_loss: 551.1968 - lr: 0.0010\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3327\n",
            "Epoch 187: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3327 - val_loss: 555.4138 - lr: 0.0010\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7731\n",
            "Epoch 188: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7731 - val_loss: 546.7404 - lr: 0.0010\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0117\n",
            "Epoch 189: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0117 - val_loss: 554.1188 - lr: 0.0010\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8320\n",
            "Epoch 190: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.8320 - val_loss: 547.5222 - lr: 0.0010\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3873\n",
            "Epoch 191: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3873 - val_loss: 548.8830 - lr: 0.0010\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2030\n",
            "Epoch 192: val_loss did not improve from 544.29767\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2030 - val_loss: 550.5925 - lr: 0.0010\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3877\n",
            "Epoch 193: val_loss improved from 544.29767 to 544.25421, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3877 - val_loss: 544.2542 - lr: 0.0010\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6466\n",
            "Epoch 194: val_loss did not improve from 544.25421\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6466 - val_loss: 550.4825 - lr: 0.0010\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7525\n",
            "Epoch 195: val_loss improved from 544.25421 to 542.94617, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.7525 - val_loss: 542.9462 - lr: 0.0010\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5149\n",
            "Epoch 196: val_loss did not improve from 542.94617\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5149 - val_loss: 546.6335 - lr: 0.0010\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2519\n",
            "Epoch 197: val_loss did not improve from 542.94617\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2519 - val_loss: 544.9232 - lr: 0.0010\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1724\n",
            "Epoch 198: val_loss improved from 542.94617 to 541.99255, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1724 - val_loss: 541.9926 - lr: 0.0010\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3051\n",
            "Epoch 199: val_loss did not improve from 541.99255\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3051 - val_loss: 545.9848 - lr: 0.0010\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4495\n",
            "Epoch 200: val_loss improved from 541.99255 to 539.57660, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4495 - val_loss: 539.5766 - lr: 0.0010\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4295\n",
            "Epoch 201: val_loss did not improve from 539.57660\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4295 - val_loss: 543.4604 - lr: 0.0010\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2825\n",
            "Epoch 202: val_loss did not improve from 539.57660\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2825 - val_loss: 540.3169 - lr: 0.0010\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1528\n",
            "Epoch 203: val_loss improved from 539.57660 to 539.36963, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.1528 - val_loss: 539.3696 - lr: 0.0010\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1614\n",
            "Epoch 204: val_loss did not improve from 539.36963\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1614 - val_loss: 541.3796 - lr: 0.0010\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2595\n",
            "Epoch 205: val_loss improved from 539.36963 to 536.40576, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2595 - val_loss: 536.4058 - lr: 0.0010\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3154\n",
            "Epoch 206: val_loss did not improve from 536.40576\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3154 - val_loss: 540.0331 - lr: 0.0010\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2887\n",
            "Epoch 207: val_loss improved from 536.40576 to 535.58295, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2887 - val_loss: 535.5829 - lr: 0.0010\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1973\n",
            "Epoch 208: val_loss did not improve from 535.58295\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1973 - val_loss: 536.9083 - lr: 0.0010\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1271\n",
            "Epoch 209: val_loss did not improve from 535.58295\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1271 - val_loss: 536.2161 - lr: 0.0010\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1199\n",
            "Epoch 210: val_loss improved from 535.58295 to 533.73700, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 11s 11s/step - loss: 0.1199 - val_loss: 533.7370 - lr: 0.0010\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1679\n",
            "Epoch 211: val_loss did not improve from 533.73700\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1679 - val_loss: 535.9818 - lr: 0.0010\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2221\n",
            "Epoch 212: val_loss improved from 533.73700 to 531.35486, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2221 - val_loss: 531.3549 - lr: 0.0010\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2268\n",
            "Epoch 213: val_loss did not improve from 531.35486\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2268 - val_loss: 534.2051 - lr: 0.0010\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1919\n",
            "Epoch 214: val_loss improved from 531.35486 to 530.75012, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.1919 - val_loss: 530.7501 - lr: 0.0010\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1377\n",
            "Epoch 215: val_loss did not improve from 530.75012\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1377 - val_loss: 531.3751 - lr: 0.0010\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0989\n",
            "Epoch 216: val_loss improved from 530.75012 to 530.58759, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0989 - val_loss: 530.5876 - lr: 0.0010\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0974\n",
            "Epoch 217: val_loss improved from 530.58759 to 528.41663, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0974 - val_loss: 528.4166 - lr: 0.0010\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1220\n",
            "Epoch 218: val_loss did not improve from 528.41663\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1220 - val_loss: 529.9708 - lr: 0.0010\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1466\n",
            "Epoch 219: val_loss improved from 528.41663 to 526.29718, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.1466 - val_loss: 526.2972 - lr: 0.0010\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1569\n",
            "Epoch 220: val_loss did not improve from 526.29718\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1569 - val_loss: 528.4760 - lr: 0.0010\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1518\n",
            "Epoch 221: val_loss improved from 526.29718 to 524.99341, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.1518 - val_loss: 524.9934 - lr: 0.0010\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1268\n",
            "Epoch 222: val_loss did not improve from 524.99341\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1268 - val_loss: 526.2921 - lr: 0.0010\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0999\n",
            "Epoch 223: val_loss improved from 524.99341 to 524.29895, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0999 - val_loss: 524.2990 - lr: 0.0010\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0819\n",
            "Epoch 224: val_loss improved from 524.29895 to 523.76355, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0819 - val_loss: 523.7635 - lr: 0.0010\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0765\n",
            "Epoch 225: val_loss improved from 523.76355 to 523.45514, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0765 - val_loss: 523.4551 - lr: 0.0010\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0790\n",
            "Epoch 226: val_loss improved from 523.45514 to 521.47644, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0790 - val_loss: 521.4764 - lr: 0.0010\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0874\n",
            "Epoch 227: val_loss did not improve from 521.47644\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0874 - val_loss: 522.3875 - lr: 0.0010\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0975\n",
            "Epoch 228: val_loss improved from 521.47644 to 519.51306, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0975 - val_loss: 519.5131 - lr: 0.0010\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1029\n",
            "Epoch 229: val_loss did not improve from 519.51306\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1029 - val_loss: 520.9666 - lr: 0.0010\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1051\n",
            "Epoch 230: val_loss improved from 519.51306 to 517.78845, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.1051 - val_loss: 517.7885 - lr: 0.0010\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1031\n",
            "Epoch 231: val_loss did not improve from 517.78845\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1031 - val_loss: 519.2626 - lr: 0.0010\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1004\n",
            "Epoch 232: val_loss improved from 517.78845 to 516.23639, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.1004 - val_loss: 516.2364 - lr: 0.0010\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0916\n",
            "Epoch 233: val_loss did not improve from 516.23639\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0916 - val_loss: 517.3837 - lr: 0.0010\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0816\n",
            "Epoch 234: val_loss improved from 516.23639 to 514.97015, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.0816 - val_loss: 514.9702 - lr: 0.0010\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728\n",
            "Epoch 235: val_loss did not improve from 514.97015\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0728 - val_loss: 515.3070 - lr: 0.0010\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0647\n",
            "Epoch 236: val_loss improved from 514.97015 to 513.59802, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0647 - val_loss: 513.5980 - lr: 0.0010\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0587\n",
            "Epoch 237: val_loss improved from 513.59802 to 513.31488, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0587 - val_loss: 513.3149 - lr: 0.0010\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0556\n",
            "Epoch 238: val_loss improved from 513.31488 to 512.31677, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0556 - val_loss: 512.3168 - lr: 0.0010\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0537\n",
            "Epoch 239: val_loss improved from 512.31677 to 511.38226, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0537 - val_loss: 511.3823 - lr: 0.0010\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0523\n",
            "Epoch 240: val_loss improved from 511.38226 to 510.85123, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0523 - val_loss: 510.8512 - lr: 0.0010\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0514\n",
            "Epoch 241: val_loss improved from 510.85123 to 509.51865, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0514 - val_loss: 509.5186 - lr: 0.0010\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0517\n",
            "Epoch 242: val_loss improved from 509.51865 to 509.31314, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0517 - val_loss: 509.3131 - lr: 0.0010\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0530\n",
            "Epoch 243: val_loss improved from 509.31314 to 507.48773, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0530 - val_loss: 507.4877 - lr: 0.0010\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0551\n",
            "Epoch 244: val_loss did not improve from 507.48773\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0551 - val_loss: 507.9721 - lr: 0.0010\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0602\n",
            "Epoch 245: val_loss improved from 507.48773 to 505.38254, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0602 - val_loss: 505.3825 - lr: 0.0010\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0697\n",
            "Epoch 246: val_loss did not improve from 505.38254\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0697 - val_loss: 506.8914 - lr: 0.0010\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0901\n",
            "Epoch 247: val_loss improved from 505.38254 to 502.79749, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0901 - val_loss: 502.7975 - lr: 0.0010\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1240\n",
            "Epoch 248: val_loss did not improve from 502.79749\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1240 - val_loss: 506.3249 - lr: 0.0010\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1945\n",
            "Epoch 249: val_loss improved from 502.79749 to 499.58932, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1945 - val_loss: 499.5893 - lr: 0.0010\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3082\n",
            "Epoch 250: val_loss did not improve from 499.58932\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3082 - val_loss: 506.7835 - lr: 0.0010\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5513\n",
            "Epoch 251: val_loss improved from 499.58932 to 495.03000, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.5513 - val_loss: 495.0300 - lr: 0.0010\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9272\n",
            "Epoch 252: val_loss did not improve from 495.03000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9272 - val_loss: 508.9662 - lr: 0.0010\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7086\n",
            "Epoch 253: val_loss improved from 495.03000 to 488.73578, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.7086 - val_loss: 488.7358 - lr: 0.0010\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.7237\n",
            "Epoch 254: val_loss did not improve from 488.73578\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.7237 - val_loss: 513.1740 - lr: 0.0010\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 4.6837\n",
            "Epoch 255: val_loss improved from 488.73578 to 481.85239, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 4.6837 - val_loss: 481.8524 - lr: 0.0010\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.5432\n",
            "Epoch 256: val_loss did not improve from 481.85239\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.5432 - val_loss: 512.4986 - lr: 0.0010\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.2181\n",
            "Epoch 257: val_loss did not improve from 481.85239\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.2181 - val_loss: 486.0817 - lr: 0.0010\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1215\n",
            "Epoch 258: val_loss did not improve from 481.85239\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.1215 - val_loss: 496.1479 - lr: 0.0010\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1764\n",
            "Epoch 259: val_loss did not improve from 481.85239\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1764 - val_loss: 501.2696 - lr: 0.0010\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1312\n",
            "Epoch 260: val_loss improved from 481.85239 to 481.67773, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1312 - val_loss: 481.6777 - lr: 0.0010\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.8011\n",
            "Epoch 261: val_loss did not improve from 481.67773\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.8011 - val_loss: 503.6579 - lr: 0.0010\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.6096\n",
            "Epoch 262: val_loss did not improve from 481.67773\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.6096 - val_loss: 485.4659 - lr: 0.0010\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7576\n",
            "Epoch 263: val_loss did not improve from 481.67773\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7576 - val_loss: 489.0155 - lr: 0.0010\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1691\n",
            "Epoch 264: val_loss did not improve from 481.67773\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1691 - val_loss: 497.8195 - lr: 0.0010\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2244\n",
            "Epoch 265: val_loss improved from 481.67773 to 479.48920, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 1.2244 - val_loss: 479.4892 - lr: 0.0010\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8379\n",
            "Epoch 266: val_loss did not improve from 479.48920\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8379 - val_loss: 495.6491 - lr: 0.0010\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3135\n",
            "Epoch 267: val_loss did not improve from 479.48920\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3135 - val_loss: 484.3472 - lr: 0.0010\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1965\n",
            "Epoch 268: val_loss did not improve from 479.48920\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1965 - val_loss: 482.3149 - lr: 0.0010\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3574\n",
            "Epoch 269: val_loss did not improve from 479.48920\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3574 - val_loss: 492.9392 - lr: 0.0010\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1495\n",
            "Epoch 270: val_loss improved from 479.48920 to 477.26288, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 1.1495 - val_loss: 477.2629 - lr: 0.0010\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9303\n",
            "Epoch 271: val_loss did not improve from 477.26288\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9303 - val_loss: 486.2192 - lr: 0.0010\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2401\n",
            "Epoch 272: val_loss did not improve from 477.26288\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2401 - val_loss: 484.5255 - lr: 0.0010\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1701\n",
            "Epoch 273: val_loss improved from 477.26288 to 475.78091, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.1701 - val_loss: 475.7809 - lr: 0.0010\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6281\n",
            "Epoch 274: val_loss did not improve from 475.78091\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6281 - val_loss: 486.8787 - lr: 0.0010\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7379\n",
            "Epoch 275: val_loss did not improve from 475.78091\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7379 - val_loss: 476.2321 - lr: 0.0010\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2623\n",
            "Epoch 276: val_loss did not improve from 475.78091\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2623 - val_loss: 477.9756 - lr: 0.0010\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0607\n",
            "Epoch 277: val_loss did not improve from 475.78091\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0607 - val_loss: 482.4214 - lr: 0.0010\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3750\n",
            "Epoch 278: val_loss improved from 475.78091 to 471.82635, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3750 - val_loss: 471.8264 - lr: 0.0010\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4993\n",
            "Epoch 279: val_loss did not improve from 471.82635\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4993 - val_loss: 479.2982 - lr: 0.0010\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2441\n",
            "Epoch 280: val_loss did not improve from 471.82635\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2441 - val_loss: 474.7817 - lr: 0.0010\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0539\n",
            "Epoch 281: val_loss improved from 471.82635 to 471.25586, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0539 - val_loss: 471.2559 - lr: 0.0010\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1836\n",
            "Epoch 282: val_loss did not improve from 471.25586\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1836 - val_loss: 477.7342 - lr: 0.0010\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3549\n",
            "Epoch 283: val_loss improved from 471.25586 to 468.84174, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3549 - val_loss: 468.8417 - lr: 0.0010\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2453\n",
            "Epoch 284: val_loss did not improve from 468.84174\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2453 - val_loss: 472.4189 - lr: 0.0010\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0559\n",
            "Epoch 285: val_loss did not improve from 468.84174\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0559 - val_loss: 471.9364 - lr: 0.0010\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721\n",
            "Epoch 286: val_loss improved from 468.84174 to 466.39935, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0721 - val_loss: 466.3994 - lr: 0.0010\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2054\n",
            "Epoch 287: val_loss did not improve from 466.39935\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2054 - val_loss: 472.0138 - lr: 0.0010\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2121\n",
            "Epoch 288: val_loss improved from 466.39935 to 465.57730, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2121 - val_loss: 465.5773 - lr: 0.0010\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0870\n",
            "Epoch 289: val_loss did not improve from 465.57730\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0870 - val_loss: 466.2215 - lr: 0.0010\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0340\n",
            "Epoch 290: val_loss did not improve from 465.57730\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0340 - val_loss: 467.9634 - lr: 0.0010\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1029\n",
            "Epoch 291: val_loss improved from 465.57730 to 461.89233, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.1029 - val_loss: 461.8923 - lr: 0.0010\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1582\n",
            "Epoch 292: val_loss did not improve from 461.89233\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1582 - val_loss: 466.0211 - lr: 0.0010\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1082\n",
            "Epoch 293: val_loss did not improve from 461.89233\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1082 - val_loss: 461.9472 - lr: 0.0010\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0318\n",
            "Epoch 294: val_loss improved from 461.89233 to 460.73392, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0318 - val_loss: 460.7339 - lr: 0.0010\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0406\n",
            "Epoch 295: val_loss did not improve from 460.73392\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0406 - val_loss: 463.1158 - lr: 0.0010\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0946\n",
            "Epoch 296: val_loss improved from 460.73392 to 457.71945, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0946 - val_loss: 457.7195 - lr: 0.0010\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0981\n",
            "Epoch 297: val_loss did not improve from 457.71945\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0981 - val_loss: 460.3524 - lr: 0.0010\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0547\n",
            "Epoch 298: val_loss did not improve from 457.71945\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0547 - val_loss: 457.7403 - lr: 0.0010\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0221\n",
            "Epoch 299: val_loss improved from 457.71945 to 455.79614, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0221 - val_loss: 455.7961 - lr: 0.0010\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390\n",
            "Epoch 300: val_loss did not improve from 455.79614\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0390 - val_loss: 457.8381 - lr: 0.0010\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0691\n",
            "Epoch 301: val_loss improved from 455.79614 to 453.25870, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0691 - val_loss: 453.2587 - lr: 0.0010\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0632\n",
            "Epoch 302: val_loss did not improve from 453.25870\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0632 - val_loss: 454.9964 - lr: 0.0010\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0329\n",
            "Epoch 303: val_loss improved from 453.25870 to 453.01440, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0329 - val_loss: 453.0144 - lr: 0.0010\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0180\n",
            "Epoch 304: val_loss improved from 453.01440 to 451.10434, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0180 - val_loss: 451.1043 - lr: 0.0010\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0307\n",
            "Epoch 305: val_loss did not improve from 451.10434\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0307 - val_loss: 452.5533 - lr: 0.0010\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0452\n",
            "Epoch 306: val_loss improved from 451.10434 to 448.76047, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0452 - val_loss: 448.7605 - lr: 0.0010\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426\n",
            "Epoch 307: val_loss did not improve from 448.76047\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0426 - val_loss: 449.9634 - lr: 0.0010\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0258\n",
            "Epoch 308: val_loss improved from 448.76047 to 448.08472, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0258 - val_loss: 448.0847 - lr: 0.0010\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0152\n",
            "Epoch 309: val_loss improved from 448.08472 to 446.45844, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0152 - val_loss: 446.4584 - lr: 0.0010\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0214\n",
            "Epoch 310: val_loss did not improve from 446.45844\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0214 - val_loss: 447.2017 - lr: 0.0010\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0310\n",
            "Epoch 311: val_loss improved from 446.45844 to 443.99084, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0310 - val_loss: 443.9908 - lr: 0.0010\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0317\n",
            "Epoch 312: val_loss did not improve from 443.99084\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0317 - val_loss: 444.9957 - lr: 0.0010\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0239\n",
            "Epoch 313: val_loss improved from 443.99084 to 442.79181, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0239 - val_loss: 442.7918 - lr: 0.0010\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0156\n",
            "Epoch 314: val_loss improved from 442.79181 to 441.91107, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.0156 - val_loss: 441.9111 - lr: 0.0010\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0136\n",
            "Epoch 315: val_loss improved from 441.91107 to 441.76688, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0136 - val_loss: 441.7669 - lr: 0.0010\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187\n",
            "Epoch 316: val_loss improved from 441.76688 to 439.28534, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0187 - val_loss: 439.2853 - lr: 0.0010\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0230\n",
            "Epoch 317: val_loss did not improve from 439.28534\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0230 - val_loss: 440.0507 - lr: 0.0010\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0219\n",
            "Epoch 318: val_loss improved from 439.28534 to 437.54083, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0219 - val_loss: 437.5408 - lr: 0.0010\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171\n",
            "Epoch 319: val_loss improved from 437.54083 to 437.38315, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0171 - val_loss: 437.3831 - lr: 0.0010\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0128\n",
            "Epoch 320: val_loss improved from 437.38315 to 436.28400, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0128 - val_loss: 436.2840 - lr: 0.0010\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0118\n",
            "Epoch 321: val_loss improved from 436.28400 to 434.66788, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0118 - val_loss: 434.6679 - lr: 0.0010\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0141\n",
            "Epoch 322: val_loss did not improve from 434.66788\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0141 - val_loss: 434.7698 - lr: 0.0010\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0166\n",
            "Epoch 323: val_loss improved from 434.66788 to 432.42871, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0166 - val_loss: 432.4287 - lr: 0.0010\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0161\n",
            "Epoch 324: val_loss did not improve from 432.42871\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0161 - val_loss: 432.5682 - lr: 0.0010\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0140\n",
            "Epoch 325: val_loss improved from 432.42871 to 430.77612, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0140 - val_loss: 430.7761 - lr: 0.0010\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0116\n",
            "Epoch 326: val_loss improved from 430.77612 to 430.09100, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0116 - val_loss: 430.0910 - lr: 0.0010\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0102\n",
            "Epoch 327: val_loss improved from 430.09100 to 429.31305, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0102 - val_loss: 429.3130 - lr: 0.0010\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0107\n",
            "Epoch 328: val_loss improved from 429.31305 to 427.60989, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0107 - val_loss: 427.6099 - lr: 0.0010\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0121\n",
            "Epoch 329: val_loss improved from 427.60989 to 427.49957, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0121 - val_loss: 427.4996 - lr: 0.0010\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0129\n",
            "Epoch 330: val_loss improved from 427.49957 to 425.45944, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0129 - val_loss: 425.4594 - lr: 0.0010\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0124\n",
            "Epoch 331: val_loss improved from 425.45944 to 425.36639, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0124 - val_loss: 425.3664 - lr: 0.0010\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0114\n",
            "Epoch 332: val_loss improved from 425.36639 to 423.66290, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0114 - val_loss: 423.6629 - lr: 0.0010\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0100\n",
            "Epoch 333: val_loss improved from 423.66290 to 423.04092, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0100 - val_loss: 423.0409 - lr: 0.0010\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0090\n",
            "Epoch 334: val_loss improved from 423.04092 to 421.92749, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0090 - val_loss: 421.9275 - lr: 0.0010\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0087\n",
            "Epoch 335: val_loss improved from 421.92749 to 420.67657, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0087 - val_loss: 420.6766 - lr: 0.0010\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0089\n",
            "Epoch 336: val_loss improved from 420.67657 to 420.11725, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0089 - val_loss: 420.1172 - lr: 0.0010\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0093\n",
            "Epoch 337: val_loss improved from 420.11725 to 418.45233, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0093 - val_loss: 418.4523 - lr: 0.0010\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0096\n",
            "Epoch 338: val_loss improved from 418.45233 to 418.15439, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0096 - val_loss: 418.1544 - lr: 0.0010\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0098\n",
            "Epoch 339: val_loss improved from 418.15439 to 416.38486, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0098 - val_loss: 416.3849 - lr: 0.0010\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0093\n",
            "Epoch 340: val_loss improved from 416.38486 to 416.01334, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0093 - val_loss: 416.0133 - lr: 0.0010\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0089\n",
            "Epoch 341: val_loss improved from 416.01334 to 414.43484, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0089 - val_loss: 414.4348 - lr: 0.0010\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0083\n",
            "Epoch 342: val_loss improved from 414.43484 to 413.82520, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0083 - val_loss: 413.8252 - lr: 0.0010\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0078\n",
            "Epoch 343: val_loss improved from 413.82520 to 412.53989, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0078 - val_loss: 412.5399 - lr: 0.0010\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0074\n",
            "Epoch 344: val_loss improved from 412.53989 to 411.58087, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0074 - val_loss: 411.5809 - lr: 0.0010\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0072\n",
            "Epoch 345: val_loss improved from 411.58087 to 410.63522, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0072 - val_loss: 410.6352 - lr: 0.0010\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0071\n",
            "Epoch 346: val_loss improved from 410.63522 to 409.33197, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0071 - val_loss: 409.3320 - lr: 0.0010\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0072\n",
            "Epoch 347: val_loss improved from 409.33197 to 408.71875, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0072 - val_loss: 408.7188 - lr: 0.0010\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0075\n",
            "Epoch 348: val_loss improved from 408.71875 to 407.13177, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0075 - val_loss: 407.1318 - lr: 0.0010\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0077\n",
            "Epoch 349: val_loss improved from 407.13177 to 406.75098, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0077 - val_loss: 406.7510 - lr: 0.0010\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0080\n",
            "Epoch 350: val_loss improved from 406.75098 to 404.96490, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0080 - val_loss: 404.9649 - lr: 0.0010\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0081\n",
            "Epoch 351: val_loss improved from 404.96490 to 404.32831, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0081 - val_loss: 404.3283 - lr: 5.0000e-04\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0064\n",
            "Epoch 352: val_loss improved from 404.32831 to 403.62741, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0064 - val_loss: 403.6274 - lr: 5.0000e-04\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0077\n",
            "Epoch 353: val_loss improved from 403.62741 to 402.22763, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0077 - val_loss: 402.2276 - lr: 5.0000e-04\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0062\n",
            "Epoch 354: val_loss improved from 402.22763 to 400.87436, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0062 - val_loss: 400.8744 - lr: 5.0000e-04\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0073\n",
            "Epoch 355: val_loss improved from 400.87436 to 400.15643, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0073 - val_loss: 400.1564 - lr: 5.0000e-04\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0061\n",
            "Epoch 356: val_loss improved from 400.15643 to 399.38443, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0061 - val_loss: 399.3844 - lr: 5.0000e-04\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0070\n",
            "Epoch 357: val_loss improved from 399.38443 to 398.03757, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0070 - val_loss: 398.0376 - lr: 5.0000e-04\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0060\n",
            "Epoch 358: val_loss improved from 398.03757 to 396.74646, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0060 - val_loss: 396.7465 - lr: 5.0000e-04\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0067\n",
            "Epoch 359: val_loss improved from 396.74646 to 395.98053, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0067 - val_loss: 395.9805 - lr: 5.0000e-04\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0059\n",
            "Epoch 360: val_loss improved from 395.98053 to 395.15259, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0059 - val_loss: 395.1526 - lr: 5.0000e-04\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0064\n",
            "Epoch 361: val_loss improved from 395.15259 to 393.85794, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0064 - val_loss: 393.8579 - lr: 5.0000e-04\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0058\n",
            "Epoch 362: val_loss improved from 393.85794 to 392.63132, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0058 - val_loss: 392.6313 - lr: 5.0000e-04\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0062\n",
            "Epoch 363: val_loss improved from 392.63132 to 391.81909, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0062 - val_loss: 391.8191 - lr: 5.0000e-04\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0057\n",
            "Epoch 364: val_loss improved from 391.81909 to 390.91452, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0057 - val_loss: 390.9145 - lr: 5.0000e-04\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0060\n",
            "Epoch 365: val_loss improved from 390.91452 to 389.64664, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0060 - val_loss: 389.6466 - lr: 5.0000e-04\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0056\n",
            "Epoch 366: val_loss improved from 389.64664 to 388.47714, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0056 - val_loss: 388.4771 - lr: 5.0000e-04\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0058\n",
            "Epoch 367: val_loss improved from 388.47714 to 387.64227, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0058 - val_loss: 387.6423 - lr: 5.0000e-04\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0055\n",
            "Epoch 368: val_loss improved from 387.64227 to 386.69653, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0055 - val_loss: 386.6965 - lr: 5.0000e-04\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0057\n",
            "Epoch 369: val_loss improved from 386.69653 to 385.45447, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0057 - val_loss: 385.4545 - lr: 5.0000e-04\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0054\n",
            "Epoch 370: val_loss improved from 385.45447 to 384.32797, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0054 - val_loss: 384.3280 - lr: 5.0000e-04\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0055\n",
            "Epoch 371: val_loss improved from 384.32797 to 383.45447, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0055 - val_loss: 383.4545 - lr: 5.0000e-04\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0054\n",
            "Epoch 372: val_loss improved from 383.45447 to 382.45700, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0054 - val_loss: 382.4570 - lr: 5.0000e-04\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0054\n",
            "Epoch 373: val_loss improved from 382.45700 to 381.25754, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0054 - val_loss: 381.2575 - lr: 5.0000e-04\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0053\n",
            "Epoch 374: val_loss improved from 381.25754 to 380.18832, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0053 - val_loss: 380.1883 - lr: 5.0000e-04\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0053\n",
            "Epoch 375: val_loss improved from 380.18832 to 379.28256, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0053 - val_loss: 379.2826 - lr: 5.0000e-04\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0052\n",
            "Epoch 376: val_loss improved from 379.28256 to 378.23978, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0052 - val_loss: 378.2398 - lr: 5.0000e-04\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0051\n",
            "Epoch 377: val_loss improved from 378.23978 to 377.06244, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0051 - val_loss: 377.0624 - lr: 5.0000e-04\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0051\n",
            "Epoch 378: val_loss improved from 377.06244 to 376.02795, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0051 - val_loss: 376.0280 - lr: 5.0000e-04\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0050\n",
            "Epoch 379: val_loss improved from 376.02795 to 375.09625, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0050 - val_loss: 375.0963 - lr: 5.0000e-04\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0050\n",
            "Epoch 380: val_loss improved from 375.09625 to 374.02014, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0050 - val_loss: 374.0201 - lr: 5.0000e-04\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0049\n",
            "Epoch 381: val_loss improved from 374.02014 to 372.87347, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0049 - val_loss: 372.8735 - lr: 5.0000e-04\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0049\n",
            "Epoch 382: val_loss improved from 372.87347 to 371.87598, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0049 - val_loss: 371.8760 - lr: 5.0000e-04\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0048\n",
            "Epoch 383: val_loss improved from 371.87598 to 370.91541, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0048 - val_loss: 370.9154 - lr: 5.0000e-04\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0048\n",
            "Epoch 384: val_loss improved from 370.91541 to 369.81223, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0048 - val_loss: 369.8122 - lr: 5.0000e-04\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0048\n",
            "Epoch 385: val_loss improved from 369.81223 to 368.69638, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0048 - val_loss: 368.6964 - lr: 5.0000e-04\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0048\n",
            "Epoch 386: val_loss improved from 368.69638 to 367.72531, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0048 - val_loss: 367.7253 - lr: 5.0000e-04\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0047\n",
            "Epoch 387: val_loss improved from 367.72531 to 366.73706, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0047 - val_loss: 366.7371 - lr: 5.0000e-04\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0047\n",
            "Epoch 388: val_loss improved from 366.73706 to 365.62628, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0047 - val_loss: 365.6263 - lr: 5.0000e-04\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0046\n",
            "Epoch 389: val_loss improved from 365.62628 to 364.54819, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0046 - val_loss: 364.5482 - lr: 5.0000e-04\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0046\n",
            "Epoch 390: val_loss improved from 364.54819 to 363.57007, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0046 - val_loss: 363.5701 - lr: 5.0000e-04\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0045\n",
            "Epoch 391: val_loss improved from 363.57007 to 362.54486, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0045 - val_loss: 362.5449 - lr: 5.0000e-04\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0045\n",
            "Epoch 392: val_loss improved from 362.54486 to 361.44553, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0045 - val_loss: 361.4455 - lr: 5.0000e-04\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0044\n",
            "Epoch 393: val_loss improved from 361.44553 to 360.40778, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0044 - val_loss: 360.4078 - lr: 5.0000e-04\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0044\n",
            "Epoch 394: val_loss improved from 360.40778 to 359.42114, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0044 - val_loss: 359.4211 - lr: 5.0000e-04\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0044\n",
            "Epoch 395: val_loss improved from 359.42114 to 358.37256, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0044 - val_loss: 358.3726 - lr: 5.0000e-04\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0043\n",
            "Epoch 396: val_loss improved from 358.37256 to 357.29498, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0043 - val_loss: 357.2950 - lr: 5.0000e-04\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0043\n",
            "Epoch 397: val_loss improved from 357.29498 to 356.28351, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0043 - val_loss: 356.2835 - lr: 5.0000e-04\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0043\n",
            "Epoch 398: val_loss improved from 356.28351 to 355.28122, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0043 - val_loss: 355.2812 - lr: 5.0000e-04\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0042\n",
            "Epoch 399: val_loss improved from 355.28122 to 354.21295, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0042 - val_loss: 354.2130 - lr: 5.0000e-04\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0042\n",
            "Epoch 400: val_loss improved from 354.21295 to 353.15463, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0042 - val_loss: 353.1546 - lr: 5.0000e-04\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0042\n",
            "Epoch 401: val_loss improved from 353.15463 to 352.14148, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0042 - val_loss: 352.1415 - lr: 2.5000e-04\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0041\n",
            "Epoch 402: val_loss improved from 352.14148 to 351.14099, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.0041 - val_loss: 351.1410 - lr: 2.5000e-04\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0041\n",
            "Epoch 403: val_loss improved from 351.14099 to 350.12177, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0041 - val_loss: 350.1218 - lr: 2.5000e-04\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0041\n",
            "Epoch 404: val_loss improved from 350.12177 to 349.07425, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0041 - val_loss: 349.0742 - lr: 2.5000e-04\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0041\n",
            "Epoch 405: val_loss improved from 349.07425 to 348.02222, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0041 - val_loss: 348.0222 - lr: 2.5000e-04\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0041\n",
            "Epoch 406: val_loss improved from 348.02222 to 346.99390, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0041 - val_loss: 346.9939 - lr: 2.5000e-04\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0040\n",
            "Epoch 407: val_loss improved from 346.99390 to 345.99008, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0040 - val_loss: 345.9901 - lr: 2.5000e-04\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0040\n",
            "Epoch 408: val_loss improved from 345.99008 to 344.98422, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0040 - val_loss: 344.9842 - lr: 2.5000e-04\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0040\n",
            "Epoch 409: val_loss improved from 344.98422 to 343.95917, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0040 - val_loss: 343.9592 - lr: 2.5000e-04\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0040\n",
            "Epoch 410: val_loss improved from 343.95917 to 342.92212, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0040 - val_loss: 342.9221 - lr: 2.5000e-04\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0040\n",
            "Epoch 411: val_loss improved from 342.92212 to 341.89349, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0040 - val_loss: 341.8935 - lr: 2.5000e-04\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0040\n",
            "Epoch 412: val_loss improved from 341.89349 to 340.88193, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0040 - val_loss: 340.8819 - lr: 2.5000e-04\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0039\n",
            "Epoch 413: val_loss improved from 340.88193 to 339.87512, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0039 - val_loss: 339.8751 - lr: 2.5000e-04\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0039\n",
            "Epoch 414: val_loss improved from 339.87512 to 338.86011, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.0039 - val_loss: 338.8601 - lr: 2.5000e-04\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0039\n",
            "Epoch 415: val_loss improved from 338.86011 to 337.83627, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0039 - val_loss: 337.8363 - lr: 2.5000e-04\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0039\n",
            "Epoch 416: val_loss improved from 337.83627 to 336.81711, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0039 - val_loss: 336.8171 - lr: 2.5000e-04\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0039\n",
            "Epoch 417: val_loss improved from 336.81711 to 335.81360, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0039 - val_loss: 335.8136 - lr: 2.5000e-04\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0039\n",
            "Epoch 418: val_loss improved from 335.81360 to 334.81931, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0039 - val_loss: 334.8193 - lr: 2.5000e-04\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0038\n",
            "Epoch 419: val_loss improved from 334.81931 to 333.81635, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0038 - val_loss: 333.8163 - lr: 2.5000e-04\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0038\n",
            "Epoch 420: val_loss improved from 333.81635 to 332.80157, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0038 - val_loss: 332.8016 - lr: 2.5000e-04\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0038\n",
            "Epoch 421: val_loss improved from 332.80157 to 331.78955, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0038 - val_loss: 331.7896 - lr: 2.5000e-04\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0038\n",
            "Epoch 422: val_loss improved from 331.78955 to 330.78845, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0038 - val_loss: 330.7885 - lr: 2.5000e-04\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0038\n",
            "Epoch 423: val_loss improved from 330.78845 to 329.79550, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0038 - val_loss: 329.7955 - lr: 2.5000e-04\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0038\n",
            "Epoch 424: val_loss improved from 329.79550 to 328.80389, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0038 - val_loss: 328.8039 - lr: 2.5000e-04\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0037\n",
            "Epoch 425: val_loss improved from 328.80389 to 327.80515, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0037 - val_loss: 327.8051 - lr: 2.5000e-04\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0037\n",
            "Epoch 426: val_loss improved from 327.80515 to 326.80295, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0037 - val_loss: 326.8029 - lr: 2.5000e-04\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0037\n",
            "Epoch 427: val_loss improved from 326.80295 to 325.80508, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0037 - val_loss: 325.8051 - lr: 2.5000e-04\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0037\n",
            "Epoch 428: val_loss improved from 325.80508 to 324.81845, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0037 - val_loss: 324.8185 - lr: 2.5000e-04\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0037\n",
            "Epoch 429: val_loss improved from 324.81845 to 323.83871, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0037 - val_loss: 323.8387 - lr: 2.5000e-04\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0037\n",
            "Epoch 430: val_loss improved from 323.83871 to 322.85956, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0037 - val_loss: 322.8596 - lr: 2.5000e-04\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0037\n",
            "Epoch 431: val_loss improved from 322.85956 to 321.87350, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0037 - val_loss: 321.8735 - lr: 2.5000e-04\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0036\n",
            "Epoch 432: val_loss improved from 321.87350 to 320.88367, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0036 - val_loss: 320.8837 - lr: 2.5000e-04\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0036\n",
            "Epoch 433: val_loss improved from 320.88367 to 319.90015, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0036 - val_loss: 319.9001 - lr: 2.5000e-04\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0036\n",
            "Epoch 434: val_loss improved from 319.90015 to 318.92557, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0036 - val_loss: 318.9256 - lr: 2.5000e-04\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0036\n",
            "Epoch 435: val_loss improved from 318.92557 to 317.95773, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0036 - val_loss: 317.9577 - lr: 2.5000e-04\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0036\n",
            "Epoch 436: val_loss improved from 317.95773 to 316.99203, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0036 - val_loss: 316.9920 - lr: 2.5000e-04\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0036\n",
            "Epoch 437: val_loss improved from 316.99203 to 316.02011, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0036 - val_loss: 316.0201 - lr: 2.5000e-04\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0035\n",
            "Epoch 438: val_loss improved from 316.02011 to 315.04590, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0035 - val_loss: 315.0459 - lr: 2.5000e-04\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0035\n",
            "Epoch 439: val_loss improved from 315.04590 to 314.07269, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0035 - val_loss: 314.0727 - lr: 2.5000e-04\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0035\n",
            "Epoch 440: val_loss improved from 314.07269 to 313.10873, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0035 - val_loss: 313.1087 - lr: 2.5000e-04\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0035\n",
            "Epoch 441: val_loss improved from 313.10873 to 312.15341, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0035 - val_loss: 312.1534 - lr: 2.5000e-04\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0035\n",
            "Epoch 442: val_loss improved from 312.15341 to 311.19907, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0035 - val_loss: 311.1991 - lr: 2.5000e-04\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0035\n",
            "Epoch 443: val_loss improved from 311.19907 to 310.23773, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0035 - val_loss: 310.2377 - lr: 2.5000e-04\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0035\n",
            "Epoch 444: val_loss improved from 310.23773 to 309.27679, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0035 - val_loss: 309.2768 - lr: 2.5000e-04\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0034\n",
            "Epoch 445: val_loss improved from 309.27679 to 308.32724, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0034 - val_loss: 308.3272 - lr: 2.5000e-04\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0034\n",
            "Epoch 446: val_loss improved from 308.32724 to 307.38785, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0034 - val_loss: 307.3878 - lr: 2.5000e-04\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0034\n",
            "Epoch 447: val_loss improved from 307.38785 to 306.44482, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0034 - val_loss: 306.4448 - lr: 2.5000e-04\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0034\n",
            "Epoch 448: val_loss improved from 306.44482 to 305.49698, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0034 - val_loss: 305.4970 - lr: 2.5000e-04\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0034\n",
            "Epoch 449: val_loss improved from 305.49698 to 304.55225, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0034 - val_loss: 304.5522 - lr: 2.5000e-04\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0034\n",
            "Epoch 450: val_loss improved from 304.55225 to 303.61380, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0034 - val_loss: 303.6138 - lr: 2.5000e-04\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0034\n",
            "Epoch 451: val_loss improved from 303.61380 to 302.68182, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0034 - val_loss: 302.6818 - lr: 1.2500e-04\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 452: val_loss improved from 302.68182 to 301.75220, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0033 - val_loss: 301.7522 - lr: 1.2500e-04\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 453: val_loss improved from 301.75220 to 300.82346, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0033 - val_loss: 300.8235 - lr: 1.2500e-04\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 454: val_loss improved from 300.82346 to 299.89594, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0033 - val_loss: 299.8959 - lr: 1.2500e-04\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 455: val_loss improved from 299.89594 to 298.96988, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0033 - val_loss: 298.9699 - lr: 1.2500e-04\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 456: val_loss improved from 298.96988 to 298.04767, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 10s 10s/step - loss: 0.0033 - val_loss: 298.0477 - lr: 1.2500e-04\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 457: val_loss improved from 298.04767 to 297.13046, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0033 - val_loss: 297.1305 - lr: 1.2500e-04\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 458: val_loss improved from 297.13046 to 296.21643, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0033 - val_loss: 296.2164 - lr: 1.2500e-04\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 459: val_loss improved from 296.21643 to 295.30478, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0033 - val_loss: 295.3048 - lr: 1.2500e-04\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 460: val_loss improved from 295.30478 to 294.39740, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0033 - val_loss: 294.3974 - lr: 1.2500e-04\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 461: val_loss improved from 294.39740 to 293.49329, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0033 - val_loss: 293.4933 - lr: 1.2500e-04\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 462: val_loss improved from 293.49329 to 292.59216, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0033 - val_loss: 292.5922 - lr: 1.2500e-04\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 463: val_loss improved from 292.59216 to 291.69351, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0033 - val_loss: 291.6935 - lr: 1.2500e-04\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0033\n",
            "Epoch 464: val_loss improved from 291.69351 to 290.79733, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0033 - val_loss: 290.7973 - lr: 1.2500e-04\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 465: val_loss improved from 290.79733 to 289.90387, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0032 - val_loss: 289.9039 - lr: 1.2500e-04\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 466: val_loss improved from 289.90387 to 289.01303, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0032 - val_loss: 289.0130 - lr: 1.2500e-04\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 467: val_loss improved from 289.01303 to 288.12592, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0032 - val_loss: 288.1259 - lr: 1.2500e-04\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 468: val_loss improved from 288.12592 to 287.24347, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0032 - val_loss: 287.2435 - lr: 1.2500e-04\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 469: val_loss improved from 287.24347 to 286.36633, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0032 - val_loss: 286.3663 - lr: 1.2500e-04\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 470: val_loss improved from 286.36633 to 285.49301, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0032 - val_loss: 285.4930 - lr: 1.2500e-04\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 471: val_loss improved from 285.49301 to 284.62164, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0032 - val_loss: 284.6216 - lr: 1.2500e-04\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 472: val_loss improved from 284.62164 to 283.75116, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0032 - val_loss: 283.7512 - lr: 1.2500e-04\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 473: val_loss improved from 283.75116 to 282.88348, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0032 - val_loss: 282.8835 - lr: 1.2500e-04\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 474: val_loss improved from 282.88348 to 282.02008, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0032 - val_loss: 282.0201 - lr: 1.2500e-04\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 475: val_loss improved from 282.02008 to 281.16129, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0032 - val_loss: 281.1613 - lr: 1.2500e-04\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 476: val_loss improved from 281.16129 to 280.30673, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0032 - val_loss: 280.3067 - lr: 1.2500e-04\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 477: val_loss improved from 280.30673 to 279.45609, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0032 - val_loss: 279.4561 - lr: 1.2500e-04\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 478: val_loss improved from 279.45609 to 278.60834, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0032 - val_loss: 278.6083 - lr: 1.2500e-04\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0032\n",
            "Epoch 479: val_loss improved from 278.60834 to 277.76483, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0032 - val_loss: 277.7648 - lr: 1.2500e-04\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 480: val_loss improved from 277.76483 to 276.92487, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0031 - val_loss: 276.9249 - lr: 1.2500e-04\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 481: val_loss improved from 276.92487 to 276.08768, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0031 - val_loss: 276.0877 - lr: 1.2500e-04\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 482: val_loss improved from 276.08768 to 275.25433, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0031 - val_loss: 275.2543 - lr: 1.2500e-04\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 483: val_loss improved from 275.25433 to 274.42502, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0031 - val_loss: 274.4250 - lr: 1.2500e-04\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 484: val_loss improved from 274.42502 to 273.59885, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0031 - val_loss: 273.5988 - lr: 1.2500e-04\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 485: val_loss improved from 273.59885 to 272.77579, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0031 - val_loss: 272.7758 - lr: 1.2500e-04\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 486: val_loss improved from 272.77579 to 271.95654, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0031 - val_loss: 271.9565 - lr: 1.2500e-04\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 487: val_loss improved from 271.95654 to 271.14218, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0031 - val_loss: 271.1422 - lr: 1.2500e-04\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 488: val_loss improved from 271.14218 to 270.33112, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0031 - val_loss: 270.3311 - lr: 1.2500e-04\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 489: val_loss improved from 270.33112 to 269.52332, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0031 - val_loss: 269.5233 - lr: 1.2500e-04\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 490: val_loss improved from 269.52332 to 268.71945, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0031 - val_loss: 268.7195 - lr: 1.2500e-04\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 491: val_loss improved from 268.71945 to 267.92029, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0031 - val_loss: 267.9203 - lr: 1.2500e-04\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 492: val_loss improved from 267.92029 to 267.12653, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0031 - val_loss: 267.1265 - lr: 1.2500e-04\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 493: val_loss improved from 267.12653 to 266.33673, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0031 - val_loss: 266.3367 - lr: 1.2500e-04\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0030\n",
            "Epoch 494: val_loss improved from 266.33673 to 265.54880, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0030 - val_loss: 265.5488 - lr: 1.2500e-04\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0030\n",
            "Epoch 495: val_loss improved from 265.54880 to 264.76321, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0030 - val_loss: 264.7632 - lr: 1.2500e-04\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0030\n",
            "Epoch 496: val_loss improved from 264.76321 to 263.98178, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0030 - val_loss: 263.9818 - lr: 1.2500e-04\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0030\n",
            "Epoch 497: val_loss improved from 263.98178 to 263.20679, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0030 - val_loss: 263.2068 - lr: 1.2500e-04\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0030\n",
            "Epoch 498: val_loss improved from 263.20679 to 262.43747, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0030 - val_loss: 262.4375 - lr: 1.2500e-04\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0030\n",
            "Epoch 499: val_loss improved from 262.43747 to 261.67215, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0030 - val_loss: 261.6721 - lr: 1.2500e-04\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0030\n",
            "Epoch 500: val_loss improved from 261.67215 to 260.90939, saving model to best_model_cnn_treating_as_real.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0030 - val_loss: 260.9094 - lr: 1.2500e-04\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 260.9094\n",
            "Test Loss: 260.909423828125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 59ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 60ms/step - loss: 260.9094\n",
            "Test Loss (MSE) with Best CNN Model: 260.909423828125\n"
          ]
        }
      ],
      "source": [
        "def learning_rate_schedule(epoch, initial_lr=0.001):\n",
        "    \"\"\"\n",
        "    Custom learning rate schedule. Adjust the learning rate based on the current epoch.\n",
        "    You can customize this function to fit your needs.\n",
        "    \"\"\"\n",
        "    if epoch < 300 or epoch == 300:\n",
        "        return initial_lr  # Keep the initial learning rate for the first 50 epochs\n",
        "    elif epoch > 300 and epoch%50 == 0:\n",
        "        initial_lr = initial_lr * 0.5  # Reduce the learning rate by a factor of 10 after epoch 50\n",
        "        return initial_lr\n",
        "    else:\n",
        "        return initial_lr\n",
        "\n",
        "\n",
        "# Create the Adam optimizer with the initial learning rate\n",
        "initial_learning_rate = 0.001\n",
        "adam_optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
        "\n",
        "checkpoint_1 = ModelCheckpoint('best_model_cnn_treating_as_real.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "model_cnn.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[checkpoint_1, lr_scheduler])\n",
        "\n",
        "loss_1 = model_cnn.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss_1)\n",
        "\n",
        "predictions_1 = model_cnn.predict(X_test)\n",
        "\n",
        "errors_1 = np.abs(predictions_1 - y_test)\n",
        "\n",
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model('best_model_cnn_treating_as_real.h5')\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "test_loss = best_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss (MSE) with Best CNN Model: {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HODmS79x-7r"
      },
      "source": [
        "#Complex-valued network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWBwyfCpaRX9"
      },
      "outputs": [],
      "source": [
        "def normalize_complex_arr(a):\n",
        "    a_oo = a - a.real.min() - 1j*a.imag.min() # origin offsetted\n",
        "    return a_oo/np.abs(a_oo).max()\n",
        "\n",
        "X_normalized_complex = normalize_complex_arr(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhkrHtIgaptz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized_complex, y, test_size=0.245, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwnyGk2aatrT",
        "outputId": "8545624f-ca94-49ac-b5a7-c5db9242667f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03488395+0.46272427j, 0.03776906+0.46753024j,\n",
              "       0.04076648+0.47212205j, ..., 0.20155413+0.44343721j,\n",
              "       0.20217624+0.44346365j, 0.20277801+0.44345772j])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byJwgBKt4Eaz",
        "outputId": "b4788863-a2b3-49b3-936e-80d9e485eba4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2, 50,  8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8yMf46M8lRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e18192-8a69-4f8f-f34d-3585f1dde8f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3, 50, 13],\n",
              "       [ 3, 20,  8],\n",
              "       [ 3, 70, 12],\n",
              "       [ 3, 10,  8],\n",
              "       [ 4, 30, 13],\n",
              "       [ 4, 10, 14],\n",
              "       [ 4, 40, 14],\n",
              "       [ 3, 20,  9],\n",
              "       [ 4, 70, 11],\n",
              "       [ 4, 40,  9],\n",
              "       [ 4, 10,  7],\n",
              "       [ 3, 30, 14],\n",
              "       [ 2, 70, 13],\n",
              "       [ 2, 30,  9],\n",
              "       [ 2, 50, 10],\n",
              "       [ 2, 10, 11],\n",
              "       [ 2, 30,  8],\n",
              "       [ 2, 40, 12],\n",
              "       [ 3, 50, 10],\n",
              "       [ 2, 30, 12],\n",
              "       [ 4, 50,  7],\n",
              "       [ 4, 20,  7],\n",
              "       [ 3, 30,  8],\n",
              "       [ 4, 40,  7],\n",
              "       [ 3, 50,  7],\n",
              "       [ 3, 20,  7],\n",
              "       [ 4, 50, 10],\n",
              "       [ 3, 70, 11],\n",
              "       [ 3, 60, 10],\n",
              "       [ 3, 60,  9],\n",
              "       [ 4, 70,  8],\n",
              "       [ 3, 70,  9],\n",
              "       [ 2, 60, 14],\n",
              "       [ 2, 60, 12],\n",
              "       [ 2, 60, 10],\n",
              "       [ 3, 40, 13],\n",
              "       [ 2, 20,  7],\n",
              "       [ 3, 20, 11],\n",
              "       [ 3, 20, 13],\n",
              "       [ 2, 30, 14],\n",
              "       [ 2, 60, 11],\n",
              "       [ 3, 20, 12]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vgf-n1gyCBM",
        "outputId": "3e0a69c5-6614-4bf4-e2c1-3ad90831c3c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cvnn[full]\n",
            "  Downloading cvnn-2.0.tar.gz (59 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m51.2/59.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (0.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (1.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (23.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (1.11.4)\n",
            "Collecting colorlog (from cvnn[full])\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (3.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (4.66.2)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (3.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (0.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from cvnn[full]) (5.15.0)\n",
            "Collecting tikzplotlib (from cvnn[full])\n",
            "  Downloading tikzplotlib-0.10.1-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (1.62.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->cvnn[full]) (2.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cvnn[full]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cvnn[full]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cvnn[full]) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cvnn[full]) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cvnn[full]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cvnn[full]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cvnn[full]) (2.8.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->cvnn[full]) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cvnn[full]) (2023.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->cvnn[full]) (8.2.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->cvnn[full]) (0.2.13)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->cvnn[full]) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->cvnn[full]) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->cvnn[full]) (0.1.8)\n",
            "Requirement already satisfied: webcolors in /usr/local/lib/python3.10/dist-packages (from tikzplotlib->cvnn[full]) (1.13)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->cvnn[full]) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.0->cvnn[full]) (3.2.2)\n",
            "Building wheels for collected packages: cvnn\n",
            "  Building wheel for cvnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cvnn: filename=cvnn-2.0-py2.py3-none-any.whl size=47611 sha256=cf8c055955647d76e96abf9f62c384b9fc4444256a7a14e1cab735bb7d19a0c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/7c/d5/a4816f12ef5f955ed83cd22822d376f57aa0bc00b7ea2c4486\n",
            "Successfully built cvnn\n",
            "Installing collected packages: colorlog, tikzplotlib, cvnn\n",
            "Successfully installed colorlog-6.8.2 cvnn-2.0 tikzplotlib-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install cvnn[full]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6joaEuO6yzNK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cvnn.layers as complex_layers\n",
        "import tensorflow as tf\n",
        "# import cvnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgO5ZHGisLsC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class ComplexToReal(Layer):\n",
        "    def __init__(self):\n",
        "        super(ComplexToReal, self).__init__()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        real = tf.math.real(inputs)  # Extract real part\n",
        "        imag = tf.math.imag(inputs)  # Extract imaginary part\n",
        "\n",
        "        output = tf.concat([real, imag], axis=-1)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpi-qEX5tRpf"
      },
      "outputs": [],
      "source": [
        "def learning_rate_schedule(epoch, initial_lr=0.001):\n",
        "    \"\"\"\n",
        "    Custom learning rate schedule. Adjust the learning rate based on the current epoch.\n",
        "    You can customize this function to fit your needs.\n",
        "    \"\"\"\n",
        "    if epoch < 300 or epoch == 300:\n",
        "        return initial_lr  # Keep the initial learning rate for the first 50 epochs\n",
        "    elif epoch > 300 and epoch%50 == 0:\n",
        "        initial_lr = initial_lr * 0.75  # Reduce the learning rate by a factor of 10 after epoch 50\n",
        "        return initial_lr\n",
        "    else:\n",
        "        return initial_lr\n",
        "\n",
        "\n",
        "# Create the Adam optimizer with the initial learning rate\n",
        "initial_learning_rate = 0.001\n",
        "adam_optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single complex-valued network"
      ],
      "metadata": {
        "id": "zqzJk1RHj3P_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyfenwQ7zFFp"
      },
      "outputs": [],
      "source": [
        "model_complex = tf.keras.models.Sequential()\n",
        "model_complex.add(complex_layers.ComplexInput(input_shape=(4004,), dtype = tf.complex64))\n",
        "#model_complex.add(complex_layers.ComplexFlatten())\n",
        "# model_complex.add(complex_layers.ComplexDense(units=64, activation='cart_leaky_relu', dtype = tf.complex64))\n",
        "# model_complex.add(complex_layers.ComplexDense(units=128, activation='cart_leaky_relu', dtype = tf.complex64))\n",
        "model_complex.add(complex_layers.ComplexDense(units=512, activation='cart_leaky_relu'))\n",
        "model_complex.add(complex_layers.ComplexDense(units=128, activation='cart_leaky_relu'))\n",
        "model_complex.add(complex_layers.ComplexDense(units=64, activation='cart_leaky_relu'))\n",
        "model_complex.add(complex_layers.ComplexDense(units=32, activation='cart_leaky_relu'))\n",
        "# model_complex.add(complex_layers.ComplexDense(units=16, activation='cart_leaky_relu'))\n",
        "#model_complex.add(complex_layers.ComplexDense(units=3, activation='cart_leaky_relu'))\n",
        "# model_complex.add(Dense(units=3, activation='linear'))\n",
        "model_complex.add(ComplexToReal())\n",
        "model_complex.add(Dense(units=3, activation='linear'))\n",
        "model_complex.compile(loss='mean_squared_error', optimizer=adam_optimizer, metrics=[tf.keras.metrics.MeanSquaredError(dtype=tf.complex64)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnGbBG07XgGa",
        "outputId": "8e54c0c2-fb91-43c8-cd9d-234aba17279d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " complex_dense_5 (ComplexDe  (None, 512)               4101120   \n",
            " nse)                                                            \n",
            "                                                                 \n",
            " complex_dense_6 (ComplexDe  (None, 128)               131328    \n",
            " nse)                                                            \n",
            "                                                                 \n",
            " complex_dense_7 (ComplexDe  (None, 64)                16512     \n",
            " nse)                                                            \n",
            "                                                                 \n",
            " complex_dense_8 (ComplexDe  (None, 32)                4160      \n",
            " nse)                                                            \n",
            "                                                                 \n",
            " complex_to_real_1 (Complex  (None, 64)                0         \n",
            " ToReal)                                                         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4253315 (16.23 MB)\n",
            "Trainable params: 4253315 (16.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_complex.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWFVLqwAV_1d",
        "outputId": "73e5e257-966c-48c1-c5ce-6e3c67167008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 698.7119 - mean_squared_error: 698.7120+0.0000j\n",
            "Epoch 1: val_loss improved from inf to 163.11472, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 698.7119 - mean_squared_error: 698.7120+0.0000j - val_loss: 163.1147 - val_mean_squared_error: 163.1147+0.0000j - lr: 0.0010\n",
            "Epoch 2/500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r1/1 [==============================] - ETA: 0s - loss: 168.2360 - mean_squared_error: 168.2360+0.0000j\n",
            "Epoch 2: val_loss did not improve from 163.11472\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 168.2360 - mean_squared_error: 168.2360+0.0000j - val_loss: 171.3313 - val_mean_squared_error: 171.3313+0.0000j - lr: 0.0010\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 185.2742 - mean_squared_error: 185.2742+0.0000j\n",
            "Epoch 3: val_loss improved from 163.11472 to 111.55620, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 185.2742 - mean_squared_error: 185.2742+0.0000j - val_loss: 111.5562 - val_mean_squared_error: 111.5562+0.0000j - lr: 0.0010\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 115.1415 - mean_squared_error: 115.1415+0.0000j\n",
            "Epoch 4: val_loss improved from 111.55620 to 57.46763, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 115.1415 - mean_squared_error: 115.1415+0.0000j - val_loss: 57.4676 - val_mean_squared_error: 57.4676+0.0000j - lr: 0.0010\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 58.1938 - mean_squared_error: 58.1938+0.0000j\n",
            "Epoch 5: val_loss improved from 57.46763 to 20.05910, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 58.1938 - mean_squared_error: 58.1938+0.0000j - val_loss: 20.0591 - val_mean_squared_error: 20.0591+0.0000j - lr: 0.0010\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 24.1861 - mean_squared_error: 24.1861+0.0000j\n",
            "Epoch 6: val_loss did not improve from 20.05910\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 24.1861 - mean_squared_error: 24.1861+0.0000j - val_loss: 43.9508 - val_mean_squared_error: 43.9508+0.0000j - lr: 0.0010\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 54.1657 - mean_squared_error: 54.1657+0.0000j\n",
            "Epoch 7: val_loss did not improve from 20.05910\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 54.1657 - mean_squared_error: 54.1657+0.0000j - val_loss: 32.8088 - val_mean_squared_error: 32.8088+0.0000j - lr: 0.0010\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 43.1508 - mean_squared_error: 43.1508+0.0000j\n",
            "Epoch 8: val_loss improved from 20.05910 to 11.38645, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 446ms/step - loss: 43.1508 - mean_squared_error: 43.1508+0.0000j - val_loss: 11.3865 - val_mean_squared_error: 11.3865+0.0000j - lr: 0.0010\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.3420 - mean_squared_error: 19.3420+0.0000j\n",
            "Epoch 9: val_loss did not improve from 11.38645\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 19.3420 - mean_squared_error: 19.3420+0.0000j - val_loss: 17.5937 - val_mean_squared_error: 17.5937+0.0000j - lr: 0.0010\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 23.2888 - mean_squared_error: 23.2888+0.0000j\n",
            "Epoch 10: val_loss did not improve from 11.38645\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 23.2888 - mean_squared_error: 23.2888+0.0000j - val_loss: 30.5748 - val_mean_squared_error: 30.5748+0.0000j - lr: 0.0010\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 35.4760 - mean_squared_error: 35.4760+0.0000j\n",
            "Epoch 11: val_loss did not improve from 11.38645\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 35.4760 - mean_squared_error: 35.4760+0.0000j - val_loss: 19.0725 - val_mean_squared_error: 19.0725+0.0000j - lr: 0.0010\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 24.8332 - mean_squared_error: 24.8332+0.0000j\n",
            "Epoch 12: val_loss improved from 11.38645 to 6.34694, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 24.8332 - mean_squared_error: 24.8332+0.0000j - val_loss: 6.3469 - val_mean_squared_error: 6.3469+0.0000j - lr: 0.0010\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.1189 - mean_squared_error: 14.1189+0.0000j\n",
            "Epoch 13: val_loss did not improve from 6.34694\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 14.1189 - mean_squared_error: 14.1189+0.0000j - val_loss: 10.9420 - val_mean_squared_error: 10.9420+0.0000j - lr: 0.0010\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 20.4306 - mean_squared_error: 20.4306+0.0000j\n",
            "Epoch 14: val_loss did not improve from 6.34694\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 20.4306 - mean_squared_error: 20.4306+0.0000j - val_loss: 15.2606 - val_mean_squared_error: 15.2606+0.0000j - lr: 0.0010\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 24.9770 - mean_squared_error: 24.9770+0.0000j\n",
            "Epoch 15: val_loss did not improve from 6.34694\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 24.9770 - mean_squared_error: 24.9770+0.0000j - val_loss: 11.4993 - val_mean_squared_error: 11.4993+0.0000j - lr: 0.0010\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 19.9938 - mean_squared_error: 19.9938+0.0000j\n",
            "Epoch 16: val_loss did not improve from 6.34694\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 19.9938 - mean_squared_error: 19.9938+0.0000j - val_loss: 8.2249 - val_mean_squared_error: 8.2249+0.0000j - lr: 0.0010\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.1353 - mean_squared_error: 15.1353+0.0000j\n",
            "Epoch 17: val_loss did not improve from 6.34694\n",
            "1/1 [==============================] - 0s 445ms/step - loss: 15.1353 - mean_squared_error: 15.1353+0.0000j - val_loss: 9.2070 - val_mean_squared_error: 9.2070+0.0000j - lr: 0.0010\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.2513 - mean_squared_error: 15.2513+0.0000j\n",
            "Epoch 18: val_loss did not improve from 6.34694\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 15.2513 - mean_squared_error: 15.2513+0.0000j - val_loss: 11.8293 - val_mean_squared_error: 11.8293+0.0000j - lr: 0.0010\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.1854 - mean_squared_error: 18.1854+0.0000j\n",
            "Epoch 19: val_loss did not improve from 6.34694\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 18.1854 - mean_squared_error: 18.1854+0.0000j - val_loss: 11.4897 - val_mean_squared_error: 11.4897+0.0000j - lr: 0.0010\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.6208 - mean_squared_error: 18.6208+0.0000j\n",
            "Epoch 20: val_loss did not improve from 6.34694\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 18.6208 - mean_squared_error: 18.6208+0.0000j - val_loss: 7.2150 - val_mean_squared_error: 7.2150+0.0000j - lr: 0.0010\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.6425 - mean_squared_error: 14.6425+0.0000j\n",
            "Epoch 21: val_loss improved from 6.34694 to 5.48895, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 751ms/step - loss: 14.6425 - mean_squared_error: 14.6425+0.0000j - val_loss: 5.4889 - val_mean_squared_error: 5.4889+0.0000j - lr: 0.0010\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 12.5398 - mean_squared_error: 12.5398+0.0000j\n",
            "Epoch 22: val_loss did not improve from 5.48895\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 12.5398 - mean_squared_error: 12.5398+0.0000j - val_loss: 8.4982 - val_mean_squared_error: 8.4982+0.0000j - lr: 0.0010\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.1486 - mean_squared_error: 15.1486+0.0000j\n",
            "Epoch 23: val_loss did not improve from 5.48895\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 15.1486 - mean_squared_error: 15.1486+0.0000j - val_loss: 9.7012 - val_mean_squared_error: 9.7012+0.0000j - lr: 0.0010\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 16.3000 - mean_squared_error: 16.3000+0.0000j\n",
            "Epoch 24: val_loss did not improve from 5.48895\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 16.3000 - mean_squared_error: 16.3000+0.0000j - val_loss: 6.9693 - val_mean_squared_error: 6.9693+0.0000j - lr: 0.0010\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.8226 - mean_squared_error: 13.8226+0.0000j\n",
            "Epoch 25: val_loss improved from 5.48895 to 4.66689, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 576ms/step - loss: 13.8226 - mean_squared_error: 13.8226+0.0000j - val_loss: 4.6669 - val_mean_squared_error: 4.6669+0.0000j - lr: 0.0010\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.9577 - mean_squared_error: 11.9577+0.0000j\n",
            "Epoch 26: val_loss did not improve from 4.66689\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 11.9577 - mean_squared_error: 11.9577+0.0000j - val_loss: 5.8156 - val_mean_squared_error: 5.8156+0.0000j - lr: 0.0010\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.2631 - mean_squared_error: 13.2631+0.0000j\n",
            "Epoch 27: val_loss did not improve from 4.66689\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 13.2631 - mean_squared_error: 13.2631+0.0000j - val_loss: 7.1497 - val_mean_squared_error: 7.1497+0.0000j - lr: 0.0010\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 14.4274 - mean_squared_error: 14.4274+0.0000j\n",
            "Epoch 28: val_loss did not improve from 4.66689\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 14.4274 - mean_squared_error: 14.4274+0.0000j - val_loss: 6.4772 - val_mean_squared_error: 6.4772+0.0000j - lr: 0.0010\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.4114 - mean_squared_error: 13.4114+0.0000j\n",
            "Epoch 29: val_loss did not improve from 4.66689\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 13.4114 - mean_squared_error: 13.4114+0.0000j - val_loss: 5.2960 - val_mean_squared_error: 5.2960+0.0000j - lr: 0.0010\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 12.0771 - mean_squared_error: 12.0771+0.0000j\n",
            "Epoch 30: val_loss did not improve from 4.66689\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 12.0771 - mean_squared_error: 12.0771+0.0000j - val_loss: 4.9698 - val_mean_squared_error: 4.9698+0.0000j - lr: 0.0010\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.9545 - mean_squared_error: 11.9545+0.0000j\n",
            "Epoch 31: val_loss did not improve from 4.66689\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 11.9545 - mean_squared_error: 11.9545+0.0000j - val_loss: 5.3513 - val_mean_squared_error: 5.3513+0.0000j - lr: 0.0010\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 12.8021 - mean_squared_error: 12.8021+0.0000j\n",
            "Epoch 32: val_loss did not improve from 4.66689\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 12.8021 - mean_squared_error: 12.8021+0.0000j - val_loss: 5.3171 - val_mean_squared_error: 5.3171+0.0000j - lr: 0.0010\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 13.0865 - mean_squared_error: 13.0865+0.0000j\n",
            "Epoch 33: val_loss improved from 4.66689 to 4.41051, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 13.0865 - mean_squared_error: 13.0865+0.0000j - val_loss: 4.4105 - val_mean_squared_error: 4.4105+0.0000j - lr: 0.0010\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 12.1521 - mean_squared_error: 12.1521+0.0000j\n",
            "Epoch 34: val_loss improved from 4.41051 to 4.11330, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 12.1521 - mean_squared_error: 12.1521+0.0000j - val_loss: 4.1133 - val_mean_squared_error: 4.1133+0.0000j - lr: 0.0010\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.4909 - mean_squared_error: 11.4909+0.0000j\n",
            "Epoch 35: val_loss did not improve from 4.11330\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 11.4909 - mean_squared_error: 11.4909+0.0000j - val_loss: 5.1322 - val_mean_squared_error: 5.1322+0.0000j - lr: 0.0010\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 12.0697 - mean_squared_error: 12.0697+0.0000j\n",
            "Epoch 36: val_loss did not improve from 4.11330\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 12.0697 - mean_squared_error: 12.0697+0.0000j - val_loss: 5.6874 - val_mean_squared_error: 5.6874+0.0000j - lr: 0.0010\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 12.4596 - mean_squared_error: 12.4596+0.0000j\n",
            "Epoch 37: val_loss did not improve from 4.11330\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 12.4596 - mean_squared_error: 12.4596+0.0000j - val_loss: 4.8223 - val_mean_squared_error: 4.8223+0.0000j - lr: 0.0010\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.8121 - mean_squared_error: 11.8121+0.0000j\n",
            "Epoch 38: val_loss improved from 4.11330 to 3.97374, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 11.8121 - mean_squared_error: 11.8121+0.0000j - val_loss: 3.9737 - val_mean_squared_error: 3.9737+0.0000j - lr: 0.0010\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.3625 - mean_squared_error: 11.3625+0.0000j\n",
            "Epoch 39: val_loss did not improve from 3.97374\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 11.3625 - mean_squared_error: 11.3625+0.0000j - val_loss: 4.0741 - val_mean_squared_error: 4.0741+0.0000j - lr: 0.0010\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.7202 - mean_squared_error: 11.7202+0.0000j\n",
            "Epoch 40: val_loss did not improve from 3.97374\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 11.7202 - mean_squared_error: 11.7202+0.0000j - val_loss: 4.3877 - val_mean_squared_error: 4.3877+0.0000j - lr: 0.0010\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.9053 - mean_squared_error: 11.9053+0.0000j\n",
            "Epoch 41: val_loss did not improve from 3.97374\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 11.9053 - mean_squared_error: 11.9053+0.0000j - val_loss: 4.3892 - val_mean_squared_error: 4.3892+0.0000j - lr: 0.0010\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.5635 - mean_squared_error: 11.5635+0.0000j\n",
            "Epoch 42: val_loss did not improve from 3.97374\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 11.5635 - mean_squared_error: 11.5635+0.0000j - val_loss: 4.3235 - val_mean_squared_error: 4.3235+0.0000j - lr: 0.0010\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.2483 - mean_squared_error: 11.2483+0.0000j\n",
            "Epoch 43: val_loss did not improve from 3.97374\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 11.2483 - mean_squared_error: 11.2483+0.0000j - val_loss: 4.3757 - val_mean_squared_error: 4.3757+0.0000j - lr: 0.0010\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.3162 - mean_squared_error: 11.3162+0.0000j\n",
            "Epoch 44: val_loss did not improve from 3.97374\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 11.3162 - mean_squared_error: 11.3162+0.0000j - val_loss: 4.4479 - val_mean_squared_error: 4.4479+0.0000j - lr: 0.0010\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.5408 - mean_squared_error: 11.5408+0.0000j\n",
            "Epoch 45: val_loss did not improve from 3.97374\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 11.5408 - mean_squared_error: 11.5408+0.0000j - val_loss: 4.2232 - val_mean_squared_error: 4.2232+0.0000j - lr: 0.0010\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.4364 - mean_squared_error: 11.4364+0.0000j\n",
            "Epoch 46: val_loss improved from 3.97374 to 3.87871, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 11.4364 - mean_squared_error: 11.4364+0.0000j - val_loss: 3.8787 - val_mean_squared_error: 3.8787+0.0000j - lr: 0.0010\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.1062 - mean_squared_error: 11.1062+0.0000j\n",
            "Epoch 47: val_loss did not improve from 3.87871\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 11.1062 - mean_squared_error: 11.1062+0.0000j - val_loss: 3.9844 - val_mean_squared_error: 3.9844+0.0000j - lr: 0.0010\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.1294 - mean_squared_error: 11.1294+0.0000j\n",
            "Epoch 48: val_loss did not improve from 3.87871\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 11.1294 - mean_squared_error: 11.1294+0.0000j - val_loss: 4.2409 - val_mean_squared_error: 4.2409+0.0000j - lr: 0.0010\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.3275 - mean_squared_error: 11.3275+0.0000j\n",
            "Epoch 49: val_loss did not improve from 3.87871\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 11.3275 - mean_squared_error: 11.3275+0.0000j - val_loss: 4.0112 - val_mean_squared_error: 4.0112+0.0000j - lr: 0.0010\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.1712 - mean_squared_error: 11.1712+0.0000j\n",
            "Epoch 50: val_loss improved from 3.87871 to 3.65872, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 11.1712 - mean_squared_error: 11.1712+0.0000j - val_loss: 3.6587 - val_mean_squared_error: 3.6587+0.0000j - lr: 0.0010\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.9672 - mean_squared_error: 10.9672+0.0000j\n",
            "Epoch 51: val_loss did not improve from 3.65872\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 10.9672 - mean_squared_error: 10.9672+0.0000j - val_loss: 3.7005 - val_mean_squared_error: 3.7005+0.0000j - lr: 0.0010\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.0319 - mean_squared_error: 11.0319+0.0000j\n",
            "Epoch 52: val_loss did not improve from 3.65872\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 11.0319 - mean_squared_error: 11.0319+0.0000j - val_loss: 3.8950 - val_mean_squared_error: 3.8950+0.0000j - lr: 0.0010\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.0818 - mean_squared_error: 11.0818+0.0000j\n",
            "Epoch 53: val_loss did not improve from 3.65872\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 11.0818 - mean_squared_error: 11.0818+0.0000j - val_loss: 3.9859 - val_mean_squared_error: 3.9859+0.0000j - lr: 0.0010\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.9988 - mean_squared_error: 10.9988+0.0000j\n",
            "Epoch 54: val_loss did not improve from 3.65872\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 10.9988 - mean_squared_error: 10.9988+0.0000j - val_loss: 3.8956 - val_mean_squared_error: 3.8956+0.0000j - lr: 0.0010\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.9036 - mean_squared_error: 10.9036+0.0000j\n",
            "Epoch 55: val_loss did not improve from 3.65872\n",
            "1/1 [==============================] - 0s 437ms/step - loss: 10.9036 - mean_squared_error: 10.9036+0.0000j - val_loss: 3.7185 - val_mean_squared_error: 3.7185+0.0000j - lr: 0.0010\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.9102 - mean_squared_error: 10.9102+0.0000j\n",
            "Epoch 56: val_loss improved from 3.65872 to 3.59793, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 10.9102 - mean_squared_error: 10.9102+0.0000j - val_loss: 3.5979 - val_mean_squared_error: 3.5979+0.0000j - lr: 0.0010\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.9770 - mean_squared_error: 10.9770+0.0000j\n",
            "Epoch 57: val_loss improved from 3.59793 to 3.53476, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 10.9770 - mean_squared_error: 10.9770+0.0000j - val_loss: 3.5348 - val_mean_squared_error: 3.5348+0.0000j - lr: 0.0010\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.8947 - mean_squared_error: 10.8947+0.0000j\n",
            "Epoch 58: val_loss did not improve from 3.53476\n",
            "1/1 [==============================] - 0s 425ms/step - loss: 10.8947 - mean_squared_error: 10.8947+0.0000j - val_loss: 3.6254 - val_mean_squared_error: 3.6254+0.0000j - lr: 0.0010\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.7996 - mean_squared_error: 10.7996+0.0000j\n",
            "Epoch 59: val_loss did not improve from 3.53476\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 10.7996 - mean_squared_error: 10.7996+0.0000j - val_loss: 3.8311 - val_mean_squared_error: 3.8311+0.0000j - lr: 0.0010\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.8571 - mean_squared_error: 10.8571+0.0000j\n",
            "Epoch 60: val_loss did not improve from 3.53476\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 10.8571 - mean_squared_error: 10.8571+0.0000j - val_loss: 3.8042 - val_mean_squared_error: 3.8042+0.0000j - lr: 0.0010\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.8658 - mean_squared_error: 10.8658+0.0000j\n",
            "Epoch 61: val_loss did not improve from 3.53476\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 10.8658 - mean_squared_error: 10.8658+0.0000j - val_loss: 3.5724 - val_mean_squared_error: 3.5724+0.0000j - lr: 0.0010\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.7884 - mean_squared_error: 10.7884+0.0000j\n",
            "Epoch 62: val_loss improved from 3.53476 to 3.42663, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 10.7884 - mean_squared_error: 10.7884+0.0000j - val_loss: 3.4266 - val_mean_squared_error: 3.4266+0.0000j - lr: 0.0010\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.7578 - mean_squared_error: 10.7578+0.0000j\n",
            "Epoch 63: val_loss did not improve from 3.42663\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 10.7578 - mean_squared_error: 10.7578+0.0000j - val_loss: 3.4619 - val_mean_squared_error: 3.4619+0.0000j - lr: 0.0010\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.7688 - mean_squared_error: 10.7688+0.0000j\n",
            "Epoch 64: val_loss did not improve from 3.42663\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 10.7688 - mean_squared_error: 10.7688+0.0000j - val_loss: 3.5465 - val_mean_squared_error: 3.5465+0.0000j - lr: 0.0010\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.7640 - mean_squared_error: 10.7640+0.0000j\n",
            "Epoch 65: val_loss did not improve from 3.42663\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 10.7640 - mean_squared_error: 10.7640+0.0000j - val_loss: 3.5268 - val_mean_squared_error: 3.5268+0.0000j - lr: 0.0010\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.7113 - mean_squared_error: 10.7113+0.0000j\n",
            "Epoch 66: val_loss did not improve from 3.42663\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 10.7113 - mean_squared_error: 10.7113+0.0000j - val_loss: 3.4597 - val_mean_squared_error: 3.4597+0.0000j - lr: 0.0010\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.6877 - mean_squared_error: 10.6877+0.0000j\n",
            "Epoch 67: val_loss did not improve from 3.42663\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 10.6877 - mean_squared_error: 10.6877+0.0000j - val_loss: 3.4514 - val_mean_squared_error: 3.4514+0.0000j - lr: 0.0010\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.7127 - mean_squared_error: 10.7127+0.0000j\n",
            "Epoch 68: val_loss did not improve from 3.42663\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 10.7127 - mean_squared_error: 10.7127+0.0000j - val_loss: 3.4723 - val_mean_squared_error: 3.4723+0.0000j - lr: 0.0010\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.6822 - mean_squared_error: 10.6822+0.0000j\n",
            "Epoch 69: val_loss did not improve from 3.42663\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 10.6822 - mean_squared_error: 10.6822+0.0000j - val_loss: 3.5251 - val_mean_squared_error: 3.5251+0.0000j - lr: 0.0010\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.6428 - mean_squared_error: 10.6428+0.0000j\n",
            "Epoch 70: val_loss did not improve from 3.42663\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 10.6428 - mean_squared_error: 10.6428+0.0000j - val_loss: 3.5588 - val_mean_squared_error: 3.5588+0.0000j - lr: 0.0010\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.6497 - mean_squared_error: 10.6497+0.0000j\n",
            "Epoch 71: val_loss did not improve from 3.42663\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 10.6497 - mean_squared_error: 10.6497+0.0000j - val_loss: 3.4822 - val_mean_squared_error: 3.4822+0.0000j - lr: 0.0010\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.6428 - mean_squared_error: 10.6428+0.0000j\n",
            "Epoch 72: val_loss improved from 3.42663 to 3.38127, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 10.6428 - mean_squared_error: 10.6428+0.0000j - val_loss: 3.3813 - val_mean_squared_error: 3.3813+0.0000j - lr: 0.0010\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.6119 - mean_squared_error: 10.6119+0.0000j\n",
            "Epoch 73: val_loss improved from 3.38127 to 3.33692, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 10.6119 - mean_squared_error: 10.6119+0.0000j - val_loss: 3.3369 - val_mean_squared_error: 3.3369+0.0000j - lr: 0.0010\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.5931 - mean_squared_error: 10.5931+0.0000j\n",
            "Epoch 74: val_loss did not improve from 3.33692\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 10.5931 - mean_squared_error: 10.5931+0.0000j - val_loss: 3.3593 - val_mean_squared_error: 3.3593+0.0000j - lr: 0.0010\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.5872 - mean_squared_error: 10.5872+0.0000j\n",
            "Epoch 75: val_loss did not improve from 3.33692\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 10.5872 - mean_squared_error: 10.5872+0.0000j - val_loss: 3.3971 - val_mean_squared_error: 3.3971+0.0000j - lr: 0.0010\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.5783 - mean_squared_error: 10.5783+0.0000j\n",
            "Epoch 76: val_loss did not improve from 3.33692\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 10.5783 - mean_squared_error: 10.5783+0.0000j - val_loss: 3.3599 - val_mean_squared_error: 3.3599+0.0000j - lr: 0.0010\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.5507 - mean_squared_error: 10.5507+0.0000j\n",
            "Epoch 77: val_loss improved from 3.33692 to 3.26556, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 10.5507 - mean_squared_error: 10.5507+0.0000j - val_loss: 3.2656 - val_mean_squared_error: 3.2656+0.0000j - lr: 0.0010\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.5349 - mean_squared_error: 10.5349+0.0000j\n",
            "Epoch 78: val_loss improved from 3.26556 to 3.26226, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 10.5349 - mean_squared_error: 10.5349+0.0000j - val_loss: 3.2623 - val_mean_squared_error: 3.2623+0.0000j - lr: 0.0010\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.5273 - mean_squared_error: 10.5273+0.0000j\n",
            "Epoch 79: val_loss did not improve from 3.26226\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 10.5273 - mean_squared_error: 10.5273+0.0000j - val_loss: 3.2878 - val_mean_squared_error: 3.2878+0.0000j - lr: 0.0010\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.4986 - mean_squared_error: 10.4986+0.0000j\n",
            "Epoch 80: val_loss did not improve from 3.26226\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 10.4986 - mean_squared_error: 10.4986+0.0000j - val_loss: 3.3580 - val_mean_squared_error: 3.3580+0.0000j - lr: 0.0010\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.4754 - mean_squared_error: 10.4754+0.0000j\n",
            "Epoch 81: val_loss did not improve from 3.26226\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 10.4754 - mean_squared_error: 10.4754+0.0000j - val_loss: 3.3610 - val_mean_squared_error: 3.3610+0.0000j - lr: 0.0010\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.4681 - mean_squared_error: 10.4681+0.0000j\n",
            "Epoch 82: val_loss improved from 3.26226 to 3.23532, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 10.4681 - mean_squared_error: 10.4681+0.0000j - val_loss: 3.2353 - val_mean_squared_error: 3.2353+0.0000j - lr: 0.0010\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.4558 - mean_squared_error: 10.4558+0.0000j\n",
            "Epoch 83: val_loss improved from 3.23532 to 3.16450, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 796ms/step - loss: 10.4558 - mean_squared_error: 10.4558+0.0000j - val_loss: 3.1645 - val_mean_squared_error: 3.1645+0.0000j - lr: 0.0010\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.4421 - mean_squared_error: 10.4421+0.0000j\n",
            "Epoch 84: val_loss did not improve from 3.16450\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 10.4421 - mean_squared_error: 10.4421+0.0000j - val_loss: 3.1830 - val_mean_squared_error: 3.1830+0.0000j - lr: 0.0010\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.4329 - mean_squared_error: 10.4329+0.0000j\n",
            "Epoch 85: val_loss did not improve from 3.16450\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 10.4329 - mean_squared_error: 10.4329+0.0000j - val_loss: 3.2308 - val_mean_squared_error: 3.2308+0.0000j - lr: 0.0010\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.4259 - mean_squared_error: 10.4259+0.0000j\n",
            "Epoch 86: val_loss did not improve from 3.16450\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 10.4259 - mean_squared_error: 10.4259+0.0000j - val_loss: 3.1961 - val_mean_squared_error: 3.1961+0.0000j - lr: 0.0010\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.4047 - mean_squared_error: 10.4047+0.0000j\n",
            "Epoch 87: val_loss improved from 3.16450 to 3.16163, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 10.4047 - mean_squared_error: 10.4047+0.0000j - val_loss: 3.1616 - val_mean_squared_error: 3.1616+0.0000j - lr: 0.0010\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.3945 - mean_squared_error: 10.3945+0.0000j\n",
            "Epoch 88: val_loss improved from 3.16163 to 3.14101, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 760ms/step - loss: 10.3945 - mean_squared_error: 10.3945+0.0000j - val_loss: 3.1410 - val_mean_squared_error: 3.1410+0.0000j - lr: 0.0010\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.3837 - mean_squared_error: 10.3837+0.0000j\n",
            "Epoch 89: val_loss did not improve from 3.14101\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 10.3837 - mean_squared_error: 10.3837+0.0000j - val_loss: 3.1632 - val_mean_squared_error: 3.1632+0.0000j - lr: 0.0010\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.3715 - mean_squared_error: 10.3715+0.0000j\n",
            "Epoch 90: val_loss did not improve from 3.14101\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 10.3715 - mean_squared_error: 10.3715+0.0000j - val_loss: 3.1795 - val_mean_squared_error: 3.1795+0.0000j - lr: 0.0010\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.3604 - mean_squared_error: 10.3604+0.0000j\n",
            "Epoch 91: val_loss improved from 3.14101 to 3.13413, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 717ms/step - loss: 10.3604 - mean_squared_error: 10.3604+0.0000j - val_loss: 3.1341 - val_mean_squared_error: 3.1341+0.0000j - lr: 0.0010\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.3450 - mean_squared_error: 10.3450+0.0000j\n",
            "Epoch 92: val_loss improved from 3.13413 to 3.09176, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 740ms/step - loss: 10.3450 - mean_squared_error: 10.3450+0.0000j - val_loss: 3.0918 - val_mean_squared_error: 3.0918+0.0000j - lr: 0.0010\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.3309 - mean_squared_error: 10.3309+0.0000j\n",
            "Epoch 93: val_loss did not improve from 3.09176\n",
            "1/1 [==============================] - 1s 556ms/step - loss: 10.3309 - mean_squared_error: 10.3309+0.0000j - val_loss: 3.1033 - val_mean_squared_error: 3.1033+0.0000j - lr: 0.0010\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.3156 - mean_squared_error: 10.3156+0.0000j\n",
            "Epoch 94: val_loss did not improve from 3.09176\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 10.3156 - mean_squared_error: 10.3156+0.0000j - val_loss: 3.1283 - val_mean_squared_error: 3.1283+0.0000j - lr: 0.0010\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.3030 - mean_squared_error: 10.3030+0.0000j\n",
            "Epoch 95: val_loss did not improve from 3.09176\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 10.3030 - mean_squared_error: 10.3030+0.0000j - val_loss: 3.1090 - val_mean_squared_error: 3.1090+0.0000j - lr: 0.0010\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.2901 - mean_squared_error: 10.2901+0.0000j\n",
            "Epoch 96: val_loss improved from 3.09176 to 3.05138, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 439ms/step - loss: 10.2901 - mean_squared_error: 10.2901+0.0000j - val_loss: 3.0514 - val_mean_squared_error: 3.0514+0.0000j - lr: 0.0010\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.2813 - mean_squared_error: 10.2813+0.0000j\n",
            "Epoch 97: val_loss improved from 3.05138 to 3.02392, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 450ms/step - loss: 10.2813 - mean_squared_error: 10.2813+0.0000j - val_loss: 3.0239 - val_mean_squared_error: 3.0239+0.0000j - lr: 0.0010\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.2711 - mean_squared_error: 10.2711+0.0000j\n",
            "Epoch 98: val_loss did not improve from 3.02392\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 10.2711 - mean_squared_error: 10.2711+0.0000j - val_loss: 3.0360 - val_mean_squared_error: 3.0360+0.0000j - lr: 0.0010\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.2563 - mean_squared_error: 10.2563+0.0000j\n",
            "Epoch 99: val_loss did not improve from 3.02392\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 10.2563 - mean_squared_error: 10.2563+0.0000j - val_loss: 3.0863 - val_mean_squared_error: 3.0863+0.0000j - lr: 0.0010\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.2540 - mean_squared_error: 10.2540+0.0000j\n",
            "Epoch 100: val_loss did not improve from 3.02392\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 10.2540 - mean_squared_error: 10.2540+0.0000j - val_loss: 3.0396 - val_mean_squared_error: 3.0396+0.0000j - lr: 0.0010\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.2365 - mean_squared_error: 10.2365+0.0000j\n",
            "Epoch 101: val_loss improved from 3.02392 to 2.93742, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 10.2365 - mean_squared_error: 10.2365+0.0000j - val_loss: 2.9374 - val_mean_squared_error: 2.9374+0.0000j - lr: 0.0010\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.2274 - mean_squared_error: 10.2274+0.0000j\n",
            "Epoch 102: val_loss improved from 2.93742 to 2.89624, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 10.2274 - mean_squared_error: 10.2274+0.0000j - val_loss: 2.8962 - val_mean_squared_error: 2.8962+0.0000j - lr: 0.0010\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.2176 - mean_squared_error: 10.2176+0.0000j\n",
            "Epoch 103: val_loss did not improve from 2.89624\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 10.2176 - mean_squared_error: 10.2176+0.0000j - val_loss: 2.9168 - val_mean_squared_error: 2.9168+0.0000j - lr: 0.0010\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.2042 - mean_squared_error: 10.2042+0.0000j\n",
            "Epoch 104: val_loss did not improve from 2.89624\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 10.2042 - mean_squared_error: 10.2042+0.0000j - val_loss: 2.9540 - val_mean_squared_error: 2.9540+0.0000j - lr: 0.0010\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.1944 - mean_squared_error: 10.1944+0.0000j\n",
            "Epoch 105: val_loss did not improve from 2.89624\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 10.1944 - mean_squared_error: 10.1944+0.0000j - val_loss: 2.9594 - val_mean_squared_error: 2.9594+0.0000j - lr: 0.0010\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.1837 - mean_squared_error: 10.1837+0.0000j\n",
            "Epoch 106: val_loss did not improve from 2.89624\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 10.1837 - mean_squared_error: 10.1837+0.0000j - val_loss: 2.9430 - val_mean_squared_error: 2.9430+0.0000j - lr: 0.0010\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.1728 - mean_squared_error: 10.1728+0.0000j\n",
            "Epoch 107: val_loss did not improve from 2.89624\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 10.1728 - mean_squared_error: 10.1728+0.0000j - val_loss: 2.9374 - val_mean_squared_error: 2.9374+0.0000j - lr: 0.0010\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.1611 - mean_squared_error: 10.1611+0.0000j\n",
            "Epoch 108: val_loss did not improve from 2.89624\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 10.1611 - mean_squared_error: 10.1611+0.0000j - val_loss: 2.9415 - val_mean_squared_error: 2.9415+0.0000j - lr: 0.0010\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.1500 - mean_squared_error: 10.1500+0.0000j\n",
            "Epoch 109: val_loss did not improve from 2.89624\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 10.1500 - mean_squared_error: 10.1500+0.0000j - val_loss: 2.9210 - val_mean_squared_error: 2.9210+0.0000j - lr: 0.0010\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.1372 - mean_squared_error: 10.1372+0.0000j\n",
            "Epoch 110: val_loss improved from 2.89624 to 2.88954, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 10.1372 - mean_squared_error: 10.1372+0.0000j - val_loss: 2.8895 - val_mean_squared_error: 2.8895+0.0000j - lr: 0.0010\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.1266 - mean_squared_error: 10.1266+0.0000j\n",
            "Epoch 111: val_loss improved from 2.88954 to 2.87375, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 10.1266 - mean_squared_error: 10.1266+0.0000j - val_loss: 2.8738 - val_mean_squared_error: 2.8738+0.0000j - lr: 0.0010\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.1199 - mean_squared_error: 10.1199+0.0000j\n",
            "Epoch 112: val_loss improved from 2.87375 to 2.86046, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 10.1199 - mean_squared_error: 10.1199+0.0000j - val_loss: 2.8605 - val_mean_squared_error: 2.8605+0.0000j - lr: 0.0010\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.1069 - mean_squared_error: 10.1069+0.0000j\n",
            "Epoch 113: val_loss improved from 2.86046 to 2.84544, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 10.1069 - mean_squared_error: 10.1069+0.0000j - val_loss: 2.8454 - val_mean_squared_error: 2.8454+0.0000j - lr: 0.0010\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0964 - mean_squared_error: 10.0964+0.0000j\n",
            "Epoch 114: val_loss improved from 2.84544 to 2.82233, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 10.0964 - mean_squared_error: 10.0964+0.0000j - val_loss: 2.8223 - val_mean_squared_error: 2.8223+0.0000j - lr: 0.0010\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0824 - mean_squared_error: 10.0824+0.0000j\n",
            "Epoch 115: val_loss improved from 2.82233 to 2.81539, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 10.0824 - mean_squared_error: 10.0824+0.0000j - val_loss: 2.8154 - val_mean_squared_error: 2.8154+0.0000j - lr: 0.0010\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0661 - mean_squared_error: 10.0661+0.0000j\n",
            "Epoch 116: val_loss improved from 2.81539 to 2.77947, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 10.0661 - mean_squared_error: 10.0661+0.0000j - val_loss: 2.7795 - val_mean_squared_error: 2.7795+0.0000j - lr: 0.0010\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0458 - mean_squared_error: 10.0458+0.0000j\n",
            "Epoch 117: val_loss improved from 2.77947 to 2.71718, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 724ms/step - loss: 10.0458 - mean_squared_error: 10.0458+0.0000j - val_loss: 2.7172 - val_mean_squared_error: 2.7172+0.0000j - lr: 0.0010\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0380 - mean_squared_error: 10.0380+0.0000j\n",
            "Epoch 118: val_loss did not improve from 2.71718\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 10.0380 - mean_squared_error: 10.0380+0.0000j - val_loss: 2.7179 - val_mean_squared_error: 2.7179+0.0000j - lr: 0.0010\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0295 - mean_squared_error: 10.0295+0.0000j\n",
            "Epoch 119: val_loss did not improve from 2.71718\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 10.0295 - mean_squared_error: 10.0295+0.0000j - val_loss: 2.7340 - val_mean_squared_error: 2.7340+0.0000j - lr: 0.0010\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0218 - mean_squared_error: 10.0218+0.0000j\n",
            "Epoch 120: val_loss did not improve from 2.71718\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 10.0218 - mean_squared_error: 10.0218+0.0000j - val_loss: 2.7561 - val_mean_squared_error: 2.7561+0.0000j - lr: 0.0010\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0130 - mean_squared_error: 10.0130+0.0000j\n",
            "Epoch 121: val_loss did not improve from 2.71718\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 10.0130 - mean_squared_error: 10.0130+0.0000j - val_loss: 2.7413 - val_mean_squared_error: 2.7413+0.0000j - lr: 0.0010\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0032 - mean_squared_error: 10.0032+0.0000j\n",
            "Epoch 122: val_loss did not improve from 2.71718\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 10.0032 - mean_squared_error: 10.0032+0.0000j - val_loss: 2.7613 - val_mean_squared_error: 2.7613+0.0000j - lr: 0.0010\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9924 - mean_squared_error: 9.9924+0.0000j\n",
            "Epoch 123: val_loss did not improve from 2.71718\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 9.9924 - mean_squared_error: 9.9924+0.0000j - val_loss: 2.7933 - val_mean_squared_error: 2.7933+0.0000j - lr: 0.0010\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9904 - mean_squared_error: 9.9904+0.0000j\n",
            "Epoch 124: val_loss did not improve from 2.71718\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 9.9904 - mean_squared_error: 9.9904+0.0000j - val_loss: 2.7651 - val_mean_squared_error: 2.7651+0.0000j - lr: 0.0010\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9721 - mean_squared_error: 9.9721+0.0000j\n",
            "Epoch 125: val_loss did not improve from 2.71718\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 9.9721 - mean_squared_error: 9.9721+0.0000j - val_loss: 2.7266 - val_mean_squared_error: 2.7266+0.0000j - lr: 0.0010\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9646 - mean_squared_error: 9.9646+0.0000j\n",
            "Epoch 126: val_loss improved from 2.71718 to 2.69489, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 734ms/step - loss: 9.9646 - mean_squared_error: 9.9646+0.0000j - val_loss: 2.6949 - val_mean_squared_error: 2.6949+0.0000j - lr: 0.0010\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9554 - mean_squared_error: 9.9554+0.0000j\n",
            "Epoch 127: val_loss improved from 2.69489 to 2.65209, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 9.9554 - mean_squared_error: 9.9554+0.0000j - val_loss: 2.6521 - val_mean_squared_error: 2.6521+0.0000j - lr: 0.0010\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9436 - mean_squared_error: 9.9436+0.0000j\n",
            "Epoch 128: val_loss improved from 2.65209 to 2.63499, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 9.9436 - mean_squared_error: 9.9436+0.0000j - val_loss: 2.6350 - val_mean_squared_error: 2.6350+0.0000j - lr: 0.0010\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9304 - mean_squared_error: 9.9304+0.0000j\n",
            "Epoch 129: val_loss improved from 2.63499 to 2.61525, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 9.9304 - mean_squared_error: 9.9304+0.0000j - val_loss: 2.6152 - val_mean_squared_error: 2.6152+0.0000j - lr: 0.0010\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9239 - mean_squared_error: 9.9239+0.0000j\n",
            "Epoch 130: val_loss did not improve from 2.61525\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 9.9239 - mean_squared_error: 9.9239+0.0000j - val_loss: 2.6383 - val_mean_squared_error: 2.6383+0.0000j - lr: 0.0010\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9134 - mean_squared_error: 9.9134+0.0000j\n",
            "Epoch 131: val_loss did not improve from 2.61525\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 9.9134 - mean_squared_error: 9.9134+0.0000j - val_loss: 2.6724 - val_mean_squared_error: 2.6724+0.0000j - lr: 0.0010\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8996 - mean_squared_error: 9.8996+0.0000j\n",
            "Epoch 132: val_loss did not improve from 2.61525\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 9.8996 - mean_squared_error: 9.8996+0.0000j - val_loss: 2.7126 - val_mean_squared_error: 2.7126+0.0000j - lr: 0.0010\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8974 - mean_squared_error: 9.8974+0.0000j\n",
            "Epoch 133: val_loss did not improve from 2.61525\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 9.8974 - mean_squared_error: 9.8974+0.0000j - val_loss: 2.6311 - val_mean_squared_error: 2.6311+0.0000j - lr: 0.0010\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8845 - mean_squared_error: 9.8845+0.0000j\n",
            "Epoch 134: val_loss improved from 2.61525 to 2.57295, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 9.8845 - mean_squared_error: 9.8845+0.0000j - val_loss: 2.5729 - val_mean_squared_error: 2.5729+0.0000j - lr: 0.0010\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8833 - mean_squared_error: 9.8833+0.0000j\n",
            "Epoch 135: val_loss did not improve from 2.57295\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 9.8833 - mean_squared_error: 9.8833+0.0000j - val_loss: 2.5892 - val_mean_squared_error: 2.5892+0.0000j - lr: 0.0010\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8780 - mean_squared_error: 9.8780+0.0000j\n",
            "Epoch 136: val_loss did not improve from 2.57295\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 9.8780 - mean_squared_error: 9.8780+0.0000j - val_loss: 2.6242 - val_mean_squared_error: 2.6242+0.0000j - lr: 0.0010\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8688 - mean_squared_error: 9.8688+0.0000j\n",
            "Epoch 137: val_loss did not improve from 2.57295\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 9.8688 - mean_squared_error: 9.8688+0.0000j - val_loss: 2.6046 - val_mean_squared_error: 2.6046+0.0000j - lr: 0.0010\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8570 - mean_squared_error: 9.8570+0.0000j\n",
            "Epoch 138: val_loss improved from 2.57295 to 2.57272, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 9.8570 - mean_squared_error: 9.8570+0.0000j - val_loss: 2.5727 - val_mean_squared_error: 2.5727+0.0000j - lr: 0.0010\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8438 - mean_squared_error: 9.8438+0.0000j\n",
            "Epoch 139: val_loss did not improve from 2.57272\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 9.8438 - mean_squared_error: 9.8438+0.0000j - val_loss: 2.6239 - val_mean_squared_error: 2.6239+0.0000j - lr: 0.0010\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8359 - mean_squared_error: 9.8359+0.0000j\n",
            "Epoch 140: val_loss did not improve from 2.57272\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 9.8359 - mean_squared_error: 9.8359+0.0000j - val_loss: 2.5949 - val_mean_squared_error: 2.5949+0.0000j - lr: 0.0010\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8244 - mean_squared_error: 9.8244+0.0000j\n",
            "Epoch 141: val_loss improved from 2.57272 to 2.52260, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 9.8244 - mean_squared_error: 9.8244+0.0000j - val_loss: 2.5226 - val_mean_squared_error: 2.5226+0.0000j - lr: 0.0010\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8229 - mean_squared_error: 9.8229+0.0000j\n",
            "Epoch 142: val_loss did not improve from 2.52260\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 9.8229 - mean_squared_error: 9.8229+0.0000j - val_loss: 2.5532 - val_mean_squared_error: 2.5532+0.0000j - lr: 0.0010\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8159 - mean_squared_error: 9.8159+0.0000j\n",
            "Epoch 143: val_loss did not improve from 2.52260\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 9.8159 - mean_squared_error: 9.8159+0.0000j - val_loss: 2.6032 - val_mean_squared_error: 2.6032+0.0000j - lr: 0.0010\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8116 - mean_squared_error: 9.8116+0.0000j\n",
            "Epoch 144: val_loss did not improve from 2.52260\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 9.8116 - mean_squared_error: 9.8116+0.0000j - val_loss: 2.5850 - val_mean_squared_error: 2.5850+0.0000j - lr: 0.0010\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8030 - mean_squared_error: 9.8030+0.0000j\n",
            "Epoch 145: val_loss did not improve from 2.52260\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 9.8030 - mean_squared_error: 9.8030+0.0000j - val_loss: 2.5445 - val_mean_squared_error: 2.5445+0.0000j - lr: 0.0010\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7926 - mean_squared_error: 9.7926+0.0000j\n",
            "Epoch 146: val_loss did not improve from 2.52260\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 9.7926 - mean_squared_error: 9.7926+0.0000j - val_loss: 2.5525 - val_mean_squared_error: 2.5525+0.0000j - lr: 0.0010\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7819 - mean_squared_error: 9.7819+0.0000j\n",
            "Epoch 147: val_loss did not improve from 2.52260\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 9.7819 - mean_squared_error: 9.7819+0.0000j - val_loss: 2.5988 - val_mean_squared_error: 2.5988+0.0000j - lr: 0.0010\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7791 - mean_squared_error: 9.7791+0.0000j\n",
            "Epoch 148: val_loss did not improve from 2.52260\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 9.7791 - mean_squared_error: 9.7791+0.0000j - val_loss: 2.5542 - val_mean_squared_error: 2.5542+0.0000j - lr: 0.0010\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7605 - mean_squared_error: 9.7605+0.0000j\n",
            "Epoch 149: val_loss did not improve from 2.52260\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 9.7605 - mean_squared_error: 9.7605+0.0000j - val_loss: 2.5525 - val_mean_squared_error: 2.5525+0.0000j - lr: 0.0010\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7573 - mean_squared_error: 9.7573+0.0000j\n",
            "Epoch 150: val_loss improved from 2.52260 to 2.47510, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 9.7573 - mean_squared_error: 9.7573+0.0000j - val_loss: 2.4751 - val_mean_squared_error: 2.4751+0.0000j - lr: 0.0010\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7601 - mean_squared_error: 9.7601+0.0000j\n",
            "Epoch 151: val_loss improved from 2.47510 to 2.46766, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 9.7601 - mean_squared_error: 9.7601+0.0000j - val_loss: 2.4677 - val_mean_squared_error: 2.4677+0.0000j - lr: 0.0010\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7523 - mean_squared_error: 9.7523+0.0000j\n",
            "Epoch 152: val_loss did not improve from 2.46766\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 9.7523 - mean_squared_error: 9.7523+0.0000j - val_loss: 2.5082 - val_mean_squared_error: 2.5082+0.0000j - lr: 0.0010\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7451 - mean_squared_error: 9.7451+0.0000j\n",
            "Epoch 153: val_loss did not improve from 2.46766\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 9.7451 - mean_squared_error: 9.7451+0.0000j - val_loss: 2.4967 - val_mean_squared_error: 2.4967+0.0000j - lr: 0.0010\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7360 - mean_squared_error: 9.7360+0.0000j\n",
            "Epoch 154: val_loss did not improve from 2.46766\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 9.7360 - mean_squared_error: 9.7360+0.0000j - val_loss: 2.5017 - val_mean_squared_error: 2.5017+0.0000j - lr: 0.0010\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7235 - mean_squared_error: 9.7235+0.0000j\n",
            "Epoch 155: val_loss did not improve from 2.46766\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 9.7235 - mean_squared_error: 9.7235+0.0000j - val_loss: 2.4995 - val_mean_squared_error: 2.4995+0.0000j - lr: 0.0010\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7201 - mean_squared_error: 9.7201+0.0000j\n",
            "Epoch 156: val_loss did not improve from 2.46766\n",
            "1/1 [==============================] - 0s 444ms/step - loss: 9.7201 - mean_squared_error: 9.7201+0.0000j - val_loss: 2.5360 - val_mean_squared_error: 2.5360+0.0000j - lr: 0.0010\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7177 - mean_squared_error: 9.7177+0.0000j\n",
            "Epoch 157: val_loss did not improve from 2.46766\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 9.7177 - mean_squared_error: 9.7177+0.0000j - val_loss: 2.5278 - val_mean_squared_error: 2.5278+0.0000j - lr: 0.0010\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7075 - mean_squared_error: 9.7075+0.0000j\n",
            "Epoch 158: val_loss did not improve from 2.46766\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 9.7075 - mean_squared_error: 9.7075+0.0000j - val_loss: 2.5217 - val_mean_squared_error: 2.5217+0.0000j - lr: 0.0010\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6979 - mean_squared_error: 9.6979+0.0000j\n",
            "Epoch 159: val_loss did not improve from 2.46766\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 9.6979 - mean_squared_error: 9.6979+0.0000j - val_loss: 2.4725 - val_mean_squared_error: 2.4725+0.0000j - lr: 0.0010\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6949 - mean_squared_error: 9.6949+0.0000j\n",
            "Epoch 160: val_loss improved from 2.46766 to 2.40094, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 705ms/step - loss: 9.6949 - mean_squared_error: 9.6949+0.0000j - val_loss: 2.4009 - val_mean_squared_error: 2.4009+0.0000j - lr: 0.0010\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6950 - mean_squared_error: 9.6950+0.0000j\n",
            "Epoch 161: val_loss improved from 2.40094 to 2.38019, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 9.6950 - mean_squared_error: 9.6950+0.0000j - val_loss: 2.3802 - val_mean_squared_error: 2.3802+0.0000j - lr: 0.0010\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6884 - mean_squared_error: 9.6884+0.0000j\n",
            "Epoch 162: val_loss did not improve from 2.38019\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 9.6884 - mean_squared_error: 9.6884+0.0000j - val_loss: 2.4159 - val_mean_squared_error: 2.4159+0.0000j - lr: 0.0010\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6784 - mean_squared_error: 9.6784+0.0000j\n",
            "Epoch 163: val_loss did not improve from 2.38019\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 9.6784 - mean_squared_error: 9.6784+0.0000j - val_loss: 2.4657 - val_mean_squared_error: 2.4657+0.0000j - lr: 0.0010\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6698 - mean_squared_error: 9.6698+0.0000j\n",
            "Epoch 164: val_loss did not improve from 2.38019\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 9.6698 - mean_squared_error: 9.6698+0.0000j - val_loss: 2.4135 - val_mean_squared_error: 2.4135+0.0000j - lr: 0.0010\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6549 - mean_squared_error: 9.6549+0.0000j\n",
            "Epoch 165: val_loss did not improve from 2.38019\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 9.6549 - mean_squared_error: 9.6549+0.0000j - val_loss: 2.4067 - val_mean_squared_error: 2.4067+0.0000j - lr: 0.0010\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6551 - mean_squared_error: 9.6551+0.0000j\n",
            "Epoch 166: val_loss did not improve from 2.38019\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 9.6551 - mean_squared_error: 9.6551+0.0000j - val_loss: 2.4134 - val_mean_squared_error: 2.4134+0.0000j - lr: 0.0010\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6470 - mean_squared_error: 9.6470+0.0000j\n",
            "Epoch 167: val_loss did not improve from 2.38019\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 9.6470 - mean_squared_error: 9.6470+0.0000j - val_loss: 2.4502 - val_mean_squared_error: 2.4502+0.0000j - lr: 0.0010\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6450 - mean_squared_error: 9.6450+0.0000j\n",
            "Epoch 168: val_loss did not improve from 2.38019\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 9.6450 - mean_squared_error: 9.6450+0.0000j - val_loss: 2.3866 - val_mean_squared_error: 2.3866+0.0000j - lr: 0.0010\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6328 - mean_squared_error: 9.6328+0.0000j\n",
            "Epoch 169: val_loss improved from 2.38019 to 2.35482, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 9.6328 - mean_squared_error: 9.6328+0.0000j - val_loss: 2.3548 - val_mean_squared_error: 2.3548+0.0000j - lr: 0.0010\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6348 - mean_squared_error: 9.6348+0.0000j\n",
            "Epoch 170: val_loss improved from 2.35482 to 2.35189, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 985ms/step - loss: 9.6348 - mean_squared_error: 9.6348+0.0000j - val_loss: 2.3519 - val_mean_squared_error: 2.3519+0.0000j - lr: 0.0010\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6286 - mean_squared_error: 9.6286+0.0000j\n",
            "Epoch 171: val_loss did not improve from 2.35189\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 9.6286 - mean_squared_error: 9.6286+0.0000j - val_loss: 2.3960 - val_mean_squared_error: 2.3960+0.0000j - lr: 0.0010\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6172 - mean_squared_error: 9.6172+0.0000j\n",
            "Epoch 172: val_loss did not improve from 2.35189\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 9.6172 - mean_squared_error: 9.6172+0.0000j - val_loss: 2.3880 - val_mean_squared_error: 2.3880+0.0000j - lr: 0.0010\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6083 - mean_squared_error: 9.6083+0.0000j\n",
            "Epoch 173: val_loss did not improve from 2.35189\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 9.6083 - mean_squared_error: 9.6083+0.0000j - val_loss: 2.4015 - val_mean_squared_error: 2.4015+0.0000j - lr: 0.0010\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6039 - mean_squared_error: 9.6039+0.0000j\n",
            "Epoch 174: val_loss did not improve from 2.35189\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 9.6039 - mean_squared_error: 9.6039+0.0000j - val_loss: 2.3674 - val_mean_squared_error: 2.3674+0.0000j - lr: 0.0010\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5996 - mean_squared_error: 9.5996+0.0000j\n",
            "Epoch 175: val_loss did not improve from 2.35189\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 9.5996 - mean_squared_error: 9.5996+0.0000j - val_loss: 2.3557 - val_mean_squared_error: 2.3557+0.0000j - lr: 0.0010\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5875 - mean_squared_error: 9.5875+0.0000j\n",
            "Epoch 176: val_loss did not improve from 2.35189\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 9.5875 - mean_squared_error: 9.5875+0.0000j - val_loss: 2.4307 - val_mean_squared_error: 2.4307+0.0000j - lr: 0.0010\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5949 - mean_squared_error: 9.5949+0.0000j\n",
            "Epoch 177: val_loss did not improve from 2.35189\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 9.5949 - mean_squared_error: 9.5949+0.0000j - val_loss: 2.3524 - val_mean_squared_error: 2.3524+0.0000j - lr: 0.0010\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5728 - mean_squared_error: 9.5728+0.0000j\n",
            "Epoch 178: val_loss improved from 2.35189 to 2.28647, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 9.5728 - mean_squared_error: 9.5728+0.0000j - val_loss: 2.2865 - val_mean_squared_error: 2.2865+0.0000j - lr: 0.0010\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5711 - mean_squared_error: 9.5711+0.0000j\n",
            "Epoch 179: val_loss improved from 2.28647 to 2.26908, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 9.5711 - mean_squared_error: 9.5711+0.0000j - val_loss: 2.2691 - val_mean_squared_error: 2.2691+0.0000j - lr: 0.0010\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5724 - mean_squared_error: 9.5724+0.0000j\n",
            "Epoch 180: val_loss improved from 2.26908 to 2.25494, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 9.5724 - mean_squared_error: 9.5724+0.0000j - val_loss: 2.2549 - val_mean_squared_error: 2.2549+0.0000j - lr: 0.0010\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5617 - mean_squared_error: 9.5617+0.0000j\n",
            "Epoch 181: val_loss did not improve from 2.25494\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 9.5617 - mean_squared_error: 9.5617+0.0000j - val_loss: 2.2903 - val_mean_squared_error: 2.2903+0.0000j - lr: 0.0010\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5592 - mean_squared_error: 9.5592+0.0000j\n",
            "Epoch 182: val_loss did not improve from 2.25494\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 9.5592 - mean_squared_error: 9.5592+0.0000j - val_loss: 2.2867 - val_mean_squared_error: 2.2867+0.0000j - lr: 0.0010\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5506 - mean_squared_error: 9.5506+0.0000j\n",
            "Epoch 183: val_loss did not improve from 2.25494\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 9.5506 - mean_squared_error: 9.5506+0.0000j - val_loss: 2.2869 - val_mean_squared_error: 2.2869+0.0000j - lr: 0.0010\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5428 - mean_squared_error: 9.5428+0.0000j\n",
            "Epoch 184: val_loss did not improve from 2.25494\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 9.5428 - mean_squared_error: 9.5428+0.0000j - val_loss: 2.3514 - val_mean_squared_error: 2.3514+0.0000j - lr: 0.0010\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5628 - mean_squared_error: 9.5628+0.0000j\n",
            "Epoch 185: val_loss did not improve from 2.25494\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 9.5628 - mean_squared_error: 9.5628+0.0000j - val_loss: 2.3181 - val_mean_squared_error: 2.3181+0.0000j - lr: 0.0010\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5500 - mean_squared_error: 9.5500+0.0000j\n",
            "Epoch 186: val_loss improved from 2.25494 to 2.20623, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 9.5500 - mean_squared_error: 9.5500+0.0000j - val_loss: 2.2062 - val_mean_squared_error: 2.2062+0.0000j - lr: 0.0010\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5299 - mean_squared_error: 9.5299+0.0000j\n",
            "Epoch 187: val_loss did not improve from 2.20623\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 9.5299 - mean_squared_error: 9.5299+0.0000j - val_loss: 2.2202 - val_mean_squared_error: 2.2202+0.0000j - lr: 0.0010\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5266 - mean_squared_error: 9.5266+0.0000j\n",
            "Epoch 188: val_loss did not improve from 2.20623\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 9.5266 - mean_squared_error: 9.5266+0.0000j - val_loss: 2.2829 - val_mean_squared_error: 2.2829+0.0000j - lr: 0.0010\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5354 - mean_squared_error: 9.5354+0.0000j\n",
            "Epoch 189: val_loss improved from 2.20623 to 2.20399, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 9.5354 - mean_squared_error: 9.5354+0.0000j - val_loss: 2.2040 - val_mean_squared_error: 2.2040+0.0000j - lr: 0.0010\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5126 - mean_squared_error: 9.5126+0.0000j\n",
            "Epoch 190: val_loss improved from 2.20399 to 2.19478, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 9.5126 - mean_squared_error: 9.5126+0.0000j - val_loss: 2.1948 - val_mean_squared_error: 2.1948+0.0000j - lr: 0.0010\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5042 - mean_squared_error: 9.5042+0.0000j\n",
            "Epoch 191: val_loss did not improve from 2.19478\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 9.5042 - mean_squared_error: 9.5042+0.0000j - val_loss: 2.3000 - val_mean_squared_error: 2.3000+0.0000j - lr: 0.0010\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5112 - mean_squared_error: 9.5112+0.0000j\n",
            "Epoch 192: val_loss did not improve from 2.19478\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 9.5112 - mean_squared_error: 9.5112+0.0000j - val_loss: 2.3121 - val_mean_squared_error: 2.3121+0.0000j - lr: 0.0010\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5203 - mean_squared_error: 9.5203+0.0000j\n",
            "Epoch 193: val_loss did not improve from 2.19478\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 9.5203 - mean_squared_error: 9.5203+0.0000j - val_loss: 2.2102 - val_mean_squared_error: 2.2102+0.0000j - lr: 0.0010\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4936 - mean_squared_error: 9.4936+0.0000j\n",
            "Epoch 194: val_loss improved from 2.19478 to 2.14089, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 9.4936 - mean_squared_error: 9.4936+0.0000j - val_loss: 2.1409 - val_mean_squared_error: 2.1409+0.0000j - lr: 0.0010\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5093 - mean_squared_error: 9.5093+0.0000j\n",
            "Epoch 195: val_loss did not improve from 2.14089\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 9.5093 - mean_squared_error: 9.5093+0.0000j - val_loss: 2.1657 - val_mean_squared_error: 2.1657+0.0000j - lr: 0.0010\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4999 - mean_squared_error: 9.4999+0.0000j\n",
            "Epoch 196: val_loss did not improve from 2.14089\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 9.4999 - mean_squared_error: 9.4999+0.0000j - val_loss: 2.2480 - val_mean_squared_error: 2.2480+0.0000j - lr: 0.0010\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5061 - mean_squared_error: 9.5061+0.0000j\n",
            "Epoch 197: val_loss did not improve from 2.14089\n",
            "1/1 [==============================] - 0s 449ms/step - loss: 9.5061 - mean_squared_error: 9.5061+0.0000j - val_loss: 2.2182 - val_mean_squared_error: 2.2182+0.0000j - lr: 0.0010\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4823 - mean_squared_error: 9.4823+0.0000j\n",
            "Epoch 198: val_loss did not improve from 2.14089\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 9.4823 - mean_squared_error: 9.4823+0.0000j - val_loss: 2.1994 - val_mean_squared_error: 2.1994+0.0000j - lr: 0.0010\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4945 - mean_squared_error: 9.4945+0.0000j\n",
            "Epoch 199: val_loss did not improve from 2.14089\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 9.4945 - mean_squared_error: 9.4945+0.0000j - val_loss: 2.2621 - val_mean_squared_error: 2.2621+0.0000j - lr: 0.0010\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4837 - mean_squared_error: 9.4837+0.0000j\n",
            "Epoch 200: val_loss did not improve from 2.14089\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 9.4837 - mean_squared_error: 9.4837+0.0000j - val_loss: 2.3149 - val_mean_squared_error: 2.3149+0.0000j - lr: 0.0010\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4780 - mean_squared_error: 9.4780+0.0000j\n",
            "Epoch 201: val_loss did not improve from 2.14089\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 9.4780 - mean_squared_error: 9.4780+0.0000j - val_loss: 2.2382 - val_mean_squared_error: 2.2382+0.0000j - lr: 0.0010\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4610 - mean_squared_error: 9.4610+0.0000j\n",
            "Epoch 202: val_loss did not improve from 2.14089\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 9.4610 - mean_squared_error: 9.4610+0.0000j - val_loss: 2.1853 - val_mean_squared_error: 2.1853+0.0000j - lr: 0.0010\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4614 - mean_squared_error: 9.4614+0.0000j\n",
            "Epoch 203: val_loss did not improve from 2.14089\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 9.4614 - mean_squared_error: 9.4614+0.0000j - val_loss: 2.1706 - val_mean_squared_error: 2.1706+0.0000j - lr: 0.0010\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4553 - mean_squared_error: 9.4553+0.0000j\n",
            "Epoch 204: val_loss did not improve from 2.14089\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 9.4553 - mean_squared_error: 9.4553+0.0000j - val_loss: 2.1995 - val_mean_squared_error: 2.1995+0.0000j - lr: 0.0010\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4493 - mean_squared_error: 9.4493+0.0000j\n",
            "Epoch 205: val_loss did not improve from 2.14089\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 9.4493 - mean_squared_error: 9.4493+0.0000j - val_loss: 2.1924 - val_mean_squared_error: 2.1924+0.0000j - lr: 0.0010\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4445 - mean_squared_error: 9.4445+0.0000j\n",
            "Epoch 206: val_loss improved from 2.14089 to 2.11990, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 9.4445 - mean_squared_error: 9.4445+0.0000j - val_loss: 2.1199 - val_mean_squared_error: 2.1199+0.0000j - lr: 0.0010\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4481 - mean_squared_error: 9.4481+0.0000j\n",
            "Epoch 207: val_loss did not improve from 2.11990\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 9.4481 - mean_squared_error: 9.4481+0.0000j - val_loss: 2.1488 - val_mean_squared_error: 2.1488+0.0000j - lr: 0.0010\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4410 - mean_squared_error: 9.4410+0.0000j\n",
            "Epoch 208: val_loss did not improve from 2.11990\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 9.4410 - mean_squared_error: 9.4410+0.0000j - val_loss: 2.2126 - val_mean_squared_error: 2.2126+0.0000j - lr: 0.0010\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4379 - mean_squared_error: 9.4379+0.0000j\n",
            "Epoch 209: val_loss did not improve from 2.11990\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 9.4379 - mean_squared_error: 9.4379+0.0000j - val_loss: 2.2260 - val_mean_squared_error: 2.2260+0.0000j - lr: 0.0010\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4335 - mean_squared_error: 9.4335+0.0000j\n",
            "Epoch 210: val_loss did not improve from 2.11990\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 9.4335 - mean_squared_error: 9.4335+0.0000j - val_loss: 2.2000 - val_mean_squared_error: 2.2000+0.0000j - lr: 0.0010\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4271 - mean_squared_error: 9.4271+0.0000j\n",
            "Epoch 211: val_loss did not improve from 2.11990\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 9.4271 - mean_squared_error: 9.4271+0.0000j - val_loss: 2.1858 - val_mean_squared_error: 2.1858+0.0000j - lr: 0.0010\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4187 - mean_squared_error: 9.4187+0.0000j\n",
            "Epoch 212: val_loss did not improve from 2.11990\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 9.4187 - mean_squared_error: 9.4187+0.0000j - val_loss: 2.2504 - val_mean_squared_error: 2.2504+0.0000j - lr: 0.0010\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4345 - mean_squared_error: 9.4345+0.0000j\n",
            "Epoch 213: val_loss did not improve from 2.11990\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 9.4345 - mean_squared_error: 9.4345+0.0000j - val_loss: 2.2241 - val_mean_squared_error: 2.2241+0.0000j - lr: 0.0010\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4265 - mean_squared_error: 9.4265+0.0000j\n",
            "Epoch 214: val_loss improved from 2.11990 to 2.11000, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 9.4265 - mean_squared_error: 9.4265+0.0000j - val_loss: 2.1100 - val_mean_squared_error: 2.1100+0.0000j - lr: 0.0010\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4173 - mean_squared_error: 9.4173+0.0000j\n",
            "Epoch 215: val_loss improved from 2.11000 to 2.09298, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 9.4173 - mean_squared_error: 9.4173+0.0000j - val_loss: 2.0930 - val_mean_squared_error: 2.0930+0.0000j - lr: 0.0010\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4190 - mean_squared_error: 9.4189+0.0000j\n",
            "Epoch 216: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 9.4190 - mean_squared_error: 9.4189+0.0000j - val_loss: 2.1711 - val_mean_squared_error: 2.1711+0.0000j - lr: 0.0010\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4179 - mean_squared_error: 9.4179+0.0000j\n",
            "Epoch 217: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 9.4179 - mean_squared_error: 9.4179+0.0000j - val_loss: 2.2045 - val_mean_squared_error: 2.2045+0.0000j - lr: 0.0010\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4083 - mean_squared_error: 9.4083+0.0000j\n",
            "Epoch 218: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 9.4083 - mean_squared_error: 9.4083+0.0000j - val_loss: 2.1420 - val_mean_squared_error: 2.1420+0.0000j - lr: 0.0010\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4068 - mean_squared_error: 9.4068+0.0000j\n",
            "Epoch 219: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 9.4068 - mean_squared_error: 9.4068+0.0000j - val_loss: 2.1838 - val_mean_squared_error: 2.1838+0.0000j - lr: 0.0010\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4016 - mean_squared_error: 9.4016+0.0000j\n",
            "Epoch 220: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 9.4016 - mean_squared_error: 9.4016+0.0000j - val_loss: 2.2145 - val_mean_squared_error: 2.2145+0.0000j - lr: 0.0010\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3960 - mean_squared_error: 9.3960+0.0000j\n",
            "Epoch 221: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 9.3960 - mean_squared_error: 9.3960+0.0000j - val_loss: 2.2278 - val_mean_squared_error: 2.2278+0.0000j - lr: 0.0010\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4051 - mean_squared_error: 9.4051+0.0000j\n",
            "Epoch 222: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 9.4051 - mean_squared_error: 9.4051+0.0000j - val_loss: 2.1068 - val_mean_squared_error: 2.1068+0.0000j - lr: 0.0010\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3943 - mean_squared_error: 9.3943+0.0000j\n",
            "Epoch 223: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 9.3943 - mean_squared_error: 9.3943+0.0000j - val_loss: 2.1293 - val_mean_squared_error: 2.1293+0.0000j - lr: 0.0010\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3963 - mean_squared_error: 9.3963+0.0000j\n",
            "Epoch 224: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 9.3963 - mean_squared_error: 9.3963+0.0000j - val_loss: 2.1636 - val_mean_squared_error: 2.1636+0.0000j - lr: 0.0010\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3872 - mean_squared_error: 9.3872+0.0000j\n",
            "Epoch 225: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 9.3872 - mean_squared_error: 9.3872+0.0000j - val_loss: 2.2619 - val_mean_squared_error: 2.2619+0.0000j - lr: 0.0010\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3805 - mean_squared_error: 9.3805+0.0000j\n",
            "Epoch 226: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 9.3805 - mean_squared_error: 9.3805+0.0000j - val_loss: 2.2180 - val_mean_squared_error: 2.2180+0.0000j - lr: 0.0010\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3912 - mean_squared_error: 9.3912+0.0000j\n",
            "Epoch 227: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 9.3912 - mean_squared_error: 9.3912+0.0000j - val_loss: 2.1657 - val_mean_squared_error: 2.1657+0.0000j - lr: 0.0010\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3907 - mean_squared_error: 9.3907+0.0000j\n",
            "Epoch 228: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 9.3907 - mean_squared_error: 9.3907+0.0000j - val_loss: 2.1175 - val_mean_squared_error: 2.1175+0.0000j - lr: 0.0010\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3705 - mean_squared_error: 9.3705+0.0000j\n",
            "Epoch 229: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9.3705 - mean_squared_error: 9.3705+0.0000j - val_loss: 2.2037 - val_mean_squared_error: 2.2037+0.0000j - lr: 0.0010\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3813 - mean_squared_error: 9.3813+0.0000j\n",
            "Epoch 230: val_loss did not improve from 2.09298\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 9.3813 - mean_squared_error: 9.3813+0.0000j - val_loss: 2.1804 - val_mean_squared_error: 2.1804+0.0000j - lr: 0.0010\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3661 - mean_squared_error: 9.3661+0.0000j\n",
            "Epoch 231: val_loss improved from 2.09298 to 2.07254, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 706ms/step - loss: 9.3661 - mean_squared_error: 9.3661+0.0000j - val_loss: 2.0725 - val_mean_squared_error: 2.0725+0.0000j - lr: 0.0010\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3832 - mean_squared_error: 9.3832+0.0000j\n",
            "Epoch 232: val_loss did not improve from 2.07254\n",
            "1/1 [==============================] - 0s 446ms/step - loss: 9.3832 - mean_squared_error: 9.3832+0.0000j - val_loss: 2.1890 - val_mean_squared_error: 2.1890+0.0000j - lr: 0.0010\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3608 - mean_squared_error: 9.3608+0.0000j\n",
            "Epoch 233: val_loss did not improve from 2.07254\n",
            "1/1 [==============================] - 0s 433ms/step - loss: 9.3608 - mean_squared_error: 9.3608+0.0000j - val_loss: 2.2379 - val_mean_squared_error: 2.2379+0.0000j - lr: 0.0010\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3579 - mean_squared_error: 9.3579+0.0000j\n",
            "Epoch 234: val_loss did not improve from 2.07254\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 9.3579 - mean_squared_error: 9.3579+0.0000j - val_loss: 2.1296 - val_mean_squared_error: 2.1296+0.0000j - lr: 0.0010\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3503 - mean_squared_error: 9.3503+0.0000j\n",
            "Epoch 235: val_loss did not improve from 2.07254\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 9.3503 - mean_squared_error: 9.3503+0.0000j - val_loss: 2.2736 - val_mean_squared_error: 2.2736+0.0000j - lr: 0.0010\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3712 - mean_squared_error: 9.3712+0.0000j\n",
            "Epoch 236: val_loss did not improve from 2.07254\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 9.3712 - mean_squared_error: 9.3712+0.0000j - val_loss: 2.1245 - val_mean_squared_error: 2.1245+0.0000j - lr: 0.0010\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3582 - mean_squared_error: 9.3582+0.0000j\n",
            "Epoch 237: val_loss did not improve from 2.07254\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 9.3582 - mean_squared_error: 9.3582+0.0000j - val_loss: 2.2652 - val_mean_squared_error: 2.2652+0.0000j - lr: 0.0010\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3539 - mean_squared_error: 9.3539+0.0000j\n",
            "Epoch 238: val_loss did not improve from 2.07254\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 9.3539 - mean_squared_error: 9.3539+0.0000j - val_loss: 2.1631 - val_mean_squared_error: 2.1631+0.0000j - lr: 0.0010\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3707 - mean_squared_error: 9.3707+0.0000j\n",
            "Epoch 239: val_loss did not improve from 2.07254\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 9.3707 - mean_squared_error: 9.3707+0.0000j - val_loss: 2.1939 - val_mean_squared_error: 2.1939+0.0000j - lr: 0.0010\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3659 - mean_squared_error: 9.3659+0.0000j\n",
            "Epoch 240: val_loss improved from 2.07254 to 2.03911, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 9.3659 - mean_squared_error: 9.3659+0.0000j - val_loss: 2.0391 - val_mean_squared_error: 2.0391+0.0000j - lr: 0.0010\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3628 - mean_squared_error: 9.3628+0.0000j\n",
            "Epoch 241: val_loss did not improve from 2.03911\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 9.3628 - mean_squared_error: 9.3628+0.0000j - val_loss: 2.3256 - val_mean_squared_error: 2.3256+0.0000j - lr: 0.0010\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3707 - mean_squared_error: 9.3707+0.0000j\n",
            "Epoch 242: val_loss did not improve from 2.03911\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 9.3707 - mean_squared_error: 9.3707+0.0000j - val_loss: 2.0788 - val_mean_squared_error: 2.0788+0.0000j - lr: 0.0010\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3946 - mean_squared_error: 9.3946+0.0000j\n",
            "Epoch 243: val_loss did not improve from 2.03911\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 9.3946 - mean_squared_error: 9.3946+0.0000j - val_loss: 2.3613 - val_mean_squared_error: 2.3613+0.0000j - lr: 0.0010\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4361 - mean_squared_error: 9.4361+0.0000j\n",
            "Epoch 244: val_loss improved from 2.03911 to 1.98076, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 9.4361 - mean_squared_error: 9.4361+0.0000j - val_loss: 1.9808 - val_mean_squared_error: 1.9808+0.0000j - lr: 0.0010\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4799 - mean_squared_error: 9.4799+0.0000j\n",
            "Epoch 245: val_loss did not improve from 1.98076\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 9.4799 - mean_squared_error: 9.4799+0.0000j - val_loss: 2.7518 - val_mean_squared_error: 2.7518+0.0000j - lr: 0.0010\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5543 - mean_squared_error: 9.5543+0.0000j\n",
            "Epoch 246: val_loss did not improve from 1.98076\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 9.5543 - mean_squared_error: 9.5543+0.0000j - val_loss: 2.1012 - val_mean_squared_error: 2.1012+0.0000j - lr: 0.0010\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6501 - mean_squared_error: 9.6501+0.0000j\n",
            "Epoch 247: val_loss did not improve from 1.98076\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 9.6501 - mean_squared_error: 9.6501+0.0000j - val_loss: 3.0075 - val_mean_squared_error: 3.0075+0.0000j - lr: 0.0010\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7640 - mean_squared_error: 9.7640+0.0000j\n",
            "Epoch 248: val_loss did not improve from 1.98076\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 9.7640 - mean_squared_error: 9.7640+0.0000j - val_loss: 2.1661 - val_mean_squared_error: 2.1661+0.0000j - lr: 0.0010\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.8240 - mean_squared_error: 9.8240+0.0000j\n",
            "Epoch 249: val_loss did not improve from 1.98076\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 9.8240 - mean_squared_error: 9.8240+0.0000j - val_loss: 3.1744 - val_mean_squared_error: 3.1744+0.0000j - lr: 0.0010\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7995 - mean_squared_error: 9.7995+0.0000j\n",
            "Epoch 250: val_loss did not improve from 1.98076\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 9.7995 - mean_squared_error: 9.7995+0.0000j - val_loss: 2.0462 - val_mean_squared_error: 2.0462+0.0000j - lr: 0.0010\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6460 - mean_squared_error: 9.6460+0.0000j\n",
            "Epoch 251: val_loss did not improve from 1.98076\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 9.6460 - mean_squared_error: 9.6460+0.0000j - val_loss: 2.5310 - val_mean_squared_error: 2.5310+0.0000j - lr: 0.0010\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4675 - mean_squared_error: 9.4675+0.0000j\n",
            "Epoch 252: val_loss did not improve from 1.98076\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 9.4675 - mean_squared_error: 9.4675+0.0000j - val_loss: 2.1465 - val_mean_squared_error: 2.1465+0.0000j - lr: 0.0010\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3448 - mean_squared_error: 9.3448+0.0000j\n",
            "Epoch 253: val_loss did not improve from 1.98076\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 9.3448 - mean_squared_error: 9.3448+0.0000j - val_loss: 2.0992 - val_mean_squared_error: 2.0992+0.0000j - lr: 0.0010\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3344 - mean_squared_error: 9.3344+0.0000j\n",
            "Epoch 254: val_loss did not improve from 1.98076\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 9.3344 - mean_squared_error: 9.3344+0.0000j - val_loss: 2.3838 - val_mean_squared_error: 2.3838+0.0000j - lr: 0.0010\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4182 - mean_squared_error: 9.4182+0.0000j\n",
            "Epoch 255: val_loss improved from 1.98076 to 1.94870, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 9.4182 - mean_squared_error: 9.4182+0.0000j - val_loss: 1.9487 - val_mean_squared_error: 1.9487+0.0000j - lr: 0.0010\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4908 - mean_squared_error: 9.4908+0.0000j\n",
            "Epoch 256: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 9.4908 - mean_squared_error: 9.4908+0.0000j - val_loss: 2.6198 - val_mean_squared_error: 2.6198+0.0000j - lr: 0.0010\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5048 - mean_squared_error: 9.5048+0.0000j\n",
            "Epoch 257: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 9.5048 - mean_squared_error: 9.5048+0.0000j - val_loss: 2.0000 - val_mean_squared_error: 2.0000+0.0000j - lr: 0.0010\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4487 - mean_squared_error: 9.4487+0.0000j\n",
            "Epoch 258: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 9.4487 - mean_squared_error: 9.4487+0.0000j - val_loss: 2.3734 - val_mean_squared_error: 2.3734+0.0000j - lr: 0.0010\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3708 - mean_squared_error: 9.3708+0.0000j\n",
            "Epoch 259: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 9.3708 - mean_squared_error: 9.3708+0.0000j - val_loss: 2.1864 - val_mean_squared_error: 2.1864+0.0000j - lr: 0.0010\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3236 - mean_squared_error: 9.3236+0.0000j\n",
            "Epoch 260: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 9.3236 - mean_squared_error: 9.3236+0.0000j - val_loss: 2.1450 - val_mean_squared_error: 2.1450+0.0000j - lr: 0.0010\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3163 - mean_squared_error: 9.3163+0.0000j\n",
            "Epoch 261: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 9.3163 - mean_squared_error: 9.3163+0.0000j - val_loss: 2.3104 - val_mean_squared_error: 2.3104+0.0000j - lr: 0.0010\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3415 - mean_squared_error: 9.3415+0.0000j\n",
            "Epoch 262: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 9.3415 - mean_squared_error: 9.3415+0.0000j - val_loss: 2.0232 - val_mean_squared_error: 2.0232+0.0000j - lr: 0.0010\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3603 - mean_squared_error: 9.3603+0.0000j\n",
            "Epoch 263: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 9.3603 - mean_squared_error: 9.3603+0.0000j - val_loss: 2.3414 - val_mean_squared_error: 2.3414+0.0000j - lr: 0.0010\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3800 - mean_squared_error: 9.3800+0.0000j\n",
            "Epoch 264: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 9.3800 - mean_squared_error: 9.3800+0.0000j - val_loss: 1.9931 - val_mean_squared_error: 1.9931+0.0000j - lr: 0.0010\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3673 - mean_squared_error: 9.3673+0.0000j\n",
            "Epoch 265: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 9.3673 - mean_squared_error: 9.3673+0.0000j - val_loss: 2.3711 - val_mean_squared_error: 2.3711+0.0000j - lr: 0.0010\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3471 - mean_squared_error: 9.3471+0.0000j\n",
            "Epoch 266: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 9.3471 - mean_squared_error: 9.3471+0.0000j - val_loss: 2.1223 - val_mean_squared_error: 2.1223+0.0000j - lr: 0.0010\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3222 - mean_squared_error: 9.3222+0.0000j\n",
            "Epoch 267: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 9.3222 - mean_squared_error: 9.3222+0.0000j - val_loss: 2.2250 - val_mean_squared_error: 2.2250+0.0000j - lr: 0.0010\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3126 - mean_squared_error: 9.3126+0.0000j\n",
            "Epoch 268: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 406ms/step - loss: 9.3126 - mean_squared_error: 9.3126+0.0000j - val_loss: 2.1899 - val_mean_squared_error: 2.1899+0.0000j - lr: 0.0010\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2986 - mean_squared_error: 9.2986+0.0000j\n",
            "Epoch 269: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 9.2986 - mean_squared_error: 9.2986+0.0000j - val_loss: 2.0867 - val_mean_squared_error: 2.0867+0.0000j - lr: 0.0010\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3047 - mean_squared_error: 9.3047+0.0000j\n",
            "Epoch 270: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 9.3047 - mean_squared_error: 9.3047+0.0000j - val_loss: 2.1718 - val_mean_squared_error: 2.1718+0.0000j - lr: 0.0010\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3163 - mean_squared_error: 9.3163+0.0000j\n",
            "Epoch 271: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 9.3163 - mean_squared_error: 9.3163+0.0000j - val_loss: 2.0500 - val_mean_squared_error: 2.0500+0.0000j - lr: 0.0010\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3216 - mean_squared_error: 9.3216+0.0000j\n",
            "Epoch 272: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 437ms/step - loss: 9.3216 - mean_squared_error: 9.3216+0.0000j - val_loss: 2.3202 - val_mean_squared_error: 2.3202+0.0000j - lr: 0.0010\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3184 - mean_squared_error: 9.3184+0.0000j\n",
            "Epoch 273: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 9.3184 - mean_squared_error: 9.3184+0.0000j - val_loss: 2.1429 - val_mean_squared_error: 2.1429+0.0000j - lr: 0.0010\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3417 - mean_squared_error: 9.3417+0.0000j\n",
            "Epoch 274: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 9.3417 - mean_squared_error: 9.3417+0.0000j - val_loss: 2.3359 - val_mean_squared_error: 2.3359+0.0000j - lr: 0.0010\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3363 - mean_squared_error: 9.3363+0.0000j\n",
            "Epoch 275: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 383ms/step - loss: 9.3363 - mean_squared_error: 9.3363+0.0000j - val_loss: 2.0459 - val_mean_squared_error: 2.0459+0.0000j - lr: 0.0010\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3419 - mean_squared_error: 9.3419+0.0000j\n",
            "Epoch 276: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 9.3419 - mean_squared_error: 9.3419+0.0000j - val_loss: 2.3472 - val_mean_squared_error: 2.3472+0.0000j - lr: 0.0010\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3363 - mean_squared_error: 9.3363+0.0000j\n",
            "Epoch 277: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 9.3363 - mean_squared_error: 9.3363+0.0000j - val_loss: 2.0440 - val_mean_squared_error: 2.0440+0.0000j - lr: 0.0010\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3364 - mean_squared_error: 9.3364+0.0000j\n",
            "Epoch 278: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 9.3364 - mean_squared_error: 9.3364+0.0000j - val_loss: 2.3068 - val_mean_squared_error: 2.3068+0.0000j - lr: 0.0010\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3431 - mean_squared_error: 9.3431+0.0000j\n",
            "Epoch 279: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 9.3431 - mean_squared_error: 9.3431+0.0000j - val_loss: 2.0697 - val_mean_squared_error: 2.0697+0.0000j - lr: 0.0010\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3273 - mean_squared_error: 9.3273+0.0000j\n",
            "Epoch 280: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 9.3273 - mean_squared_error: 9.3273+0.0000j - val_loss: 2.3726 - val_mean_squared_error: 2.3726+0.0000j - lr: 0.0010\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3243 - mean_squared_error: 9.3243+0.0000j\n",
            "Epoch 281: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 9.3243 - mean_squared_error: 9.3243+0.0000j - val_loss: 2.0613 - val_mean_squared_error: 2.0613+0.0000j - lr: 0.0010\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3140 - mean_squared_error: 9.3140+0.0000j\n",
            "Epoch 282: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 9.3140 - mean_squared_error: 9.3140+0.0000j - val_loss: 2.3011 - val_mean_squared_error: 2.3011+0.0000j - lr: 0.0010\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3101 - mean_squared_error: 9.3101+0.0000j\n",
            "Epoch 283: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 9.3101 - mean_squared_error: 9.3101+0.0000j - val_loss: 2.1166 - val_mean_squared_error: 2.1166+0.0000j - lr: 0.0010\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2909 - mean_squared_error: 9.2909+0.0000j\n",
            "Epoch 284: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 9.2909 - mean_squared_error: 9.2909+0.0000j - val_loss: 2.1985 - val_mean_squared_error: 2.1985+0.0000j - lr: 0.0010\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2917 - mean_squared_error: 9.2917+0.0000j\n",
            "Epoch 285: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 9.2917 - mean_squared_error: 9.2917+0.0000j - val_loss: 2.1173 - val_mean_squared_error: 2.1173+0.0000j - lr: 0.0010\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2917 - mean_squared_error: 9.2917+0.0000j\n",
            "Epoch 286: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 9.2917 - mean_squared_error: 9.2917+0.0000j - val_loss: 2.1185 - val_mean_squared_error: 2.1185+0.0000j - lr: 0.0010\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2854 - mean_squared_error: 9.2854+0.0000j\n",
            "Epoch 287: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 9.2854 - mean_squared_error: 9.2854+0.0000j - val_loss: 2.1589 - val_mean_squared_error: 2.1589+0.0000j - lr: 0.0010\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2818 - mean_squared_error: 9.2818+0.0000j\n",
            "Epoch 288: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 9.2818 - mean_squared_error: 9.2818+0.0000j - val_loss: 2.1522 - val_mean_squared_error: 2.1522+0.0000j - lr: 0.0010\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2797 - mean_squared_error: 9.2797+0.0000j\n",
            "Epoch 289: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 9.2797 - mean_squared_error: 9.2797+0.0000j - val_loss: 2.2875 - val_mean_squared_error: 2.2875+0.0000j - lr: 0.0010\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2917 - mean_squared_error: 9.2917+0.0000j\n",
            "Epoch 290: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 9.2917 - mean_squared_error: 9.2917+0.0000j - val_loss: 2.0947 - val_mean_squared_error: 2.0947+0.0000j - lr: 0.0010\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2886 - mean_squared_error: 9.2886+0.0000j\n",
            "Epoch 291: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 9.2886 - mean_squared_error: 9.2886+0.0000j - val_loss: 2.2371 - val_mean_squared_error: 2.2371+0.0000j - lr: 0.0010\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2882 - mean_squared_error: 9.2882+0.0000j\n",
            "Epoch 292: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 9.2882 - mean_squared_error: 9.2882+0.0000j - val_loss: 2.1036 - val_mean_squared_error: 2.1036+0.0000j - lr: 0.0010\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3062 - mean_squared_error: 9.3062+0.0000j\n",
            "Epoch 293: val_loss did not improve from 1.94870\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 9.3062 - mean_squared_error: 9.3062+0.0000j - val_loss: 2.2965 - val_mean_squared_error: 2.2965+0.0000j - lr: 0.0010\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3160 - mean_squared_error: 9.3160+0.0000j\n",
            "Epoch 294: val_loss improved from 1.94870 to 1.94854, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 9.3160 - mean_squared_error: 9.3160+0.0000j - val_loss: 1.9485 - val_mean_squared_error: 1.9485+0.0000j - lr: 0.0010\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3473 - mean_squared_error: 9.3473+0.0000j\n",
            "Epoch 295: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 9.3473 - mean_squared_error: 9.3473+0.0000j - val_loss: 2.4868 - val_mean_squared_error: 2.4868+0.0000j - lr: 0.0010\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3787 - mean_squared_error: 9.3787+0.0000j\n",
            "Epoch 296: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 9.3787 - mean_squared_error: 9.3787+0.0000j - val_loss: 2.0713 - val_mean_squared_error: 2.0713+0.0000j - lr: 0.0010\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4495 - mean_squared_error: 9.4495+0.0000j\n",
            "Epoch 297: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 9.4495 - mean_squared_error: 9.4495+0.0000j - val_loss: 2.8441 - val_mean_squared_error: 2.8441+0.0000j - lr: 0.0010\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5821 - mean_squared_error: 9.5821+0.0000j\n",
            "Epoch 298: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 9.5821 - mean_squared_error: 9.5821+0.0000j - val_loss: 2.1168 - val_mean_squared_error: 2.1168+0.0000j - lr: 0.0010\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7672 - mean_squared_error: 9.7672+0.0000j\n",
            "Epoch 299: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 9.7672 - mean_squared_error: 9.7672+0.0000j - val_loss: 3.5686 - val_mean_squared_error: 3.5686+0.0000j - lr: 0.0010\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0186 - mean_squared_error: 10.0186+0.0000j\n",
            "Epoch 300: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 10.0186 - mean_squared_error: 10.0186+0.0000j - val_loss: 2.3658 - val_mean_squared_error: 2.3658+0.0000j - lr: 0.0010\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.2405 - mean_squared_error: 10.2406+0.0000j\n",
            "Epoch 301: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 10.2405 - mean_squared_error: 10.2406+0.0000j - val_loss: 3.9137 - val_mean_squared_error: 3.9137+0.0000j - lr: 0.0010\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.3216 - mean_squared_error: 10.3216+0.0000j\n",
            "Epoch 302: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 10.3216 - mean_squared_error: 10.3216+0.0000j - val_loss: 2.3073 - val_mean_squared_error: 2.3073+0.0000j - lr: 0.0010\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0940 - mean_squared_error: 10.0940+0.0000j\n",
            "Epoch 303: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 10.0940 - mean_squared_error: 10.0940+0.0000j - val_loss: 3.1860 - val_mean_squared_error: 3.1860+0.0000j - lr: 0.0010\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6792 - mean_squared_error: 9.6792+0.0000j\n",
            "Epoch 304: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 9.6792 - mean_squared_error: 9.6792+0.0000j - val_loss: 2.0549 - val_mean_squared_error: 2.0549+0.0000j - lr: 0.0010\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3225 - mean_squared_error: 9.3225+0.0000j\n",
            "Epoch 305: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 9.3225 - mean_squared_error: 9.3225+0.0000j - val_loss: 1.9622 - val_mean_squared_error: 1.9622+0.0000j - lr: 0.0010\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3259 - mean_squared_error: 9.3259+0.0000j\n",
            "Epoch 306: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 9.3259 - mean_squared_error: 9.3259+0.0000j - val_loss: 2.8593 - val_mean_squared_error: 2.8593+0.0000j - lr: 0.0010\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5606 - mean_squared_error: 9.5606+0.0000j\n",
            "Epoch 307: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 438ms/step - loss: 9.5606 - mean_squared_error: 9.5606+0.0000j - val_loss: 2.0494 - val_mean_squared_error: 2.0494+0.0000j - lr: 0.0010\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7081 - mean_squared_error: 9.7081+0.0000j\n",
            "Epoch 308: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 9.7081 - mean_squared_error: 9.7081+0.0000j - val_loss: 2.8222 - val_mean_squared_error: 2.8222+0.0000j - lr: 0.0010\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6043 - mean_squared_error: 9.6043+0.0000j\n",
            "Epoch 309: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 9.6043 - mean_squared_error: 9.6043+0.0000j - val_loss: 2.0092 - val_mean_squared_error: 2.0092+0.0000j - lr: 0.0010\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3692 - mean_squared_error: 9.3692+0.0000j\n",
            "Epoch 310: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 9.3692 - mean_squared_error: 9.3692+0.0000j - val_loss: 2.1827 - val_mean_squared_error: 2.1827+0.0000j - lr: 0.0010\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2772 - mean_squared_error: 9.2772+0.0000j\n",
            "Epoch 311: val_loss did not improve from 1.94854\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 9.2772 - mean_squared_error: 9.2772+0.0000j - val_loss: 2.4507 - val_mean_squared_error: 2.4507+0.0000j - lr: 0.0010\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3711 - mean_squared_error: 9.3711+0.0000j\n",
            "Epoch 312: val_loss improved from 1.94854 to 1.92728, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 9.3711 - mean_squared_error: 9.3711+0.0000j - val_loss: 1.9273 - val_mean_squared_error: 1.9273+0.0000j - lr: 0.0010\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4883 - mean_squared_error: 9.4883+0.0000j\n",
            "Epoch 313: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 9.4883 - mean_squared_error: 9.4883+0.0000j - val_loss: 2.7108 - val_mean_squared_error: 2.7108+0.0000j - lr: 0.0010\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4807 - mean_squared_error: 9.4807+0.0000j\n",
            "Epoch 314: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 9.4807 - mean_squared_error: 9.4807+0.0000j - val_loss: 2.0463 - val_mean_squared_error: 2.0463+0.0000j - lr: 0.0010\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3507 - mean_squared_error: 9.3507+0.0000j\n",
            "Epoch 315: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 9.3507 - mean_squared_error: 9.3507+0.0000j - val_loss: 2.0841 - val_mean_squared_error: 2.0841+0.0000j - lr: 0.0010\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2729 - mean_squared_error: 9.2729+0.0000j\n",
            "Epoch 316: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 9.2729 - mean_squared_error: 9.2729+0.0000j - val_loss: 2.3483 - val_mean_squared_error: 2.3483+0.0000j - lr: 0.0010\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3194 - mean_squared_error: 9.3194+0.0000j\n",
            "Epoch 317: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 9.3194 - mean_squared_error: 9.3194+0.0000j - val_loss: 2.0147 - val_mean_squared_error: 2.0147+0.0000j - lr: 0.0010\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4079 - mean_squared_error: 9.4079+0.0000j\n",
            "Epoch 318: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 9.4079 - mean_squared_error: 9.4079+0.0000j - val_loss: 2.4892 - val_mean_squared_error: 2.4892+0.0000j - lr: 0.0010\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3987 - mean_squared_error: 9.3987+0.0000j\n",
            "Epoch 319: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 9.3987 - mean_squared_error: 9.3987+0.0000j - val_loss: 1.9879 - val_mean_squared_error: 1.9879+0.0000j - lr: 0.0010\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3185 - mean_squared_error: 9.3185+0.0000j\n",
            "Epoch 320: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 9.3185 - mean_squared_error: 9.3185+0.0000j - val_loss: 2.2500 - val_mean_squared_error: 2.2500+0.0000j - lr: 0.0010\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2761 - mean_squared_error: 9.2761+0.0000j\n",
            "Epoch 321: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 9.2761 - mean_squared_error: 9.2761+0.0000j - val_loss: 2.3132 - val_mean_squared_error: 2.3132+0.0000j - lr: 0.0010\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2936 - mean_squared_error: 9.2936+0.0000j\n",
            "Epoch 322: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 9.2936 - mean_squared_error: 9.2936+0.0000j - val_loss: 1.9284 - val_mean_squared_error: 1.9284+0.0000j - lr: 0.0010\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3617 - mean_squared_error: 9.3617+0.0000j\n",
            "Epoch 323: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 9.3617 - mean_squared_error: 9.3617+0.0000j - val_loss: 2.4911 - val_mean_squared_error: 2.4911+0.0000j - lr: 0.0010\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3764 - mean_squared_error: 9.3764+0.0000j\n",
            "Epoch 324: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 9.3764 - mean_squared_error: 9.3764+0.0000j - val_loss: 2.0714 - val_mean_squared_error: 2.0714+0.0000j - lr: 0.0010\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3158 - mean_squared_error: 9.3158+0.0000j\n",
            "Epoch 325: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 9.3158 - mean_squared_error: 9.3158+0.0000j - val_loss: 2.2528 - val_mean_squared_error: 2.2528+0.0000j - lr: 0.0010\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2708 - mean_squared_error: 9.2708+0.0000j\n",
            "Epoch 326: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 9.2708 - mean_squared_error: 9.2708+0.0000j - val_loss: 2.2221 - val_mean_squared_error: 2.2221+0.0000j - lr: 0.0010\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2602 - mean_squared_error: 9.2602+0.0000j\n",
            "Epoch 327: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 9.2602 - mean_squared_error: 9.2602+0.0000j - val_loss: 2.1851 - val_mean_squared_error: 2.1851+0.0000j - lr: 0.0010\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3309 - mean_squared_error: 9.3309+0.0000j\n",
            "Epoch 328: val_loss did not improve from 1.92728\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 9.3309 - mean_squared_error: 9.3309+0.0000j - val_loss: 2.4283 - val_mean_squared_error: 2.4283+0.0000j - lr: 0.0010\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3595 - mean_squared_error: 9.3595+0.0000j\n",
            "Epoch 329: val_loss improved from 1.92728 to 1.90379, saving model to best_model_complex.h5\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 9.3595 - mean_squared_error: 9.3595+0.0000j - val_loss: 1.9038 - val_mean_squared_error: 1.9038+0.0000j - lr: 0.0010\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3556 - mean_squared_error: 9.3556+0.0000j\n",
            "Epoch 330: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 9.3556 - mean_squared_error: 9.3556+0.0000j - val_loss: 2.3720 - val_mean_squared_error: 2.3720+0.0000j - lr: 0.0010\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2958 - mean_squared_error: 9.2958+0.0000j\n",
            "Epoch 331: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 9.2958 - mean_squared_error: 9.2958+0.0000j - val_loss: 2.1713 - val_mean_squared_error: 2.1713+0.0000j - lr: 0.0010\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2708 - mean_squared_error: 9.2708+0.0000j\n",
            "Epoch 332: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 9.2708 - mean_squared_error: 9.2708+0.0000j - val_loss: 2.0752 - val_mean_squared_error: 2.0752+0.0000j - lr: 0.0010\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2879 - mean_squared_error: 9.2879+0.0000j\n",
            "Epoch 333: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 9.2879 - mean_squared_error: 9.2879+0.0000j - val_loss: 2.3540 - val_mean_squared_error: 2.3540+0.0000j - lr: 0.0010\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2872 - mean_squared_error: 9.2872+0.0000j\n",
            "Epoch 334: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 9.2872 - mean_squared_error: 9.2872+0.0000j - val_loss: 2.0703 - val_mean_squared_error: 2.0703+0.0000j - lr: 0.0010\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3193 - mean_squared_error: 9.3193+0.0000j\n",
            "Epoch 335: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 9.3193 - mean_squared_error: 9.3193+0.0000j - val_loss: 2.2861 - val_mean_squared_error: 2.2861+0.0000j - lr: 0.0010\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2996 - mean_squared_error: 9.2996+0.0000j\n",
            "Epoch 336: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 9.2996 - mean_squared_error: 9.2996+0.0000j - val_loss: 1.9592 - val_mean_squared_error: 1.9592+0.0000j - lr: 0.0010\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2749 - mean_squared_error: 9.2749+0.0000j\n",
            "Epoch 337: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 9.2749 - mean_squared_error: 9.2749+0.0000j - val_loss: 2.1946 - val_mean_squared_error: 2.1946+0.0000j - lr: 0.0010\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2519 - mean_squared_error: 9.2519+0.0000j\n",
            "Epoch 338: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 9.2519 - mean_squared_error: 9.2519+0.0000j - val_loss: 2.3850 - val_mean_squared_error: 2.3850+0.0000j - lr: 0.0010\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2820 - mean_squared_error: 9.2820+0.0000j\n",
            "Epoch 339: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 9.2820 - mean_squared_error: 9.2820+0.0000j - val_loss: 2.0515 - val_mean_squared_error: 2.0515+0.0000j - lr: 0.0010\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2746 - mean_squared_error: 9.2746+0.0000j\n",
            "Epoch 340: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 9.2746 - mean_squared_error: 9.2746+0.0000j - val_loss: 2.2035 - val_mean_squared_error: 2.2035+0.0000j - lr: 0.0010\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2784 - mean_squared_error: 9.2784+0.0000j\n",
            "Epoch 341: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 9.2784 - mean_squared_error: 9.2784+0.0000j - val_loss: 2.1307 - val_mean_squared_error: 2.1307+0.0000j - lr: 0.0010\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3010 - mean_squared_error: 9.3010+0.0000j\n",
            "Epoch 342: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 9.3010 - mean_squared_error: 9.3010+0.0000j - val_loss: 2.1319 - val_mean_squared_error: 2.1319+0.0000j - lr: 0.0010\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2655 - mean_squared_error: 9.2655+0.0000j\n",
            "Epoch 343: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 9.2655 - mean_squared_error: 9.2655+0.0000j - val_loss: 1.9944 - val_mean_squared_error: 1.9944+0.0000j - lr: 0.0010\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2723 - mean_squared_error: 9.2724+0.0000j\n",
            "Epoch 344: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 9.2723 - mean_squared_error: 9.2724+0.0000j - val_loss: 2.1618 - val_mean_squared_error: 2.1618+0.0000j - lr: 0.0010\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2563 - mean_squared_error: 9.2563+0.0000j\n",
            "Epoch 345: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 9.2563 - mean_squared_error: 9.2563+0.0000j - val_loss: 2.3012 - val_mean_squared_error: 2.3012+0.0000j - lr: 0.0010\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2631 - mean_squared_error: 9.2631+0.0000j\n",
            "Epoch 346: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 9.2631 - mean_squared_error: 9.2631+0.0000j - val_loss: 2.0307 - val_mean_squared_error: 2.0308+0.0000j - lr: 0.0010\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2674 - mean_squared_error: 9.2674+0.0000j\n",
            "Epoch 347: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 9.2674 - mean_squared_error: 9.2674+0.0000j - val_loss: 2.2000 - val_mean_squared_error: 2.2000+0.0000j - lr: 0.0010\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2585 - mean_squared_error: 9.2585+0.0000j\n",
            "Epoch 348: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 9.2585 - mean_squared_error: 9.2585+0.0000j - val_loss: 2.1459 - val_mean_squared_error: 2.1459+0.0000j - lr: 0.0010\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2622 - mean_squared_error: 9.2622+0.0000j\n",
            "Epoch 349: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 9.2622 - mean_squared_error: 9.2622+0.0000j - val_loss: 2.1891 - val_mean_squared_error: 2.1891+0.0000j - lr: 0.0010\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2449 - mean_squared_error: 9.2449+0.0000j\n",
            "Epoch 350: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 9.2449 - mean_squared_error: 9.2449+0.0000j - val_loss: 2.1178 - val_mean_squared_error: 2.1178+0.0000j - lr: 0.0010\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2566 - mean_squared_error: 9.2566+0.0000j\n",
            "Epoch 351: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 438ms/step - loss: 9.2566 - mean_squared_error: 9.2566+0.0000j - val_loss: 2.2428 - val_mean_squared_error: 2.2428+0.0000j - lr: 0.0010\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2473 - mean_squared_error: 9.2473+0.0000j\n",
            "Epoch 352: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 447ms/step - loss: 9.2473 - mean_squared_error: 9.2473+0.0000j - val_loss: 2.2521 - val_mean_squared_error: 2.2521+0.0000j - lr: 0.0010\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2396 - mean_squared_error: 9.2396+0.0000j\n",
            "Epoch 353: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 438ms/step - loss: 9.2396 - mean_squared_error: 9.2396+0.0000j - val_loss: 2.0704 - val_mean_squared_error: 2.0704+0.0000j - lr: 0.0010\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2446 - mean_squared_error: 9.2446+0.0000j\n",
            "Epoch 354: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 9.2446 - mean_squared_error: 9.2446+0.0000j - val_loss: 2.1850 - val_mean_squared_error: 2.1850+0.0000j - lr: 0.0010\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2454 - mean_squared_error: 9.2454+0.0000j\n",
            "Epoch 355: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 439ms/step - loss: 9.2454 - mean_squared_error: 9.2454+0.0000j - val_loss: 2.1032 - val_mean_squared_error: 2.1032+0.0000j - lr: 0.0010\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2463 - mean_squared_error: 9.2463+0.0000j\n",
            "Epoch 356: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 9.2463 - mean_squared_error: 9.2463+0.0000j - val_loss: 2.2046 - val_mean_squared_error: 2.2046+0.0000j - lr: 0.0010\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2387 - mean_squared_error: 9.2387+0.0000j\n",
            "Epoch 357: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 9.2387 - mean_squared_error: 9.2387+0.0000j - val_loss: 2.0832 - val_mean_squared_error: 2.0832+0.0000j - lr: 0.0010\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2372 - mean_squared_error: 9.2372+0.0000j\n",
            "Epoch 358: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 9.2372 - mean_squared_error: 9.2372+0.0000j - val_loss: 2.1728 - val_mean_squared_error: 2.1728+0.0000j - lr: 0.0010\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2324 - mean_squared_error: 9.2324+0.0000j\n",
            "Epoch 359: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 9.2324 - mean_squared_error: 9.2324+0.0000j - val_loss: 2.1102 - val_mean_squared_error: 2.1102+0.0000j - lr: 0.0010\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2403 - mean_squared_error: 9.2403+0.0000j\n",
            "Epoch 360: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 9.2403 - mean_squared_error: 9.2403+0.0000j - val_loss: 2.1169 - val_mean_squared_error: 2.1169+0.0000j - lr: 0.0010\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2375 - mean_squared_error: 9.2375+0.0000j\n",
            "Epoch 361: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 9.2375 - mean_squared_error: 9.2375+0.0000j - val_loss: 2.1436 - val_mean_squared_error: 2.1436+0.0000j - lr: 0.0010\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2305 - mean_squared_error: 9.2305+0.0000j\n",
            "Epoch 362: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 9.2305 - mean_squared_error: 9.2305+0.0000j - val_loss: 2.2294 - val_mean_squared_error: 2.2294+0.0000j - lr: 0.0010\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2465 - mean_squared_error: 9.2465+0.0000j\n",
            "Epoch 363: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 9.2465 - mean_squared_error: 9.2465+0.0000j - val_loss: 2.2159 - val_mean_squared_error: 2.2159+0.0000j - lr: 0.0010\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2393 - mean_squared_error: 9.2393+0.0000j\n",
            "Epoch 364: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 9.2393 - mean_squared_error: 9.2393+0.0000j - val_loss: 2.0762 - val_mean_squared_error: 2.0762+0.0000j - lr: 0.0010\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2366 - mean_squared_error: 9.2366+0.0000j\n",
            "Epoch 365: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 9.2366 - mean_squared_error: 9.2366+0.0000j - val_loss: 2.2646 - val_mean_squared_error: 2.2646+0.0000j - lr: 0.0010\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2540 - mean_squared_error: 9.2540+0.0000j\n",
            "Epoch 366: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 9.2540 - mean_squared_error: 9.2540+0.0000j - val_loss: 2.0699 - val_mean_squared_error: 2.0699+0.0000j - lr: 0.0010\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2410 - mean_squared_error: 9.2410+0.0000j\n",
            "Epoch 367: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 9.2410 - mean_squared_error: 9.2410+0.0000j - val_loss: 2.0998 - val_mean_squared_error: 2.0998+0.0000j - lr: 0.0010\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2414 - mean_squared_error: 9.2414+0.0000j\n",
            "Epoch 368: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 9.2414 - mean_squared_error: 9.2414+0.0000j - val_loss: 2.1784 - val_mean_squared_error: 2.1784+0.0000j - lr: 0.0010\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2412 - mean_squared_error: 9.2412+0.0000j\n",
            "Epoch 369: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 9.2412 - mean_squared_error: 9.2412+0.0000j - val_loss: 2.2504 - val_mean_squared_error: 2.2504+0.0000j - lr: 0.0010\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2414 - mean_squared_error: 9.2414+0.0000j\n",
            "Epoch 370: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 9.2414 - mean_squared_error: 9.2414+0.0000j - val_loss: 2.0670 - val_mean_squared_error: 2.0670+0.0000j - lr: 0.0010\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2413 - mean_squared_error: 9.2413+0.0000j\n",
            "Epoch 371: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 9.2413 - mean_squared_error: 9.2413+0.0000j - val_loss: 2.1720 - val_mean_squared_error: 2.1720+0.0000j - lr: 0.0010\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2256 - mean_squared_error: 9.2256+0.0000j\n",
            "Epoch 372: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 9.2256 - mean_squared_error: 9.2256+0.0000j - val_loss: 2.2136 - val_mean_squared_error: 2.2136+0.0000j - lr: 0.0010\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2362 - mean_squared_error: 9.2362+0.0000j\n",
            "Epoch 373: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 9.2362 - mean_squared_error: 9.2362+0.0000j - val_loss: 2.0775 - val_mean_squared_error: 2.0775+0.0000j - lr: 0.0010\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2331 - mean_squared_error: 9.2331+0.0000j\n",
            "Epoch 374: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 9.2331 - mean_squared_error: 9.2331+0.0000j - val_loss: 2.1219 - val_mean_squared_error: 2.1219+0.0000j - lr: 0.0010\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2280 - mean_squared_error: 9.2280+0.0000j\n",
            "Epoch 375: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 9.2280 - mean_squared_error: 9.2280+0.0000j - val_loss: 2.2607 - val_mean_squared_error: 2.2607+0.0000j - lr: 0.0010\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2351 - mean_squared_error: 9.2351+0.0000j\n",
            "Epoch 376: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 9.2351 - mean_squared_error: 9.2351+0.0000j - val_loss: 2.2291 - val_mean_squared_error: 2.2291+0.0000j - lr: 0.0010\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2232 - mean_squared_error: 9.2232+0.0000j\n",
            "Epoch 377: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 9.2232 - mean_squared_error: 9.2232+0.0000j - val_loss: 2.1371 - val_mean_squared_error: 2.1371+0.0000j - lr: 0.0010\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2296 - mean_squared_error: 9.2296+0.0000j\n",
            "Epoch 378: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 9.2296 - mean_squared_error: 9.2296+0.0000j - val_loss: 2.2916 - val_mean_squared_error: 2.2916+0.0000j - lr: 0.0010\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2322 - mean_squared_error: 9.2322+0.0000j\n",
            "Epoch 379: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 9.2322 - mean_squared_error: 9.2322+0.0000j - val_loss: 2.1686 - val_mean_squared_error: 2.1686+0.0000j - lr: 0.0010\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2242 - mean_squared_error: 9.2242+0.0000j\n",
            "Epoch 380: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 9.2242 - mean_squared_error: 9.2242+0.0000j - val_loss: 2.0512 - val_mean_squared_error: 2.0512+0.0000j - lr: 0.0010\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2445 - mean_squared_error: 9.2445+0.0000j\n",
            "Epoch 381: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 9.2445 - mean_squared_error: 9.2445+0.0000j - val_loss: 2.1513 - val_mean_squared_error: 2.1513+0.0000j - lr: 0.0010\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2310 - mean_squared_error: 9.2310+0.0000j\n",
            "Epoch 382: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 9.2310 - mean_squared_error: 9.2310+0.0000j - val_loss: 2.2467 - val_mean_squared_error: 2.2467+0.0000j - lr: 0.0010\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2303 - mean_squared_error: 9.2303+0.0000j\n",
            "Epoch 383: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 9.2303 - mean_squared_error: 9.2303+0.0000j - val_loss: 2.0813 - val_mean_squared_error: 2.0813+0.0000j - lr: 0.0010\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2366 - mean_squared_error: 9.2366+0.0000j\n",
            "Epoch 384: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 9.2366 - mean_squared_error: 9.2366+0.0000j - val_loss: 2.2671 - val_mean_squared_error: 2.2671+0.0000j - lr: 0.0010\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2265 - mean_squared_error: 9.2265+0.0000j\n",
            "Epoch 385: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 9.2265 - mean_squared_error: 9.2265+0.0000j - val_loss: 2.2112 - val_mean_squared_error: 2.2112+0.0000j - lr: 0.0010\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2361 - mean_squared_error: 9.2361+0.0000j\n",
            "Epoch 386: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 9.2361 - mean_squared_error: 9.2361+0.0000j - val_loss: 2.2512 - val_mean_squared_error: 2.2512+0.0000j - lr: 0.0010\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2344 - mean_squared_error: 9.2344+0.0000j\n",
            "Epoch 387: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 9.2344 - mean_squared_error: 9.2344+0.0000j - val_loss: 2.0254 - val_mean_squared_error: 2.0254+0.0000j - lr: 0.0010\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2475 - mean_squared_error: 9.2475+0.0000j\n",
            "Epoch 388: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 454ms/step - loss: 9.2475 - mean_squared_error: 9.2475+0.0000j - val_loss: 2.4667 - val_mean_squared_error: 2.4667+0.0000j - lr: 0.0010\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2745 - mean_squared_error: 9.2745+0.0000j\n",
            "Epoch 389: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 9.2745 - mean_squared_error: 9.2745+0.0000j - val_loss: 2.0315 - val_mean_squared_error: 2.0315+0.0000j - lr: 0.0010\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3068 - mean_squared_error: 9.3068+0.0000j\n",
            "Epoch 390: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 9.3068 - mean_squared_error: 9.3068+0.0000j - val_loss: 2.6983 - val_mean_squared_error: 2.6983+0.0000j - lr: 0.0010\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3953 - mean_squared_error: 9.3953+0.0000j\n",
            "Epoch 391: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 9.3953 - mean_squared_error: 9.3953+0.0000j - val_loss: 2.1932 - val_mean_squared_error: 2.1932+0.0000j - lr: 0.0010\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.6011 - mean_squared_error: 9.6011+0.0000j\n",
            "Epoch 392: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 9.6011 - mean_squared_error: 9.6011+0.0000j - val_loss: 3.6226 - val_mean_squared_error: 3.6226+0.0000j - lr: 0.0010\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9648 - mean_squared_error: 9.9648+0.0000j\n",
            "Epoch 393: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 9.9648 - mean_squared_error: 9.9648+0.0000j - val_loss: 2.6109 - val_mean_squared_error: 2.6109+0.0000j - lr: 0.0010\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.7161 - mean_squared_error: 10.7161+0.0000j\n",
            "Epoch 394: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 10.7161 - mean_squared_error: 10.7161+0.0000j - val_loss: 5.9621 - val_mean_squared_error: 5.9621+0.0000j - lr: 0.0010\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 11.8068 - mean_squared_error: 11.8068+0.0000j\n",
            "Epoch 395: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 11.8068 - mean_squared_error: 11.8068+0.0000j - val_loss: 4.3284 - val_mean_squared_error: 4.3284+0.0000j - lr: 0.0010\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 12.9032 - mean_squared_error: 12.9032+0.0000j\n",
            "Epoch 396: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 12.9032 - mean_squared_error: 12.9032+0.0000j - val_loss: 7.1601 - val_mean_squared_error: 7.1601+0.0000j - lr: 0.0010\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 12.7914 - mean_squared_error: 12.7914+0.0000j\n",
            "Epoch 397: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 12.7914 - mean_squared_error: 12.7914+0.0000j - val_loss: 2.7220 - val_mean_squared_error: 2.7220+0.0000j - lr: 0.0010\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.7604 - mean_squared_error: 10.7604+0.0000j\n",
            "Epoch 398: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 10.7604 - mean_squared_error: 10.7604+0.0000j - val_loss: 2.4974 - val_mean_squared_error: 2.4974+0.0000j - lr: 0.0010\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2920 - mean_squared_error: 9.2920+0.0000j\n",
            "Epoch 399: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 9.2920 - mean_squared_error: 9.2920+0.0000j - val_loss: 3.7305 - val_mean_squared_error: 3.7305+0.0000j - lr: 0.0010\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0560 - mean_squared_error: 10.0560+0.0000j\n",
            "Epoch 400: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 10.0560 - mean_squared_error: 10.0560+0.0000j - val_loss: 2.7695 - val_mean_squared_error: 2.7695+0.0000j - lr: 0.0010\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.9390 - mean_squared_error: 10.9390+0.0000j\n",
            "Epoch 401: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 10.9390 - mean_squared_error: 10.9390+0.0000j - val_loss: 3.7686 - val_mean_squared_error: 3.7686+0.0000j - lr: 0.0010\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0506 - mean_squared_error: 10.0506+0.0000j\n",
            "Epoch 402: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 10.0506 - mean_squared_error: 10.0506+0.0000j - val_loss: 2.2694 - val_mean_squared_error: 2.2694+0.0000j - lr: 0.0010\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2638 - mean_squared_error: 9.2638+0.0000j\n",
            "Epoch 403: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 9.2638 - mean_squared_error: 9.2638+0.0000j - val_loss: 2.1952 - val_mean_squared_error: 2.1952+0.0000j - lr: 0.0010\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0669 - mean_squared_error: 10.0669+0.0000j\n",
            "Epoch 404: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 10.0669 - mean_squared_error: 10.0669+0.0000j - val_loss: 4.0660 - val_mean_squared_error: 4.0660+0.0000j - lr: 0.0010\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.2368 - mean_squared_error: 10.2368+0.0000j\n",
            "Epoch 405: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 10.2368 - mean_squared_error: 10.2368+0.0000j - val_loss: 2.1000 - val_mean_squared_error: 2.1000+0.0000j - lr: 0.0010\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3528 - mean_squared_error: 9.3528+0.0000j\n",
            "Epoch 406: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 9.3528 - mean_squared_error: 9.3528+0.0000j - val_loss: 1.9857 - val_mean_squared_error: 1.9857+0.0000j - lr: 0.0010\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5403 - mean_squared_error: 9.5403+0.0000j\n",
            "Epoch 407: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 9.5403 - mean_squared_error: 9.5403+0.0000j - val_loss: 3.6931 - val_mean_squared_error: 3.6931+0.0000j - lr: 0.0010\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.0198 - mean_squared_error: 10.0198+0.0000j\n",
            "Epoch 408: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 10.0198 - mean_squared_error: 10.0198+0.0000j - val_loss: 2.0541 - val_mean_squared_error: 2.0541+0.0000j - lr: 0.0010\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4590 - mean_squared_error: 9.4590+0.0000j\n",
            "Epoch 409: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 9.4590 - mean_squared_error: 9.4590+0.0000j - val_loss: 2.0371 - val_mean_squared_error: 2.0371+0.0000j - lr: 0.0010\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3161 - mean_squared_error: 9.3161+0.0000j\n",
            "Epoch 410: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 9.3161 - mean_squared_error: 9.3161+0.0000j - val_loss: 3.3374 - val_mean_squared_error: 3.3374+0.0000j - lr: 0.0010\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.7550 - mean_squared_error: 9.7550+0.0000j\n",
            "Epoch 411: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 9.7550 - mean_squared_error: 9.7550+0.0000j - val_loss: 2.1123 - val_mean_squared_error: 2.1123+0.0000j - lr: 0.0010\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4959 - mean_squared_error: 9.4959+0.0000j\n",
            "Epoch 412: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 9.4959 - mean_squared_error: 9.4959+0.0000j - val_loss: 2.1215 - val_mean_squared_error: 2.1215+0.0000j - lr: 0.0010\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2402 - mean_squared_error: 9.2402+0.0000j\n",
            "Epoch 413: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 9.2402 - mean_squared_error: 9.2402+0.0000j - val_loss: 2.9667 - val_mean_squared_error: 2.9667+0.0000j - lr: 0.0010\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.5415 - mean_squared_error: 9.5415+0.0000j\n",
            "Epoch 414: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 9.5415 - mean_squared_error: 9.5415+0.0000j - val_loss: 2.0622 - val_mean_squared_error: 2.0622+0.0000j - lr: 0.0010\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4743 - mean_squared_error: 9.4743+0.0000j\n",
            "Epoch 415: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 9.4743 - mean_squared_error: 9.4743+0.0000j - val_loss: 2.2875 - val_mean_squared_error: 2.2875+0.0000j - lr: 0.0010\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2333 - mean_squared_error: 9.2333+0.0000j\n",
            "Epoch 416: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 9.2333 - mean_squared_error: 9.2333+0.0000j - val_loss: 2.7216 - val_mean_squared_error: 2.7216+0.0000j - lr: 0.0010\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4057 - mean_squared_error: 9.4057+0.0000j\n",
            "Epoch 417: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 9.4057 - mean_squared_error: 9.4057+0.0000j - val_loss: 2.0104 - val_mean_squared_error: 2.0104+0.0000j - lr: 0.0010\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.4379 - mean_squared_error: 9.4379+0.0000j\n",
            "Epoch 418: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 335ms/step - loss: 9.4379 - mean_squared_error: 9.4379+0.0000j - val_loss: 2.3409 - val_mean_squared_error: 2.3409+0.0000j - lr: 0.0010\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2461 - mean_squared_error: 9.2461+0.0000j\n",
            "Epoch 419: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 9.2461 - mean_squared_error: 9.2461+0.0000j - val_loss: 2.5271 - val_mean_squared_error: 2.5271+0.0000j - lr: 0.0010\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3246 - mean_squared_error: 9.3246+0.0000j\n",
            "Epoch 420: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 9.3246 - mean_squared_error: 9.3246+0.0000j - val_loss: 2.0143 - val_mean_squared_error: 2.0143+0.0000j - lr: 0.0010\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3864 - mean_squared_error: 9.3864+0.0000j\n",
            "Epoch 421: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 9.3864 - mean_squared_error: 9.3864+0.0000j - val_loss: 2.4512 - val_mean_squared_error: 2.4512+0.0000j - lr: 0.0010\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2382 - mean_squared_error: 9.2382+0.0000j\n",
            "Epoch 422: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 9.2382 - mean_squared_error: 9.2382+0.0000j - val_loss: 2.5825 - val_mean_squared_error: 2.5825+0.0000j - lr: 0.0010\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2815 - mean_squared_error: 9.2815+0.0000j\n",
            "Epoch 423: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 9.2815 - mean_squared_error: 9.2815+0.0000j - val_loss: 2.0987 - val_mean_squared_error: 2.0987+0.0000j - lr: 0.0010\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3442 - mean_squared_error: 9.3442+0.0000j\n",
            "Epoch 424: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 9.3442 - mean_squared_error: 9.3442+0.0000j - val_loss: 2.3315 - val_mean_squared_error: 2.3315+0.0000j - lr: 0.0010\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2389 - mean_squared_error: 9.2389+0.0000j\n",
            "Epoch 425: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 9.2389 - mean_squared_error: 9.2389+0.0000j - val_loss: 2.3990 - val_mean_squared_error: 2.3990+0.0000j - lr: 0.0010\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2580 - mean_squared_error: 9.2580+0.0000j\n",
            "Epoch 426: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 9.2580 - mean_squared_error: 9.2580+0.0000j - val_loss: 1.9930 - val_mean_squared_error: 1.9930+0.0000j - lr: 0.0010\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.3112 - mean_squared_error: 9.3112+0.0000j\n",
            "Epoch 427: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 9.3112 - mean_squared_error: 9.3112+0.0000j - val_loss: 2.3137 - val_mean_squared_error: 2.3137+0.0000j - lr: 0.0010\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2420 - mean_squared_error: 9.2420+0.0000j\n",
            "Epoch 428: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 9.2420 - mean_squared_error: 9.2420+0.0000j - val_loss: 2.3259 - val_mean_squared_error: 2.3259+0.0000j - lr: 0.0010\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2219 - mean_squared_error: 9.2219+0.0000j\n",
            "Epoch 429: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 9.2219 - mean_squared_error: 9.2219+0.0000j - val_loss: 2.0832 - val_mean_squared_error: 2.0832+0.0000j - lr: 0.0010\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2758 - mean_squared_error: 9.2758+0.0000j\n",
            "Epoch 430: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 9.2758 - mean_squared_error: 9.2758+0.0000j - val_loss: 2.3500 - val_mean_squared_error: 2.3500+0.0000j - lr: 0.0010\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2411 - mean_squared_error: 9.2411+0.0000j\n",
            "Epoch 431: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 9.2411 - mean_squared_error: 9.2411+0.0000j - val_loss: 2.2233 - val_mean_squared_error: 2.2233+0.0000j - lr: 0.0010\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2051 - mean_squared_error: 9.2051+0.0000j\n",
            "Epoch 432: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 439ms/step - loss: 9.2051 - mean_squared_error: 9.2051+0.0000j - val_loss: 2.0621 - val_mean_squared_error: 2.0621+0.0000j - lr: 0.0010\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2458 - mean_squared_error: 9.2458+0.0000j\n",
            "Epoch 433: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 9.2458 - mean_squared_error: 9.2458+0.0000j - val_loss: 2.3678 - val_mean_squared_error: 2.3678+0.0000j - lr: 0.0010\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2360 - mean_squared_error: 9.2360+0.0000j\n",
            "Epoch 434: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 9.2360 - mean_squared_error: 9.2360+0.0000j - val_loss: 2.2599 - val_mean_squared_error: 2.2599+0.0000j - lr: 0.0010\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1969 - mean_squared_error: 9.1969+0.0000j\n",
            "Epoch 435: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 438ms/step - loss: 9.1969 - mean_squared_error: 9.1969+0.0000j - val_loss: 2.2479 - val_mean_squared_error: 2.2479+0.0000j - lr: 0.0010\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2383 - mean_squared_error: 9.2383+0.0000j\n",
            "Epoch 436: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 9.2383 - mean_squared_error: 9.2383+0.0000j - val_loss: 2.4282 - val_mean_squared_error: 2.4282+0.0000j - lr: 0.0010\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2309 - mean_squared_error: 9.2309+0.0000j\n",
            "Epoch 437: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 9.2309 - mean_squared_error: 9.2309+0.0000j - val_loss: 2.1133 - val_mean_squared_error: 2.1133+0.0000j - lr: 0.0010\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2094 - mean_squared_error: 9.2094+0.0000j\n",
            "Epoch 438: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 9.2094 - mean_squared_error: 9.2094+0.0000j - val_loss: 2.1359 - val_mean_squared_error: 2.1359+0.0000j - lr: 0.0010\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2281 - mean_squared_error: 9.2281+0.0000j\n",
            "Epoch 439: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 9.2281 - mean_squared_error: 9.2281+0.0000j - val_loss: 2.3417 - val_mean_squared_error: 2.3417+0.0000j - lr: 0.0010\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2311 - mean_squared_error: 9.2311+0.0000j\n",
            "Epoch 440: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 9.2311 - mean_squared_error: 9.2311+0.0000j - val_loss: 2.0952 - val_mean_squared_error: 2.0952+0.0000j - lr: 0.0010\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2211 - mean_squared_error: 9.2211+0.0000j\n",
            "Epoch 441: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 9.2211 - mean_squared_error: 9.2211+0.0000j - val_loss: 2.2434 - val_mean_squared_error: 2.2434+0.0000j - lr: 0.0010\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2134 - mean_squared_error: 9.2134+0.0000j\n",
            "Epoch 442: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 335ms/step - loss: 9.2134 - mean_squared_error: 9.2134+0.0000j - val_loss: 2.4389 - val_mean_squared_error: 2.4389+0.0000j - lr: 0.0010\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2200 - mean_squared_error: 9.2200+0.0000j\n",
            "Epoch 443: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 9.2200 - mean_squared_error: 9.2200+0.0000j - val_loss: 2.0894 - val_mean_squared_error: 2.0894+0.0000j - lr: 0.0010\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2115 - mean_squared_error: 9.2115+0.0000j\n",
            "Epoch 444: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 9.2115 - mean_squared_error: 9.2115+0.0000j - val_loss: 2.1967 - val_mean_squared_error: 2.1967+0.0000j - lr: 0.0010\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1954 - mean_squared_error: 9.1954+0.0000j\n",
            "Epoch 445: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 9.1954 - mean_squared_error: 9.1954+0.0000j - val_loss: 2.3280 - val_mean_squared_error: 2.3280+0.0000j - lr: 0.0010\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2089 - mean_squared_error: 9.2089+0.0000j\n",
            "Epoch 446: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 9.2089 - mean_squared_error: 9.2089+0.0000j - val_loss: 2.1146 - val_mean_squared_error: 2.1146+0.0000j - lr: 0.0010\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2056 - mean_squared_error: 9.2056+0.0000j\n",
            "Epoch 447: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 9.2056 - mean_squared_error: 9.2056+0.0000j - val_loss: 2.2464 - val_mean_squared_error: 2.2464+0.0000j - lr: 0.0010\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1964 - mean_squared_error: 9.1964+0.0000j\n",
            "Epoch 448: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 9.1964 - mean_squared_error: 9.1964+0.0000j - val_loss: 2.3162 - val_mean_squared_error: 2.3162+0.0000j - lr: 0.0010\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1987 - mean_squared_error: 9.1987+0.0000j\n",
            "Epoch 449: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 9.1987 - mean_squared_error: 9.1987+0.0000j - val_loss: 2.1595 - val_mean_squared_error: 2.1595+0.0000j - lr: 0.0010\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1986 - mean_squared_error: 9.1986+0.0000j\n",
            "Epoch 450: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 9.1986 - mean_squared_error: 9.1986+0.0000j - val_loss: 2.2355 - val_mean_squared_error: 2.2355+0.0000j - lr: 0.0010\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1973 - mean_squared_error: 9.1973+0.0000j\n",
            "Epoch 451: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 9.1973 - mean_squared_error: 9.1973+0.0000j - val_loss: 2.2009 - val_mean_squared_error: 2.2009+0.0000j - lr: 7.5000e-04\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1864 - mean_squared_error: 9.1864+0.0000j\n",
            "Epoch 452: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 9.1864 - mean_squared_error: 9.1864+0.0000j - val_loss: 2.1780 - val_mean_squared_error: 2.1780+0.0000j - lr: 7.5000e-04\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1897 - mean_squared_error: 9.1897+0.0000j\n",
            "Epoch 453: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 9.1897 - mean_squared_error: 9.1897+0.0000j - val_loss: 2.2299 - val_mean_squared_error: 2.2299+0.0000j - lr: 7.5000e-04\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1871 - mean_squared_error: 9.1871+0.0000j\n",
            "Epoch 454: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 9.1871 - mean_squared_error: 9.1871+0.0000j - val_loss: 2.2611 - val_mean_squared_error: 2.2611+0.0000j - lr: 7.5000e-04\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1877 - mean_squared_error: 9.1877+0.0000j\n",
            "Epoch 455: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 9.1877 - mean_squared_error: 9.1877+0.0000j - val_loss: 2.2562 - val_mean_squared_error: 2.2562+0.0000j - lr: 7.5000e-04\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1829 - mean_squared_error: 9.1829+0.0000j\n",
            "Epoch 456: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 9.1829 - mean_squared_error: 9.1829+0.0000j - val_loss: 2.2981 - val_mean_squared_error: 2.2981+0.0000j - lr: 7.5000e-04\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1935 - mean_squared_error: 9.1935+0.0000j\n",
            "Epoch 457: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 9.1935 - mean_squared_error: 9.1935+0.0000j - val_loss: 2.2245 - val_mean_squared_error: 2.2245+0.0000j - lr: 7.5000e-04\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1891 - mean_squared_error: 9.1891+0.0000j\n",
            "Epoch 458: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 9.1891 - mean_squared_error: 9.1891+0.0000j - val_loss: 2.1726 - val_mean_squared_error: 2.1726+0.0000j - lr: 7.5000e-04\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1779 - mean_squared_error: 9.1779+0.0000j\n",
            "Epoch 459: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 9.1779 - mean_squared_error: 9.1779+0.0000j - val_loss: 2.1456 - val_mean_squared_error: 2.1456+0.0000j - lr: 7.5000e-04\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1848 - mean_squared_error: 9.1848+0.0000j\n",
            "Epoch 460: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 9.1848 - mean_squared_error: 9.1848+0.0000j - val_loss: 2.2232 - val_mean_squared_error: 2.2232+0.0000j - lr: 7.5000e-04\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1765 - mean_squared_error: 9.1765+0.0000j\n",
            "Epoch 461: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 9.1765 - mean_squared_error: 9.1765+0.0000j - val_loss: 2.1679 - val_mean_squared_error: 2.1679+0.0000j - lr: 7.5000e-04\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1853 - mean_squared_error: 9.1853+0.0000j\n",
            "Epoch 462: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 9.1853 - mean_squared_error: 9.1853+0.0000j - val_loss: 2.1623 - val_mean_squared_error: 2.1623+0.0000j - lr: 7.5000e-04\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1760 - mean_squared_error: 9.1760+0.0000j\n",
            "Epoch 463: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 9.1760 - mean_squared_error: 9.1760+0.0000j - val_loss: 2.3216 - val_mean_squared_error: 2.3216+0.0000j - lr: 7.5000e-04\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1890 - mean_squared_error: 9.1890+0.0000j\n",
            "Epoch 464: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 9.1890 - mean_squared_error: 9.1890+0.0000j - val_loss: 2.2414 - val_mean_squared_error: 2.2414+0.0000j - lr: 7.5000e-04\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1743 - mean_squared_error: 9.1743+0.0000j\n",
            "Epoch 465: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 9.1743 - mean_squared_error: 9.1743+0.0000j - val_loss: 2.1573 - val_mean_squared_error: 2.1573+0.0000j - lr: 7.5000e-04\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1767 - mean_squared_error: 9.1767+0.0000j\n",
            "Epoch 466: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 9.1767 - mean_squared_error: 9.1767+0.0000j - val_loss: 2.2581 - val_mean_squared_error: 2.2581+0.0000j - lr: 7.5000e-04\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1790 - mean_squared_error: 9.1790+0.0000j\n",
            "Epoch 467: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 9.1790 - mean_squared_error: 9.1790+0.0000j - val_loss: 2.2936 - val_mean_squared_error: 2.2936+0.0000j - lr: 7.5000e-04\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1731 - mean_squared_error: 9.1731+0.0000j\n",
            "Epoch 468: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 9.1731 - mean_squared_error: 9.1731+0.0000j - val_loss: 2.1397 - val_mean_squared_error: 2.1397+0.0000j - lr: 7.5000e-04\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1852 - mean_squared_error: 9.1852+0.0000j\n",
            "Epoch 469: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 9.1852 - mean_squared_error: 9.1852+0.0000j - val_loss: 2.1257 - val_mean_squared_error: 2.1257+0.0000j - lr: 7.5000e-04\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1725 - mean_squared_error: 9.1725+0.0000j\n",
            "Epoch 470: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 9.1725 - mean_squared_error: 9.1725+0.0000j - val_loss: 2.2839 - val_mean_squared_error: 2.2839+0.0000j - lr: 7.5000e-04\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1885 - mean_squared_error: 9.1885+0.0000j\n",
            "Epoch 471: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 9.1885 - mean_squared_error: 9.1885+0.0000j - val_loss: 2.2276 - val_mean_squared_error: 2.2276+0.0000j - lr: 7.5000e-04\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1798 - mean_squared_error: 9.1798+0.0000j\n",
            "Epoch 472: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 9.1798 - mean_squared_error: 9.1798+0.0000j - val_loss: 2.1034 - val_mean_squared_error: 2.1034+0.0000j - lr: 7.5000e-04\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1814 - mean_squared_error: 9.1814+0.0000j\n",
            "Epoch 473: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 9.1814 - mean_squared_error: 9.1814+0.0000j - val_loss: 2.1933 - val_mean_squared_error: 2.1933+0.0000j - lr: 7.5000e-04\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1709 - mean_squared_error: 9.1709+0.0000j\n",
            "Epoch 474: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 9.1709 - mean_squared_error: 9.1709+0.0000j - val_loss: 2.2595 - val_mean_squared_error: 2.2595+0.0000j - lr: 7.5000e-04\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1786 - mean_squared_error: 9.1786+0.0000j\n",
            "Epoch 475: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 9.1786 - mean_squared_error: 9.1786+0.0000j - val_loss: 2.1591 - val_mean_squared_error: 2.1591+0.0000j - lr: 7.5000e-04\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1729 - mean_squared_error: 9.1729+0.0000j\n",
            "Epoch 476: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 9.1729 - mean_squared_error: 9.1729+0.0000j - val_loss: 2.1137 - val_mean_squared_error: 2.1137+0.0000j - lr: 7.5000e-04\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1763 - mean_squared_error: 9.1763+0.0000j\n",
            "Epoch 477: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 9.1763 - mean_squared_error: 9.1763+0.0000j - val_loss: 2.2398 - val_mean_squared_error: 2.2398+0.0000j - lr: 7.5000e-04\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1710 - mean_squared_error: 9.1710+0.0000j\n",
            "Epoch 478: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 9.1710 - mean_squared_error: 9.1710+0.0000j - val_loss: 2.3085 - val_mean_squared_error: 2.3085+0.0000j - lr: 7.5000e-04\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1729 - mean_squared_error: 9.1729+0.0000j\n",
            "Epoch 479: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 9.1729 - mean_squared_error: 9.1729+0.0000j - val_loss: 2.2427 - val_mean_squared_error: 2.2427+0.0000j - lr: 7.5000e-04\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1663 - mean_squared_error: 9.1663+0.0000j\n",
            "Epoch 480: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 392ms/step - loss: 9.1663 - mean_squared_error: 9.1663+0.0000j - val_loss: 2.1944 - val_mean_squared_error: 2.1944+0.0000j - lr: 7.5000e-04\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1717 - mean_squared_error: 9.1717+0.0000j\n",
            "Epoch 481: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 9.1717 - mean_squared_error: 9.1717+0.0000j - val_loss: 2.2696 - val_mean_squared_error: 2.2696+0.0000j - lr: 7.5000e-04\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1628 - mean_squared_error: 9.1628+0.0000j\n",
            "Epoch 482: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 9.1628 - mean_squared_error: 9.1628+0.0000j - val_loss: 2.2742 - val_mean_squared_error: 2.2742+0.0000j - lr: 7.5000e-04\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1702 - mean_squared_error: 9.1702+0.0000j\n",
            "Epoch 483: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 9.1702 - mean_squared_error: 9.1702+0.0000j - val_loss: 2.1514 - val_mean_squared_error: 2.1514+0.0000j - lr: 7.5000e-04\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1646 - mean_squared_error: 9.1646+0.0000j\n",
            "Epoch 484: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 9.1646 - mean_squared_error: 9.1646+0.0000j - val_loss: 2.1792 - val_mean_squared_error: 2.1792+0.0000j - lr: 7.5000e-04\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1637 - mean_squared_error: 9.1637+0.0000j\n",
            "Epoch 485: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 9.1637 - mean_squared_error: 9.1637+0.0000j - val_loss: 2.2426 - val_mean_squared_error: 2.2426+0.0000j - lr: 7.5000e-04\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1638 - mean_squared_error: 9.1638+0.0000j\n",
            "Epoch 486: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 9.1638 - mean_squared_error: 9.1638+0.0000j - val_loss: 2.1896 - val_mean_squared_error: 2.1896+0.0000j - lr: 7.5000e-04\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1629 - mean_squared_error: 9.1629+0.0000j\n",
            "Epoch 487: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 9.1629 - mean_squared_error: 9.1629+0.0000j - val_loss: 2.1552 - val_mean_squared_error: 2.1552+0.0000j - lr: 7.5000e-04\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1639 - mean_squared_error: 9.1639+0.0000j\n",
            "Epoch 488: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 9.1639 - mean_squared_error: 9.1639+0.0000j - val_loss: 2.1955 - val_mean_squared_error: 2.1955+0.0000j - lr: 7.5000e-04\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1629 - mean_squared_error: 9.1629+0.0000j\n",
            "Epoch 489: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 9.1629 - mean_squared_error: 9.1629+0.0000j - val_loss: 2.2455 - val_mean_squared_error: 2.2455+0.0000j - lr: 7.5000e-04\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1630 - mean_squared_error: 9.1630+0.0000j\n",
            "Epoch 490: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 9.1630 - mean_squared_error: 9.1630+0.0000j - val_loss: 2.2010 - val_mean_squared_error: 2.2010+0.0000j - lr: 7.5000e-04\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1592 - mean_squared_error: 9.1592+0.0000j\n",
            "Epoch 491: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 9.1592 - mean_squared_error: 9.1592+0.0000j - val_loss: 2.1577 - val_mean_squared_error: 2.1577+0.0000j - lr: 7.5000e-04\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1611 - mean_squared_error: 9.1611+0.0000j\n",
            "Epoch 492: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 9.1611 - mean_squared_error: 9.1611+0.0000j - val_loss: 2.1922 - val_mean_squared_error: 2.1922+0.0000j - lr: 7.5000e-04\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1599 - mean_squared_error: 9.1599+0.0000j\n",
            "Epoch 493: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 9.1599 - mean_squared_error: 9.1599+0.0000j - val_loss: 2.2253 - val_mean_squared_error: 2.2253+0.0000j - lr: 7.5000e-04\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1597 - mean_squared_error: 9.1597+0.0000j\n",
            "Epoch 494: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 9.1597 - mean_squared_error: 9.1597+0.0000j - val_loss: 2.2110 - val_mean_squared_error: 2.2110+0.0000j - lr: 7.5000e-04\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1581 - mean_squared_error: 9.1581+0.0000j\n",
            "Epoch 495: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 9.1581 - mean_squared_error: 9.1581+0.0000j - val_loss: 2.2251 - val_mean_squared_error: 2.2251+0.0000j - lr: 7.5000e-04\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1579 - mean_squared_error: 9.1579+0.0000j\n",
            "Epoch 496: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 9.1579 - mean_squared_error: 9.1579+0.0000j - val_loss: 2.2407 - val_mean_squared_error: 2.2407+0.0000j - lr: 7.5000e-04\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1541 - mean_squared_error: 9.1541+0.0000j\n",
            "Epoch 497: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 9.1541 - mean_squared_error: 9.1541+0.0000j - val_loss: 2.2448 - val_mean_squared_error: 2.2448+0.0000j - lr: 7.5000e-04\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1580 - mean_squared_error: 9.1580+0.0000j\n",
            "Epoch 498: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 9.1580 - mean_squared_error: 9.1580+0.0000j - val_loss: 2.2073 - val_mean_squared_error: 2.2073+0.0000j - lr: 7.5000e-04\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1534 - mean_squared_error: 9.1534+0.0000j\n",
            "Epoch 499: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 9.1534 - mean_squared_error: 9.1534+0.0000j - val_loss: 2.1499 - val_mean_squared_error: 2.1499+0.0000j - lr: 7.5000e-04\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.1596 - mean_squared_error: 9.1596+0.0000j\n",
            "Epoch 500: val_loss did not improve from 1.90379\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 9.1596 - mean_squared_error: 9.1596+0.0000j - val_loss: 2.1718 - val_mean_squared_error: 2.1718+0.0000j - lr: 7.5000e-04\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 2.1718 - mean_squared_error: 2.1718+0.0000j\n",
            "Test Loss: [2.171785593032837, (2.171785593032837+0j)]\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "[[ 3 50 13]\n",
            " [ 3 20  8]\n",
            " [ 3 70 12]\n",
            " [ 3 10  8]\n",
            " [ 4 30 13]\n",
            " [ 4 10 14]\n",
            " [ 4 40 14]\n",
            " [ 3 20  9]\n",
            " [ 4 70 11]\n",
            " [ 4 40  9]\n",
            " [ 4 10  7]\n",
            " [ 3 30 14]\n",
            " [ 2 70 13]\n",
            " [ 2 30  9]\n",
            " [ 2 50 10]\n",
            " [ 2 10 11]\n",
            " [ 2 30  8]\n",
            " [ 2 40 12]\n",
            " [ 3 50 10]\n",
            " [ 2 30 12]\n",
            " [ 4 50  7]\n",
            " [ 4 20  7]\n",
            " [ 3 30  8]\n",
            " [ 4 40  7]\n",
            " [ 3 50  7]\n",
            " [ 3 20  7]\n",
            " [ 4 50 10]\n",
            " [ 3 70 11]\n",
            " [ 3 60 10]\n",
            " [ 3 60  9]\n",
            " [ 4 70  8]\n",
            " [ 3 70  9]\n",
            " [ 2 60 14]\n",
            " [ 2 60 12]\n",
            " [ 2 60 10]\n",
            " [ 3 40 13]\n",
            " [ 2 20  7]\n",
            " [ 3 20 11]\n",
            " [ 3 20 13]\n",
            " [ 2 30 14]\n",
            " [ 2 60 11]\n",
            " [ 3 20 12]]   [[ 2.9846814 49.931633  13.063028 ]\n",
            " [ 3.1772592 17.632345   8.079893 ]\n",
            " [ 2.7034428 64.9505    11.51668  ]\n",
            " [ 3.4596934  9.979285   8.645816 ]\n",
            " [ 3.137306  29.949907  12.620928 ]\n",
            " [ 2.856541   9.750496  12.281685 ]\n",
            " [ 2.840896  39.843933  13.508495 ]\n",
            " [ 3.1036196 18.815447   9.121485 ]\n",
            " [ 2.576573  66.94713   10.52949  ]\n",
            " [ 2.9759119 39.86357    8.871881 ]\n",
            " [ 3.892399   9.971545   6.9691734]\n",
            " [ 3.1970994 30.054665  13.871885 ]\n",
            " [ 2.8333113 62.788486  12.452187 ]\n",
            " [ 3.3656356 30.086777   9.496973 ]\n",
            " [ 2.9859688 50.00553   10.386878 ]\n",
            " [ 2.6535766  9.955276  11.980106 ]\n",
            " [ 3.4265788 30.04429    8.392848 ]\n",
            " [ 2.8593926 39.9164    12.347053 ]\n",
            " [ 2.9467816 49.80889   10.167845 ]\n",
            " [ 3.1475246 30.075407  12.534842 ]\n",
            " [ 2.87559   49.89483    6.749476 ]\n",
            " [ 3.3445535 16.09515    6.5149136]\n",
            " [ 3.4201794 29.967474   8.126081 ]\n",
            " [ 2.8184116 39.973213   6.7821374]\n",
            " [ 2.89552   50.003063   6.8862977]\n",
            " [ 3.2547352 16.471806   6.9686403]\n",
            " [ 2.9187784 49.735703   9.98884  ]\n",
            " [ 2.5913622 66.71285   10.68542  ]\n",
            " [ 3.3270774 59.933945  10.319254 ]\n",
            " [ 3.2358267 59.989418   9.255109 ]\n",
            " [ 3.0536892 70.934166   7.957503 ]\n",
            " [ 2.8430748 69.49877    9.052262 ]\n",
            " [ 3.366756  59.582916  13.995819 ]\n",
            " [ 3.4921348 59.711025  12.338471 ]\n",
            " [ 3.344037  60.00455   10.477828 ]\n",
            " [ 2.8531356 39.882507  12.980679 ]\n",
            " [ 3.413891  10.102953   8.270972 ]\n",
            " [ 3.0319936 19.793139  11.186024 ]\n",
            " [ 2.9139645 20.290771  13.04068  ]\n",
            " [ 3.2238595 30.157625  14.257799 ]\n",
            " [ 3.411943  59.719112  11.353164 ]\n",
            " [ 2.9640777 20.061033  12.140928 ]]\n"
          ]
        }
      ],
      "source": [
        "checkpoint_complex = ModelCheckpoint('best_model_complex.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "model_complex.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[checkpoint_complex, lr_scheduler])\n",
        "\n",
        "loss_complex = model_complex.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss_complex)\n",
        "\n",
        "predictions_complex = model_complex.predict(X_test)\n",
        "\n",
        "print(y_test, \" \", predictions_complex)\n",
        "\n",
        "errors_complex = np.abs(predictions_complex - y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF03PYd331Nh"
      },
      "outputs": [],
      "source": [
        "# best val loss: 1.90379"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo29RgB20wo9",
        "outputId": "42091bad-2b19-43bf-84e4-599b9ce21bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 3 50 13]   [ 2.9846814 49.931633  13.063028 ]\n",
            "[ 3 20  8]   [ 3.1772592 17.632345   8.079893 ]\n",
            "[ 3 70 12]   [ 2.7034428 64.9505    11.51668  ]\n",
            "[ 3 10  8]   [3.4596934 9.979285  8.645816 ]\n",
            "[ 4 30 13]   [ 3.137306 29.949907 12.620928]\n",
            "[ 4 10 14]   [ 2.856541  9.750496 12.281685]\n",
            "[ 4 40 14]   [ 2.840896 39.843933 13.508495]\n",
            "[ 3 20  9]   [ 3.1036196 18.815447   9.121485 ]\n",
            "[ 4 70 11]   [ 2.576573 66.94713  10.52949 ]\n",
            "[ 4 40  9]   [ 2.9759119 39.86357    8.871881 ]\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print(y_test[i, :], \" \", predictions_complex[i, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj3eqOZ8cEVL"
      },
      "source": [
        "# neural architecture search for complex-valued networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dynq8yMe4gWr",
        "outputId": "741e8651-ee8b-465a-8fe4-0371e7852afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttSIi7W8cOTU",
        "outputId": "973bc5dd-1aa9-4c6b-87ff-479b2bd57230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 50 Complete [00h 02m 26s]\n",
            "val_loss: 3.055241823196411\n",
            "\n",
            "Best val_loss So Far: 2.1139559745788574\n",
            "Total elapsed time: 02h 21m 33s\n",
            "Search space summary\n",
            "Default search space size: 4\n",
            "units_0 (Choice)\n",
            "{'default': 64, 'conditions': [], 'values': [64, 128, 256, 512, 1024], 'ordered': True}\n",
            "units_1 (Choice)\n",
            "{'default': 64, 'conditions': [], 'values': [64, 128, 256, 512, 1024], 'ordered': True}\n",
            "units_2 (Choice)\n",
            "{'default': 64, 'conditions': [], 'values': [64, 128, 256, 512, 1024], 'ordered': True}\n",
            "units_3 (Choice)\n",
            "{'default': 64, 'conditions': [], 'values': [64, 128, 256, 512, 1024], 'ordered': True}\n",
            "[64, 1024, 64, 512]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras_tuner\n",
        "# from keras_tuner.tuners import RandomSearch\n",
        "# from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import cvnn.layers as complex_layers\n",
        "\n",
        "# Define a function to build the model with variable neurons\n",
        "def build_model(hp):\n",
        "    model_complex = tf.keras.models.Sequential()\n",
        "    model_complex.add(complex_layers.ComplexInput(input_shape=(4004,), dtype=tf.complex64))\n",
        "\n",
        "    # Define the number of neurons for each layer as a hyperparameter\n",
        "    # units = [hp.Int(f'units_{i}', min_value=8, max_value=512, step=8, sampling=\"linear\") for i in range(4)]\n",
        "    units = [hp.Choice(f'units_{i}', [64, 128, 256, 512, 1024]) for i in range(4)]\n",
        "\n",
        "    print(units)\n",
        "\n",
        "    # Add ComplexDense layers with variable number of neurons\n",
        "    for i, num_units in enumerate(units):\n",
        "        model_complex.add(complex_layers.ComplexDense(units=num_units, activation='cart_leaky_relu'))\n",
        "\n",
        "    model_complex.add(complex_layers.ComplexDense(units=32, activation='cart_leaky_relu'))\n",
        "    # Add ComplexToReal and Dense output layer\n",
        "    model_complex.add(ComplexToReal())\n",
        "    model_complex.add(Dense(units=3, activation='linear'))\n",
        "\n",
        "    # Compile the model\n",
        "    model_complex.compile(loss='mean_squared_error', optimizer=Adam(), metrics=['mean_squared_error'])\n",
        "\n",
        "    # model_complex.summary()\n",
        "\n",
        "    return model_complex\n",
        "\n",
        "# Initialize Hyperparameters object\n",
        "hp = keras_tuner.HyperParameters()\n",
        "\n",
        "# hp.Float(\n",
        "#     \"learning_rate\",\n",
        "#     min_value=0.0001,\n",
        "#     max_value=1,\n",
        "#     step=10,\n",
        "#     sampling=\"log\")\n",
        "\n",
        "build_model(hp)\n",
        "\n",
        "# Define the tuner\n",
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel = build_model,\n",
        "    objective=keras_tuner.Objective(\"val_loss\", direction=\"min\"),\n",
        "    max_trials=50,  # Adjust the number of trials as needed\n",
        "    tune_new_entries=False,\n",
        "    loss=\"mean_squared_error\",\n",
        "    metrics=[\"mean_squared_error\"],\n",
        "    executions_per_trial=1,\n",
        "    hyperparameters=hp,\n",
        "    overwrite=True,\n",
        "    directory='nas_logs',\n",
        "    project_name='complex_model_nas'\n",
        ")\n",
        "\n",
        "tuner.search(X_train, y_train, batch_size = batch_size, validation_data=(X_test, y_test), epochs=epochs, callbacks=[lr_scheduler])  # Adjust epochs as needed\n",
        "# Perform the search\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Get the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG5vQjp3ir8d",
        "outputId": "c1fccd6e-63e1-4e7d-87d4-3f1334281a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " complex_dense (ComplexDens  (None, 64)                512640    \n",
            " e)                                                              \n",
            "                                                                 \n",
            " complex_dense_1 (ComplexDe  (None, 1024)              133120    \n",
            " nse)                                                            \n",
            "                                                                 \n",
            " complex_dense_2 (ComplexDe  (None, 64)                131200    \n",
            " nse)                                                            \n",
            "                                                                 \n",
            " complex_dense_3 (ComplexDe  (None, 512)               66560     \n",
            " nse)                                                            \n",
            "                                                                 \n",
            " complex_dense_4 (ComplexDe  (None, 32)                32832     \n",
            " nse)                                                            \n",
            "                                                                 \n",
            " complex_to_real (ComplexTo  (None, 64)                0         \n",
            " Real)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 876547 (3.34 MB)\n",
            "Trainable params: 876547 (3.34 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree(\"/content/nas_logs\")"
      ],
      "metadata": {
        "id": "Ri1wChp_88A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complex-valued 3D CNN"
      ],
      "metadata": {
        "id": "agOfTTcaDE9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(width=128, height=128, depth=64):\n",
        "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
        "\n",
        "    inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    # Define the model.\n",
        "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "8u203-FFDHmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "May be graphs: no need since of the absence of dynamic structure/topology"
      ],
      "metadata": {
        "id": "qjEDSYhaFZSM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwnsXVH0D43c"
      },
      "source": [
        "# contrastive approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC_NDd7JFwgo",
        "outputId": "10f94248-9a34-4baa-b89d-6e7c34911997"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "129"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV2BbmPMFiqA"
      },
      "outputs": [],
      "source": [
        "epochs = 500\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1cu1bXgD7L7",
        "outputId": "075dba00-aede-4d01-ea47-89ea4d0445dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(396, 1)\n",
            "(396, 4)\n",
            "(129, 1)\n",
            "(129, 4)\n",
            "<_PrefetchDataset element_spec=({'original': TensorSpec(shape=(None, 4), dtype=tf.float64, name=None), 'noisy': TensorSpec(shape=(None, 4), dtype=tf.float64, name=None)}, TensorSpec(shape=(None,), dtype=tf.float64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "# custom data loader function\n",
        "def custom_data_loader(features, labels, batch_size, noise_stddev=1, shuffle=True):\n",
        "\n",
        "    noisy_freq = features[:,2] + noise_stddev * tf.random.normal(features[:, 2].shape)\n",
        "\n",
        "    noisy_freq = noisy_freq[:, np.newaxis]\n",
        "    print(noisy_freq.shape)\n",
        "\n",
        "    # noisy_phase = features[:,2] + noise_stddev * tf.random.normal(features[:, 2].shape)\n",
        "    # noisy_phase = noisy_phase[:, np.newaxis]\n",
        "    # print(noisy_phase.shape)\n",
        "    # print(features[:, 1].shape)\n",
        "\n",
        "    features_noisy = np.hstack((features[:, 0:2], noisy_freq, features[:, 3][:, np.newaxis]))\n",
        "    # features_noisy = np.hstack((noisy_freq, features[:, 1][:, np.newaxis], noisy_phase))\n",
        "\n",
        "    print(features_noisy.shape)\n",
        "\n",
        "    # print(features_noisy[0])\n",
        "    # print(features[0])\n",
        "    # print(labels.shape)\n",
        "\n",
        "    features_dic = {'original': features, 'noisy': features_noisy}\n",
        "\n",
        "    # Create a dataset from features and labels\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features_dic, labels))\n",
        "\n",
        "    # Shuffle and repeat the dataset if needed\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(features))\n",
        "    dataset = dataset.repeat()\n",
        "\n",
        "    # Batch the dataset\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=False)\n",
        "\n",
        "    # Prefetch the data for better performance\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "train_dataset = custom_data_loader(X_train, y_train, batch_size, noise_stddev=1)\n",
        "val_dataset = custom_data_loader(X_test, y_test, batch_size, noise_stddev=1, shuffle=False)\n",
        "\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61D3hWJb6Gbh"
      },
      "source": [
        "# Dual contrastive network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYHzcXIBFeU8",
        "outputId": "8319438a-e550-479f-b204-ffc71afe6b4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " original (InputLayer)       [(None, 3)]                  0         []                            \n",
            "                                                                                                  \n",
            " noisy (InputLayer)          [(None, 3)]                  0         []                            \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 64)                   256       ['original[0][0]']            \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 64)                   256       ['noisy[0][0]']               \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 128)                  8320      ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 128)                  8320      ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 512)                  66048     ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 512)                  66048     ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 128)                  65664     ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " dense_16 (Dense)            (None, 128)                  65664     ['dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 64)                   8256      ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " dense_17 (Dense)            (None, 64)                   8256      ['dense_16[0][0]']            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 32)                   2080      ['dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " dense_18 (Dense)            (None, 32)                   2080      ['dense_17[0][0]']            \n",
            "                                                                                                  \n",
            " output_1 (Dense)            (None, 1)                    33        ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " output_2 (Dense)            (None, 1)                    33        ['dense_18[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 301314 (1.15 MB)\n",
            "Trainable params: 301314 (1.15 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 32.4980 - output_1_loss: 32.4841 - output_2_loss: 0.0028\n",
            "Epoch 1: val_loss improved from inf to 1.48363, saving model to best_model.h5\n",
            "52/52 [==============================] - 5s 15ms/step - loss: 29.0117 - output_1_loss: 28.9983 - output_2_loss: 0.0025 - val_loss: 1.4836 - val_output_1_loss: 1.4744 - val_output_2_loss: 1.9700e-05 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "13/52 [======>.......................] - ETA: 0s - loss: 1.0453 - output_1_loss: 1.0364 - output_2_loss: 6.1299e-05"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51/52 [============================>.] - ETA: 0s - loss: 0.3766 - output_1_loss: 0.3685 - output_2_loss: 6.8247e-05\n",
            "Epoch 2: val_loss improved from 1.48363 to 0.12016, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.3703 - output_1_loss: 0.3622 - output_2_loss: 6.8347e-05 - val_loss: 0.1202 - val_output_1_loss: 0.1131 - val_output_2_loss: 2.4535e-05 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0465 - output_1_loss: 0.0399 - output_2_loss: 7.9966e-05\n",
            "Epoch 3: val_loss improved from 0.12016 to 0.08829, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0451 - output_1_loss: 0.0386 - output_2_loss: 7.7997e-05 - val_loss: 0.0883 - val_output_1_loss: 0.0822 - val_output_2_loss: 2.2656e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0419 - output_1_loss: 0.0359 - output_2_loss: 6.6859e-05\n",
            "Epoch 4: val_loss improved from 0.08829 to 0.07885, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0414 - output_1_loss: 0.0355 - output_2_loss: 6.6975e-05 - val_loss: 0.0788 - val_output_1_loss: 0.0731 - val_output_2_loss: 2.0807e-05 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0405 - output_1_loss: 0.0348 - output_2_loss: 6.1168e-05\n",
            "Epoch 5: val_loss improved from 0.07885 to 0.07716, saving model to best_model.h5\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0401 - output_1_loss: 0.0345 - output_2_loss: 6.1164e-05 - val_loss: 0.0772 - val_output_1_loss: 0.0717 - val_output_2_loss: 2.0029e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0401 - output_1_loss: 0.0346 - output_2_loss: 5.6156e-05\n",
            "Epoch 6: val_loss did not improve from 0.07716\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0401 - output_1_loss: 0.0346 - output_2_loss: 5.6156e-05 - val_loss: 0.0791 - val_output_1_loss: 0.0738 - val_output_2_loss: 1.9487e-05 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0423 - output_1_loss: 0.0369 - output_2_loss: 5.3795e-05\n",
            "Epoch 7: val_loss did not improve from 0.07716\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0423 - output_1_loss: 0.0369 - output_2_loss: 5.3795e-05 - val_loss: 0.0807 - val_output_1_loss: 0.0754 - val_output_2_loss: 1.8599e-05 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0407 - output_1_loss: 0.0354 - output_2_loss: 5.0076e-05\n",
            "Epoch 8: val_loss improved from 0.07716 to 0.07466, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0420 - output_1_loss: 0.0367 - output_2_loss: 5.0201e-05 - val_loss: 0.0747 - val_output_1_loss: 0.0694 - val_output_2_loss: 1.9072e-05 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0425 - output_1_loss: 0.0373 - output_2_loss: 4.8071e-05\n",
            "Epoch 9: val_loss improved from 0.07466 to 0.07063, saving model to best_model.h5\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0421 - output_1_loss: 0.0369 - output_2_loss: 4.8021e-05 - val_loss: 0.0706 - val_output_1_loss: 0.0655 - val_output_2_loss: 1.8666e-05 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0503 - output_1_loss: 0.0452 - output_2_loss: 4.5066e-05\n",
            "Epoch 10: val_loss did not improve from 0.07063\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0497 - output_1_loss: 0.0445 - output_2_loss: 4.5191e-05 - val_loss: 0.1005 - val_output_1_loss: 0.0953 - val_output_2_loss: 1.8630e-05 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0328 - output_1_loss: 0.0277 - output_2_loss: 4.2993e-05\n",
            "Epoch 11: val_loss did not improve from 0.07063\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0364 - output_1_loss: 0.0312 - output_2_loss: 4.2709e-05 - val_loss: 0.0715 - val_output_1_loss: 0.0665 - val_output_2_loss: 1.9042e-05 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0403 - output_1_loss: 0.0352 - output_2_loss: 4.1371e-05\n",
            "Epoch 12: val_loss did not improve from 0.07063\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0400 - output_1_loss: 0.0349 - output_2_loss: 4.1363e-05 - val_loss: 0.0918 - val_output_1_loss: 0.0868 - val_output_2_loss: 1.9609e-05 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0426 - output_1_loss: 0.0375 - output_2_loss: 3.9677e-05\n",
            "Epoch 13: val_loss did not improve from 0.07063\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0418 - output_1_loss: 0.0368 - output_2_loss: 3.9658e-05 - val_loss: 0.0717 - val_output_1_loss: 0.0667 - val_output_2_loss: 1.9885e-05 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0368 - output_1_loss: 0.0317 - output_2_loss: 3.8383e-05\n",
            "Epoch 14: val_loss did not improve from 0.07063\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0376 - output_1_loss: 0.0325 - output_2_loss: 3.8243e-05 - val_loss: 0.0751 - val_output_1_loss: 0.0701 - val_output_2_loss: 2.0496e-05 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0672 - output_1_loss: 0.0622 - output_2_loss: 3.6716e-05\n",
            "Epoch 15: val_loss did not improve from 0.07063\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0649 - output_1_loss: 0.0599 - output_2_loss: 3.6623e-05 - val_loss: 0.1209 - val_output_1_loss: 0.1159 - val_output_2_loss: 2.1397e-05 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0386 - output_1_loss: 0.0337 - output_2_loss: 3.5429e-05\n",
            "Epoch 16: val_loss improved from 0.07063 to 0.06884, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0386 - output_1_loss: 0.0337 - output_2_loss: 3.5429e-05 - val_loss: 0.0688 - val_output_1_loss: 0.0639 - val_output_2_loss: 2.2118e-05 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "45/52 [========================>.....] - ETA: 0s - loss: 0.0571 - output_1_loss: 0.0522 - output_2_loss: 3.4385e-05\n",
            "Epoch 17: val_loss did not improve from 0.06884\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0594 - output_1_loss: 0.0545 - output_2_loss: 3.4270e-05 - val_loss: 0.0837 - val_output_1_loss: 0.0788 - val_output_2_loss: 2.2878e-05 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "45/52 [========================>.....] - ETA: 0s - loss: 0.0394 - output_1_loss: 0.0345 - output_2_loss: 3.3218e-05\n",
            "Epoch 18: val_loss did not improve from 0.06884\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0373 - output_1_loss: 0.0324 - output_2_loss: 3.3174e-05 - val_loss: 0.0716 - val_output_1_loss: 0.0667 - val_output_2_loss: 2.3430e-05 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0411 - output_1_loss: 0.0362 - output_2_loss: 3.2110e-05\n",
            "Epoch 19: val_loss did not improve from 0.06884\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0411 - output_1_loss: 0.0362 - output_2_loss: 3.2110e-05 - val_loss: 0.0901 - val_output_1_loss: 0.0852 - val_output_2_loss: 2.4171e-05 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0517 - output_1_loss: 0.0469 - output_2_loss: 3.1220e-05\n",
            "Epoch 20: val_loss did not improve from 0.06884\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0508 - output_1_loss: 0.0459 - output_2_loss: 3.1197e-05 - val_loss: 0.0853 - val_output_1_loss: 0.0805 - val_output_2_loss: 2.4572e-05 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0421 - output_1_loss: 0.0373 - output_2_loss: 3.0300e-05\n",
            "Epoch 21: val_loss did not improve from 0.06884\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0421 - output_1_loss: 0.0373 - output_2_loss: 3.0300e-05 - val_loss: 0.0852 - val_output_1_loss: 0.0804 - val_output_2_loss: 2.5073e-05 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0462 - output_1_loss: 0.0414 - output_2_loss: 2.9533e-05\n",
            "Epoch 22: val_loss did not improve from 0.06884\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0462 - output_1_loss: 0.0414 - output_2_loss: 2.9533e-05 - val_loss: 0.0908 - val_output_1_loss: 0.0860 - val_output_2_loss: 2.5347e-05 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0388 - output_1_loss: 0.0340 - output_2_loss: 2.8797e-05\n",
            "Epoch 23: val_loss did not improve from 0.06884\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0393 - output_1_loss: 0.0345 - output_2_loss: 2.8780e-05 - val_loss: 0.1394 - val_output_1_loss: 0.1346 - val_output_2_loss: 2.5500e-05 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.1075 - output_1_loss: 0.1027 - output_2_loss: 2.8125e-05\n",
            "Epoch 24: val_loss did not improve from 0.06884\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.1059 - output_1_loss: 0.1011 - output_2_loss: 2.8113e-05 - val_loss: 0.0983 - val_output_1_loss: 0.0935 - val_output_2_loss: 2.5554e-05 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0464 - output_1_loss: 0.0416 - output_2_loss: 2.7509e-05\n",
            "Epoch 25: val_loss did not improve from 0.06884\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0447 - output_1_loss: 0.0400 - output_2_loss: 2.7496e-05 - val_loss: 0.0718 - val_output_1_loss: 0.0670 - val_output_2_loss: 2.5502e-05 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0292 - output_1_loss: 0.0244 - output_2_loss: 2.6899e-05\n",
            "Epoch 26: val_loss did not improve from 0.06884\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0292 - output_1_loss: 0.0244 - output_2_loss: 2.6899e-05 - val_loss: 0.0700 - val_output_1_loss: 0.0653 - val_output_2_loss: 2.5318e-05 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0410 - output_1_loss: 0.0363 - output_2_loss: 2.6363e-05\n",
            "Epoch 27: val_loss did not improve from 0.06884\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0399 - output_1_loss: 0.0352 - output_2_loss: 2.6347e-05 - val_loss: 0.0696 - val_output_1_loss: 0.0649 - val_output_2_loss: 2.5115e-05 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0425 - output_1_loss: 0.0379 - output_2_loss: 2.5852e-05\n",
            "Epoch 28: val_loss improved from 0.06884 to 0.06837, saving model to best_model.h5\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0424 - output_1_loss: 0.0377 - output_2_loss: 2.5822e-05 - val_loss: 0.0684 - val_output_1_loss: 0.0637 - val_output_2_loss: 2.4849e-05 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0455 - output_1_loss: 0.0409 - output_2_loss: 2.5328e-05\n",
            "Epoch 29: val_loss did not improve from 0.06837\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0450 - output_1_loss: 0.0404 - output_2_loss: 2.5325e-05 - val_loss: 0.0893 - val_output_1_loss: 0.0846 - val_output_2_loss: 2.4531e-05 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0418 - output_1_loss: 0.0372 - output_2_loss: 2.4859e-05\n",
            "Epoch 30: val_loss did not improve from 0.06837\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0409 - output_1_loss: 0.0362 - output_2_loss: 2.4844e-05 - val_loss: 0.0890 - val_output_1_loss: 0.0844 - val_output_2_loss: 2.4210e-05 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0406 - output_1_loss: 0.0360 - output_2_loss: 2.4392e-05\n",
            "Epoch 31: val_loss did not improve from 0.06837\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0401 - output_1_loss: 0.0355 - output_2_loss: 2.4388e-05 - val_loss: 0.0761 - val_output_1_loss: 0.0714 - val_output_2_loss: 2.3892e-05 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0473 - output_1_loss: 0.0427 - output_2_loss: 2.3978e-05\n",
            "Epoch 32: val_loss did not improve from 0.06837\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0466 - output_1_loss: 0.0420 - output_2_loss: 2.3961e-05 - val_loss: 0.1396 - val_output_1_loss: 0.1350 - val_output_2_loss: 2.3496e-05 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0391 - output_1_loss: 0.0345 - output_2_loss: 2.3560e-05\n",
            "Epoch 33: val_loss did not improve from 0.06837\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0427 - output_1_loss: 0.0381 - output_2_loss: 2.3538e-05 - val_loss: 0.1032 - val_output_1_loss: 0.0986 - val_output_2_loss: 2.3169e-05 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "44/52 [========================>.....] - ETA: 0s - loss: 0.0486 - output_1_loss: 0.0440 - output_2_loss: 2.3182e-05\n",
            "Epoch 34: val_loss did not improve from 0.06837\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0451 - output_1_loss: 0.0405 - output_2_loss: 2.3153e-05 - val_loss: 0.0732 - val_output_1_loss: 0.0686 - val_output_2_loss: 2.2824e-05 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0522 - output_1_loss: 0.0477 - output_2_loss: 2.2769e-05\n",
            "Epoch 35: val_loss improved from 0.06837 to 0.06733, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0522 - output_1_loss: 0.0477 - output_2_loss: 2.2769e-05 - val_loss: 0.0673 - val_output_1_loss: 0.0628 - val_output_2_loss: 2.2502e-05 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0539 - output_1_loss: 0.0494 - output_2_loss: 2.2435e-05\n",
            "Epoch 36: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0525 - output_1_loss: 0.0480 - output_2_loss: 2.2413e-05 - val_loss: 0.0969 - val_output_1_loss: 0.0923 - val_output_2_loss: 2.2133e-05 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0545 - output_1_loss: 0.0499 - output_2_loss: 2.2084e-05\n",
            "Epoch 37: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0553 - output_1_loss: 0.0508 - output_2_loss: 2.2066e-05 - val_loss: 0.0859 - val_output_1_loss: 0.0814 - val_output_2_loss: 2.1863e-05 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0469 - output_1_loss: 0.0424 - output_2_loss: 2.1746e-05\n",
            "Epoch 38: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0513 - output_1_loss: 0.0468 - output_2_loss: 2.1739e-05 - val_loss: 0.0697 - val_output_1_loss: 0.0652 - val_output_2_loss: 2.1496e-05 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0442 - output_1_loss: 0.0397 - output_2_loss: 2.1415e-05\n",
            "Epoch 39: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0442 - output_1_loss: 0.0397 - output_2_loss: 2.1415e-05 - val_loss: 0.0736 - val_output_1_loss: 0.0691 - val_output_2_loss: 2.1212e-05 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0282 - output_1_loss: 0.0237 - output_2_loss: 2.1131e-05\n",
            "Epoch 40: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0305 - output_1_loss: 0.0261 - output_2_loss: 2.1110e-05 - val_loss: 0.1089 - val_output_1_loss: 0.1045 - val_output_2_loss: 2.0933e-05 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0622 - output_1_loss: 0.0577 - output_2_loss: 2.0818e-05\n",
            "Epoch 41: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0622 - output_1_loss: 0.0577 - output_2_loss: 2.0818e-05 - val_loss: 0.1108 - val_output_1_loss: 0.1063 - val_output_2_loss: 2.0637e-05 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0588 - output_1_loss: 0.0543 - output_2_loss: 2.0551e-05\n",
            "Epoch 42: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0538 - output_1_loss: 0.0494 - output_2_loss: 2.0537e-05 - val_loss: 0.0869 - val_output_1_loss: 0.0824 - val_output_2_loss: 2.0366e-05 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0416 - output_1_loss: 0.0372 - output_2_loss: 2.0279e-05\n",
            "Epoch 43: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0392 - output_1_loss: 0.0347 - output_2_loss: 2.0262e-05 - val_loss: 0.0914 - val_output_1_loss: 0.0870 - val_output_2_loss: 2.0127e-05 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0650 - output_1_loss: 0.0606 - output_2_loss: 2.0009e-05\n",
            "Epoch 44: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0645 - output_1_loss: 0.0600 - output_2_loss: 2.0006e-05 - val_loss: 0.0689 - val_output_1_loss: 0.0645 - val_output_2_loss: 1.9849e-05 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0505 - output_1_loss: 0.0461 - output_2_loss: 1.9756e-05\n",
            "Epoch 45: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0497 - output_1_loss: 0.0453 - output_2_loss: 1.9753e-05 - val_loss: 0.1038 - val_output_1_loss: 0.0994 - val_output_2_loss: 1.9606e-05 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0386 - output_1_loss: 0.0342 - output_2_loss: 1.9517e-05\n",
            "Epoch 46: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0386 - output_1_loss: 0.0342 - output_2_loss: 1.9517e-05 - val_loss: 0.0687 - val_output_1_loss: 0.0643 - val_output_2_loss: 1.9376e-05 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0622 - output_1_loss: 0.0579 - output_2_loss: 1.9281e-05\n",
            "Epoch 47: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0614 - output_1_loss: 0.0570 - output_2_loss: 1.9279e-05 - val_loss: 0.0860 - val_output_1_loss: 0.0816 - val_output_2_loss: 1.9167e-05 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0490 - output_1_loss: 0.0447 - output_2_loss: 1.9069e-05\n",
            "Epoch 48: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0500 - output_1_loss: 0.0457 - output_2_loss: 1.9063e-05 - val_loss: 0.1539 - val_output_1_loss: 0.1495 - val_output_2_loss: 1.8933e-05 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.1155 - output_1_loss: 0.1111 - output_2_loss: 1.8848e-05\n",
            "Epoch 49: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.1147 - output_1_loss: 0.1104 - output_2_loss: 1.8847e-05 - val_loss: 0.1954 - val_output_1_loss: 0.1911 - val_output_2_loss: 1.8746e-05 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0806 - output_1_loss: 0.0762 - output_2_loss: 1.8639e-05\n",
            "Epoch 50: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0806 - output_1_loss: 0.0762 - output_2_loss: 1.8639e-05 - val_loss: 0.0822 - val_output_1_loss: 0.0778 - val_output_2_loss: 1.8539e-05 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0489 - output_1_loss: 0.0446 - output_2_loss: 1.8444e-05\n",
            "Epoch 51: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0487 - output_1_loss: 0.0444 - output_2_loss: 1.8439e-05 - val_loss: 0.1020 - val_output_1_loss: 0.0977 - val_output_2_loss: 1.8332e-05 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0786 - output_1_loss: 0.0743 - output_2_loss: 1.8249e-05\n",
            "Epoch 52: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0775 - output_1_loss: 0.0732 - output_2_loss: 1.8245e-05 - val_loss: 0.1636 - val_output_1_loss: 0.1593 - val_output_2_loss: 1.8155e-05 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0534 - output_1_loss: 0.0492 - output_2_loss: 1.8064e-05\n",
            "Epoch 53: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0526 - output_1_loss: 0.0483 - output_2_loss: 1.8060e-05 - val_loss: 0.0784 - val_output_1_loss: 0.0741 - val_output_2_loss: 1.7961e-05 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0457 - output_1_loss: 0.0414 - output_2_loss: 1.7879e-05\n",
            "Epoch 54: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0454 - output_1_loss: 0.0411 - output_2_loss: 1.7877e-05 - val_loss: 0.0710 - val_output_1_loss: 0.0668 - val_output_2_loss: 1.7786e-05 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0335 - output_1_loss: 0.0292 - output_2_loss: 1.7707e-05\n",
            "Epoch 55: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0330 - output_1_loss: 0.0287 - output_2_loss: 1.7704e-05 - val_loss: 0.0747 - val_output_1_loss: 0.0704 - val_output_2_loss: 1.7616e-05 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0579 - output_1_loss: 0.0537 - output_2_loss: 1.7536e-05\n",
            "Epoch 56: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0601 - output_1_loss: 0.0558 - output_2_loss: 1.7534e-05 - val_loss: 0.1005 - val_output_1_loss: 0.0962 - val_output_2_loss: 1.7455e-05 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0598 - output_1_loss: 0.0556 - output_2_loss: 1.7370e-05\n",
            "Epoch 57: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0585 - output_1_loss: 0.0543 - output_2_loss: 1.7364e-05 - val_loss: 0.0840 - val_output_1_loss: 0.0798 - val_output_2_loss: 1.7304e-05 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0409 - output_1_loss: 0.0366 - output_2_loss: 1.7211e-05\n",
            "Epoch 58: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0392 - output_1_loss: 0.0350 - output_2_loss: 1.7203e-05 - val_loss: 0.0719 - val_output_1_loss: 0.0677 - val_output_2_loss: 1.7112e-05 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0503 - output_1_loss: 0.0461 - output_2_loss: 1.7050e-05\n",
            "Epoch 59: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0496 - output_1_loss: 0.0454 - output_2_loss: 1.7048e-05 - val_loss: 0.0712 - val_output_1_loss: 0.0670 - val_output_2_loss: 1.6958e-05 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0389 - output_1_loss: 0.0347 - output_2_loss: 1.6897e-05\n",
            "Epoch 60: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0379 - output_1_loss: 0.0337 - output_2_loss: 1.6889e-05 - val_loss: 0.0732 - val_output_1_loss: 0.0690 - val_output_2_loss: 1.6786e-05 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0622 - output_1_loss: 0.0580 - output_2_loss: 1.6738e-05\n",
            "Epoch 61: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0622 - output_1_loss: 0.0580 - output_2_loss: 1.6738e-05 - val_loss: 0.1127 - val_output_1_loss: 0.1085 - val_output_2_loss: 1.6667e-05 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0330 - output_1_loss: 0.0288 - output_2_loss: 1.6592e-05\n",
            "Epoch 62: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0326 - output_1_loss: 0.0285 - output_2_loss: 1.6591e-05 - val_loss: 0.0691 - val_output_1_loss: 0.0649 - val_output_2_loss: 1.6527e-05 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0600 - output_1_loss: 0.0559 - output_2_loss: 1.6455e-05\n",
            "Epoch 63: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0574 - output_1_loss: 0.0533 - output_2_loss: 1.6450e-05 - val_loss: 0.0992 - val_output_1_loss: 0.0950 - val_output_2_loss: 1.6363e-05 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0474 - output_1_loss: 0.0432 - output_2_loss: 1.6300e-05\n",
            "Epoch 64: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0474 - output_1_loss: 0.0432 - output_2_loss: 1.6300e-05 - val_loss: 0.0884 - val_output_1_loss: 0.0842 - val_output_2_loss: 1.6238e-05 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0486 - output_1_loss: 0.0444 - output_2_loss: 1.6165e-05\n",
            "Epoch 65: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0481 - output_1_loss: 0.0440 - output_2_loss: 1.6165e-05 - val_loss: 0.0904 - val_output_1_loss: 0.0863 - val_output_2_loss: 1.6085e-05 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0318 - output_1_loss: 0.0277 - output_2_loss: 1.6028e-05\n",
            "Epoch 66: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0326 - output_1_loss: 0.0285 - output_2_loss: 1.6025e-05 - val_loss: 0.0776 - val_output_1_loss: 0.0735 - val_output_2_loss: 1.5947e-05 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0421 - output_1_loss: 0.0380 - output_2_loss: 1.5894e-05\n",
            "Epoch 67: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0444 - output_1_loss: 0.0403 - output_2_loss: 1.5890e-05 - val_loss: 0.2500 - val_output_1_loss: 0.2459 - val_output_2_loss: 1.5823e-05 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0633 - output_1_loss: 0.0592 - output_2_loss: 1.5759e-05\n",
            "Epoch 68: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0637 - output_1_loss: 0.0596 - output_2_loss: 1.5758e-05 - val_loss: 0.0714 - val_output_1_loss: 0.0673 - val_output_2_loss: 1.5686e-05 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0329 - output_1_loss: 0.0289 - output_2_loss: 1.5628e-05\n",
            "Epoch 69: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 10ms/step - loss: 0.0346 - output_1_loss: 0.0306 - output_2_loss: 1.5625e-05 - val_loss: 0.1431 - val_output_1_loss: 0.1391 - val_output_2_loss: 1.5570e-05 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0592 - output_1_loss: 0.0552 - output_2_loss: 1.5499e-05\n",
            "Epoch 70: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0594 - output_1_loss: 0.0554 - output_2_loss: 1.5498e-05 - val_loss: 0.0909 - val_output_1_loss: 0.0869 - val_output_2_loss: 1.5414e-05 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0617 - output_1_loss: 0.0577 - output_2_loss: 1.5367e-05\n",
            "Epoch 71: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0617 - output_1_loss: 0.0577 - output_2_loss: 1.5367e-05 - val_loss: 0.1076 - val_output_1_loss: 0.1036 - val_output_2_loss: 1.5303e-05 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0310 - output_1_loss: 0.0270 - output_2_loss: 1.5246e-05\n",
            "Epoch 72: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0330 - output_1_loss: 0.0290 - output_2_loss: 1.5245e-05 - val_loss: 0.0769 - val_output_1_loss: 0.0729 - val_output_2_loss: 1.5210e-05 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0377 - output_1_loss: 0.0336 - output_2_loss: 1.5124e-05\n",
            "Epoch 73: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0384 - output_1_loss: 0.0344 - output_2_loss: 1.5121e-05 - val_loss: 0.0870 - val_output_1_loss: 0.0830 - val_output_2_loss: 1.5072e-05 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0531 - output_1_loss: 0.0491 - output_2_loss: 1.5001e-05\n",
            "Epoch 74: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0527 - output_1_loss: 0.0487 - output_2_loss: 1.5000e-05 - val_loss: 0.0776 - val_output_1_loss: 0.0736 - val_output_2_loss: 1.4931e-05 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0530 - output_1_loss: 0.0490 - output_2_loss: 1.4884e-05\n",
            "Epoch 75: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0559 - output_1_loss: 0.0520 - output_2_loss: 1.4882e-05 - val_loss: 0.1003 - val_output_1_loss: 0.0963 - val_output_2_loss: 1.4820e-05 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0605 - output_1_loss: 0.0565 - output_2_loss: 1.4764e-05\n",
            "Epoch 76: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0605 - output_1_loss: 0.0565 - output_2_loss: 1.4764e-05 - val_loss: 0.1515 - val_output_1_loss: 0.1476 - val_output_2_loss: 1.4713e-05 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0346 - output_1_loss: 0.0307 - output_2_loss: 1.4649e-05\n",
            "Epoch 77: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0346 - output_1_loss: 0.0306 - output_2_loss: 1.4649e-05 - val_loss: 0.0742 - val_output_1_loss: 0.0702 - val_output_2_loss: 1.4610e-05 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0395 - output_1_loss: 0.0355 - output_2_loss: 1.4544e-05\n",
            "Epoch 78: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0373 - output_1_loss: 0.0334 - output_2_loss: 1.4537e-05 - val_loss: 0.0675 - val_output_1_loss: 0.0636 - val_output_2_loss: 1.4468e-05 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0464 - output_1_loss: 0.0425 - output_2_loss: 1.4429e-05\n",
            "Epoch 79: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0455 - output_1_loss: 0.0416 - output_2_loss: 1.4424e-05 - val_loss: 0.0735 - val_output_1_loss: 0.0696 - val_output_2_loss: 1.4380e-05 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0588 - output_1_loss: 0.0549 - output_2_loss: 1.4322e-05\n",
            "Epoch 80: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0581 - output_1_loss: 0.0542 - output_2_loss: 1.4320e-05 - val_loss: 0.1093 - val_output_1_loss: 0.1054 - val_output_2_loss: 1.4278e-05 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0459 - output_1_loss: 0.0420 - output_2_loss: 1.4213e-05\n",
            "Epoch 81: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0460 - output_1_loss: 0.0421 - output_2_loss: 1.4211e-05 - val_loss: 0.0836 - val_output_1_loss: 0.0797 - val_output_2_loss: 1.4164e-05 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0368 - output_1_loss: 0.0329 - output_2_loss: 1.4106e-05\n",
            "Epoch 82: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0368 - output_1_loss: 0.0329 - output_2_loss: 1.4106e-05 - val_loss: 0.0791 - val_output_1_loss: 0.0752 - val_output_2_loss: 1.4048e-05 - lr: 0.0010\n",
            "Epoch 83/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0374 - output_1_loss: 0.0335 - output_2_loss: 1.4005e-05\n",
            "Epoch 83: val_loss did not improve from 0.06733\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0397 - output_1_loss: 0.0359 - output_2_loss: 1.4005e-05 - val_loss: 0.0851 - val_output_1_loss: 0.0812 - val_output_2_loss: 1.3966e-05 - lr: 0.0010\n",
            "Epoch 84/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0411 - output_1_loss: 0.0372 - output_2_loss: 1.3910e-05\n",
            "Epoch 84: val_loss improved from 0.06733 to 0.06607, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0395 - output_1_loss: 0.0356 - output_2_loss: 1.3902e-05 - val_loss: 0.0661 - val_output_1_loss: 0.0622 - val_output_2_loss: 1.3824e-05 - lr: 0.0010\n",
            "Epoch 85/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0448 - output_1_loss: 0.0410 - output_2_loss: 1.3805e-05\n",
            "Epoch 85: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0442 - output_1_loss: 0.0403 - output_2_loss: 1.3805e-05 - val_loss: 0.0701 - val_output_1_loss: 0.0663 - val_output_2_loss: 1.3754e-05 - lr: 0.0010\n",
            "Epoch 86/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0492 - output_1_loss: 0.0454 - output_2_loss: 1.3706e-05\n",
            "Epoch 86: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0492 - output_1_loss: 0.0454 - output_2_loss: 1.3706e-05 - val_loss: 0.0753 - val_output_1_loss: 0.0714 - val_output_2_loss: 1.3666e-05 - lr: 0.0010\n",
            "Epoch 87/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0735 - output_1_loss: 0.0697 - output_2_loss: 1.3613e-05\n",
            "Epoch 87: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0725 - output_1_loss: 0.0687 - output_2_loss: 1.3612e-05 - val_loss: 0.0809 - val_output_1_loss: 0.0771 - val_output_2_loss: 1.3562e-05 - lr: 0.0010\n",
            "Epoch 88/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0629 - output_1_loss: 0.0591 - output_2_loss: 1.3518e-05\n",
            "Epoch 88: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0646 - output_1_loss: 0.0608 - output_2_loss: 1.3517e-05 - val_loss: 0.1020 - val_output_1_loss: 0.0982 - val_output_2_loss: 1.3475e-05 - lr: 0.0010\n",
            "Epoch 89/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0700 - output_1_loss: 0.0663 - output_2_loss: 1.3424e-05\n",
            "Epoch 89: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0689 - output_1_loss: 0.0651 - output_2_loss: 1.3423e-05 - val_loss: 0.0706 - val_output_1_loss: 0.0668 - val_output_2_loss: 1.3382e-05 - lr: 0.0010\n",
            "Epoch 90/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0506 - output_1_loss: 0.0469 - output_2_loss: 1.3337e-05\n",
            "Epoch 90: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0493 - output_1_loss: 0.0455 - output_2_loss: 1.3335e-05 - val_loss: 0.0848 - val_output_1_loss: 0.0811 - val_output_2_loss: 1.3293e-05 - lr: 0.0010\n",
            "Epoch 91/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0357 - output_1_loss: 0.0319 - output_2_loss: 1.3245e-05\n",
            "Epoch 91: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0357 - output_1_loss: 0.0319 - output_2_loss: 1.3245e-05 - val_loss: 0.0678 - val_output_1_loss: 0.0640 - val_output_2_loss: 1.3190e-05 - lr: 0.0010\n",
            "Epoch 92/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0357 - output_1_loss: 0.0320 - output_2_loss: 1.3158e-05\n",
            "Epoch 92: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0350 - output_1_loss: 0.0312 - output_2_loss: 1.3156e-05 - val_loss: 0.0928 - val_output_1_loss: 0.0890 - val_output_2_loss: 1.3118e-05 - lr: 0.0010\n",
            "Epoch 93/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0314 - output_1_loss: 0.0277 - output_2_loss: 1.3075e-05\n",
            "Epoch 93: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.0301 - output_1_loss: 0.0263 - output_2_loss: 1.3073e-05 - val_loss: 0.0817 - val_output_1_loss: 0.0780 - val_output_2_loss: 1.3013e-05 - lr: 0.0010\n",
            "Epoch 94/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0429 - output_1_loss: 0.0392 - output_2_loss: 1.2986e-05\n",
            "Epoch 94: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0428 - output_1_loss: 0.0391 - output_2_loss: 1.2984e-05 - val_loss: 0.0732 - val_output_1_loss: 0.0695 - val_output_2_loss: 1.2933e-05 - lr: 0.0010\n",
            "Epoch 95/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0789 - output_1_loss: 0.0752 - output_2_loss: 1.2904e-05\n",
            "Epoch 95: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0775 - output_1_loss: 0.0738 - output_2_loss: 1.2902e-05 - val_loss: 0.0891 - val_output_1_loss: 0.0854 - val_output_2_loss: 1.2861e-05 - lr: 0.0010\n",
            "Epoch 96/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0376 - output_1_loss: 0.0339 - output_2_loss: 1.2821e-05\n",
            "Epoch 96: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0374 - output_1_loss: 0.0337 - output_2_loss: 1.2819e-05 - val_loss: 0.1064 - val_output_1_loss: 0.1027 - val_output_2_loss: 1.2776e-05 - lr: 0.0010\n",
            "Epoch 97/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0284 - output_1_loss: 0.0247 - output_2_loss: 1.2743e-05\n",
            "Epoch 97: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0316 - output_1_loss: 0.0279 - output_2_loss: 1.2740e-05 - val_loss: 0.0936 - val_output_1_loss: 0.0900 - val_output_2_loss: 1.2697e-05 - lr: 0.0010\n",
            "Epoch 98/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0375 - output_1_loss: 0.0339 - output_2_loss: 1.2661e-05\n",
            "Epoch 98: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0366 - output_1_loss: 0.0330 - output_2_loss: 1.2658e-05 - val_loss: 0.0860 - val_output_1_loss: 0.0823 - val_output_2_loss: 1.2621e-05 - lr: 0.0010\n",
            "Epoch 99/500\n",
            "45/52 [========================>.....] - ETA: 0s - loss: 0.0542 - output_1_loss: 0.0506 - output_2_loss: 1.2586e-05\n",
            "Epoch 99: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0503 - output_1_loss: 0.0466 - output_2_loss: 1.2581e-05 - val_loss: 0.0897 - val_output_1_loss: 0.0861 - val_output_2_loss: 1.2530e-05 - lr: 0.0010\n",
            "Epoch 100/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0448 - output_1_loss: 0.0412 - output_2_loss: 1.2504e-05\n",
            "Epoch 100: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0445 - output_1_loss: 0.0409 - output_2_loss: 1.2501e-05 - val_loss: 0.0776 - val_output_1_loss: 0.0740 - val_output_2_loss: 1.2466e-05 - lr: 0.0010\n",
            "Epoch 101/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0573 - output_1_loss: 0.0537 - output_2_loss: 1.2431e-05\n",
            "Epoch 101: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0597 - output_1_loss: 0.0561 - output_2_loss: 1.2426e-05 - val_loss: 0.1647 - val_output_1_loss: 0.1611 - val_output_2_loss: 1.2371e-05 - lr: 0.0010\n",
            "Epoch 102/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0479 - output_1_loss: 0.0443 - output_2_loss: 1.2351e-05\n",
            "Epoch 102: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0467 - output_1_loss: 0.0431 - output_2_loss: 1.2350e-05 - val_loss: 0.0688 - val_output_1_loss: 0.0652 - val_output_2_loss: 1.2311e-05 - lr: 0.0010\n",
            "Epoch 103/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0527 - output_1_loss: 0.0491 - output_2_loss: 1.2274e-05\n",
            "Epoch 103: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0527 - output_1_loss: 0.0491 - output_2_loss: 1.2274e-05 - val_loss: 0.0694 - val_output_1_loss: 0.0658 - val_output_2_loss: 1.2236e-05 - lr: 0.0010\n",
            "Epoch 104/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0380 - output_1_loss: 0.0345 - output_2_loss: 1.2203e-05\n",
            "Epoch 104: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0380 - output_1_loss: 0.0345 - output_2_loss: 1.2203e-05 - val_loss: 0.0691 - val_output_1_loss: 0.0655 - val_output_2_loss: 1.2166e-05 - lr: 0.0010\n",
            "Epoch 105/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0245 - output_1_loss: 0.0209 - output_2_loss: 1.2128e-05\n",
            "Epoch 105: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0245 - output_1_loss: 0.0209 - output_2_loss: 1.2128e-05 - val_loss: 0.1268 - val_output_1_loss: 0.1233 - val_output_2_loss: 1.2090e-05 - lr: 0.0010\n",
            "Epoch 106/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0745 - output_1_loss: 0.0710 - output_2_loss: 1.2057e-05\n",
            "Epoch 106: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0787 - output_1_loss: 0.0752 - output_2_loss: 1.2056e-05 - val_loss: 0.0805 - val_output_1_loss: 0.0769 - val_output_2_loss: 1.2018e-05 - lr: 0.0010\n",
            "Epoch 107/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0862 - output_1_loss: 0.0827 - output_2_loss: 1.1987e-05\n",
            "Epoch 107: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0839 - output_1_loss: 0.0804 - output_2_loss: 1.1986e-05 - val_loss: 0.0958 - val_output_1_loss: 0.0923 - val_output_2_loss: 1.1942e-05 - lr: 0.0010\n",
            "Epoch 108/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0423 - output_1_loss: 0.0387 - output_2_loss: 1.1915e-05\n",
            "Epoch 108: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0409 - output_1_loss: 0.0374 - output_2_loss: 1.1914e-05 - val_loss: 0.0713 - val_output_1_loss: 0.0678 - val_output_2_loss: 1.1871e-05 - lr: 0.0010\n",
            "Epoch 109/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0486 - output_1_loss: 0.0451 - output_2_loss: 1.1846e-05\n",
            "Epoch 109: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0496 - output_1_loss: 0.0461 - output_2_loss: 1.1845e-05 - val_loss: 0.1233 - val_output_1_loss: 0.1198 - val_output_2_loss: 1.1811e-05 - lr: 0.0010\n",
            "Epoch 110/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0442 - output_1_loss: 0.0408 - output_2_loss: 1.1775e-05\n",
            "Epoch 110: val_loss did not improve from 0.06607\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0442 - output_1_loss: 0.0408 - output_2_loss: 1.1775e-05 - val_loss: 0.0732 - val_output_1_loss: 0.0697 - val_output_2_loss: 1.1729e-05 - lr: 0.0010\n",
            "Epoch 111/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0274 - output_1_loss: 0.0239 - output_2_loss: 1.1712e-05\n",
            "Epoch 111: val_loss improved from 0.06607 to 0.06543, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0266 - output_1_loss: 0.0232 - output_2_loss: 1.1707e-05 - val_loss: 0.0654 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.1668e-05 - lr: 0.0010\n",
            "Epoch 112/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0494 - output_1_loss: 0.0460 - output_2_loss: 1.1642e-05\n",
            "Epoch 112: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0485 - output_1_loss: 0.0450 - output_2_loss: 1.1641e-05 - val_loss: 0.0943 - val_output_1_loss: 0.0909 - val_output_2_loss: 1.1604e-05 - lr: 0.0010\n",
            "Epoch 113/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0419 - output_1_loss: 0.0385 - output_2_loss: 1.1575e-05\n",
            "Epoch 113: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0439 - output_1_loss: 0.0405 - output_2_loss: 1.1571e-05 - val_loss: 0.0744 - val_output_1_loss: 0.0710 - val_output_2_loss: 1.1540e-05 - lr: 0.0010\n",
            "Epoch 114/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0289 - output_1_loss: 0.0255 - output_2_loss: 1.1509e-05\n",
            "Epoch 114: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0354 - output_1_loss: 0.0320 - output_2_loss: 1.1506e-05 - val_loss: 0.1079 - val_output_1_loss: 0.1045 - val_output_2_loss: 1.1473e-05 - lr: 0.0010\n",
            "Epoch 115/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0463 - output_1_loss: 0.0429 - output_2_loss: 1.1442e-05\n",
            "Epoch 115: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0449 - output_1_loss: 0.0415 - output_2_loss: 1.1439e-05 - val_loss: 0.0695 - val_output_1_loss: 0.0661 - val_output_2_loss: 1.1400e-05 - lr: 0.0010\n",
            "Epoch 116/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0501 - output_1_loss: 0.0467 - output_2_loss: 1.1377e-05\n",
            "Epoch 116: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0478 - output_1_loss: 0.0444 - output_2_loss: 1.1374e-05 - val_loss: 0.0661 - val_output_1_loss: 0.0627 - val_output_2_loss: 1.1346e-05 - lr: 0.0010\n",
            "Epoch 117/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0229 - output_1_loss: 0.0195 - output_2_loss: 1.1312e-05\n",
            "Epoch 117: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0219 - output_1_loss: 0.0185 - output_2_loss: 1.1309e-05 - val_loss: 0.0750 - val_output_1_loss: 0.0716 - val_output_2_loss: 1.1270e-05 - lr: 0.0010\n",
            "Epoch 118/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0490 - output_1_loss: 0.0457 - output_2_loss: 1.1246e-05\n",
            "Epoch 118: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0494 - output_1_loss: 0.0461 - output_2_loss: 1.1245e-05 - val_loss: 0.1029 - val_output_1_loss: 0.0995 - val_output_2_loss: 1.1208e-05 - lr: 0.0010\n",
            "Epoch 119/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0377 - output_1_loss: 0.0343 - output_2_loss: 1.1181e-05\n",
            "Epoch 119: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0401 - output_1_loss: 0.0367 - output_2_loss: 1.1179e-05 - val_loss: 0.1025 - val_output_1_loss: 0.0991 - val_output_2_loss: 1.1149e-05 - lr: 0.0010\n",
            "Epoch 120/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0283 - output_1_loss: 0.0250 - output_2_loss: 1.1117e-05\n",
            "Epoch 120: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0277 - output_1_loss: 0.0243 - output_2_loss: 1.1114e-05 - val_loss: 0.0790 - val_output_1_loss: 0.0757 - val_output_2_loss: 1.1084e-05 - lr: 0.0010\n",
            "Epoch 121/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0305 - output_1_loss: 0.0272 - output_2_loss: 1.1052e-05\n",
            "Epoch 121: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - output_1_loss: 0.0272 - output_2_loss: 1.1052e-05 - val_loss: 0.0706 - val_output_1_loss: 0.0673 - val_output_2_loss: 1.1015e-05 - lr: 0.0010\n",
            "Epoch 122/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0411 - output_1_loss: 0.0378 - output_2_loss: 1.0989e-05\n",
            "Epoch 122: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0400 - output_1_loss: 0.0367 - output_2_loss: 1.0987e-05 - val_loss: 0.0848 - val_output_1_loss: 0.0815 - val_output_2_loss: 1.0947e-05 - lr: 0.0010\n",
            "Epoch 123/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0244 - output_1_loss: 0.0211 - output_2_loss: 1.0927e-05\n",
            "Epoch 123: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0288 - output_1_loss: 0.0255 - output_2_loss: 1.0923e-05 - val_loss: 0.0709 - val_output_1_loss: 0.0676 - val_output_2_loss: 1.0892e-05 - lr: 0.0010\n",
            "Epoch 124/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0388 - output_1_loss: 0.0356 - output_2_loss: 1.0865e-05\n",
            "Epoch 124: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0383 - output_1_loss: 0.0350 - output_2_loss: 1.0862e-05 - val_loss: 0.0918 - val_output_1_loss: 0.0885 - val_output_2_loss: 1.0824e-05 - lr: 0.0010\n",
            "Epoch 125/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0561 - output_1_loss: 0.0529 - output_2_loss: 1.0800e-05\n",
            "Epoch 125: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0578 - output_1_loss: 0.0545 - output_2_loss: 1.0798e-05 - val_loss: 0.0939 - val_output_1_loss: 0.0906 - val_output_2_loss: 1.0761e-05 - lr: 0.0010\n",
            "Epoch 126/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0252 - output_1_loss: 0.0219 - output_2_loss: 1.0733e-05\n",
            "Epoch 126: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 10ms/step - loss: 0.0252 - output_1_loss: 0.0219 - output_2_loss: 1.0733e-05 - val_loss: 0.0693 - val_output_1_loss: 0.0660 - val_output_2_loss: 1.0704e-05 - lr: 0.0010\n",
            "Epoch 127/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0393 - output_1_loss: 0.0360 - output_2_loss: 1.0676e-05\n",
            "Epoch 127: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0411 - output_1_loss: 0.0379 - output_2_loss: 1.0672e-05 - val_loss: 0.1265 - val_output_1_loss: 0.1233 - val_output_2_loss: 1.0628e-05 - lr: 0.0010\n",
            "Epoch 128/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0377 - output_1_loss: 0.0345 - output_2_loss: 1.0609e-05\n",
            "Epoch 128: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0377 - output_1_loss: 0.0345 - output_2_loss: 1.0609e-05 - val_loss: 0.0923 - val_output_1_loss: 0.0891 - val_output_2_loss: 1.0574e-05 - lr: 0.0010\n",
            "Epoch 129/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0396 - output_1_loss: 0.0364 - output_2_loss: 1.0547e-05\n",
            "Epoch 129: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0374 - output_1_loss: 0.0342 - output_2_loss: 1.0545e-05 - val_loss: 0.0689 - val_output_1_loss: 0.0657 - val_output_2_loss: 1.0518e-05 - lr: 0.0010\n",
            "Epoch 130/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0348 - output_1_loss: 0.0316 - output_2_loss: 1.0484e-05\n",
            "Epoch 130: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0336 - output_1_loss: 0.0304 - output_2_loss: 1.0481e-05 - val_loss: 0.0728 - val_output_1_loss: 0.0696 - val_output_2_loss: 1.0444e-05 - lr: 0.0010\n",
            "Epoch 131/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0403 - output_1_loss: 0.0371 - output_2_loss: 1.0420e-05\n",
            "Epoch 131: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0400 - output_1_loss: 0.0369 - output_2_loss: 1.0420e-05 - val_loss: 0.0667 - val_output_1_loss: 0.0635 - val_output_2_loss: 1.0380e-05 - lr: 0.0010\n",
            "Epoch 132/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0699 - output_1_loss: 0.0668 - output_2_loss: 1.0356e-05\n",
            "Epoch 132: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0690 - output_1_loss: 0.0658 - output_2_loss: 1.0354e-05 - val_loss: 0.1078 - val_output_1_loss: 0.1046 - val_output_2_loss: 1.0319e-05 - lr: 0.0010\n",
            "Epoch 133/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0457 - output_1_loss: 0.0426 - output_2_loss: 1.0291e-05\n",
            "Epoch 133: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0457 - output_1_loss: 0.0426 - output_2_loss: 1.0291e-05 - val_loss: 0.0693 - val_output_1_loss: 0.0661 - val_output_2_loss: 1.0264e-05 - lr: 0.0010\n",
            "Epoch 134/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0512 - output_1_loss: 0.0480 - output_2_loss: 1.0228e-05\n",
            "Epoch 134: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0512 - output_1_loss: 0.0480 - output_2_loss: 1.0228e-05 - val_loss: 0.0746 - val_output_1_loss: 0.0715 - val_output_2_loss: 1.0197e-05 - lr: 0.0010\n",
            "Epoch 135/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0382 - output_1_loss: 0.0350 - output_2_loss: 1.0166e-05\n",
            "Epoch 135: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0365 - output_1_loss: 0.0334 - output_2_loss: 1.0164e-05 - val_loss: 0.0721 - val_output_1_loss: 0.0690 - val_output_2_loss: 1.0132e-05 - lr: 0.0010\n",
            "Epoch 136/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0499 - output_1_loss: 0.0468 - output_2_loss: 1.0100e-05\n",
            "Epoch 136: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0499 - output_1_loss: 0.0468 - output_2_loss: 1.0100e-05 - val_loss: 0.0670 - val_output_1_loss: 0.0639 - val_output_2_loss: 1.0063e-05 - lr: 0.0010\n",
            "Epoch 137/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0313 - output_1_loss: 0.0282 - output_2_loss: 1.0035e-05\n",
            "Epoch 137: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0312 - output_1_loss: 0.0281 - output_2_loss: 1.0034e-05 - val_loss: 0.1077 - val_output_1_loss: 0.1046 - val_output_2_loss: 9.9991e-06 - lr: 0.0010\n",
            "Epoch 138/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0365 - output_1_loss: 0.0334 - output_2_loss: 9.9711e-06\n",
            "Epoch 138: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0367 - output_1_loss: 0.0336 - output_2_loss: 9.9704e-06 - val_loss: 0.1066 - val_output_1_loss: 0.1035 - val_output_2_loss: 9.9311e-06 - lr: 0.0010\n",
            "Epoch 139/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0539 - output_1_loss: 0.0508 - output_2_loss: 9.9044e-06\n",
            "Epoch 139: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0532 - output_1_loss: 0.0501 - output_2_loss: 9.9033e-06 - val_loss: 0.0949 - val_output_1_loss: 0.0919 - val_output_2_loss: 9.8709e-06 - lr: 0.0010\n",
            "Epoch 140/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0601 - output_1_loss: 0.0570 - output_2_loss: 9.8424e-06\n",
            "Epoch 140: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0590 - output_1_loss: 0.0559 - output_2_loss: 9.8392e-06 - val_loss: 0.0722 - val_output_1_loss: 0.0692 - val_output_2_loss: 9.8014e-06 - lr: 0.0010\n",
            "Epoch 141/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0444 - output_1_loss: 0.0414 - output_2_loss: 9.7737e-06\n",
            "Epoch 141: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0445 - output_1_loss: 0.0414 - output_2_loss: 9.7723e-06 - val_loss: 0.0716 - val_output_1_loss: 0.0686 - val_output_2_loss: 9.7387e-06 - lr: 0.0010\n",
            "Epoch 142/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0318 - output_1_loss: 0.0288 - output_2_loss: 9.7071e-06\n",
            "Epoch 142: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0309 - output_1_loss: 0.0279 - output_2_loss: 9.7059e-06 - val_loss: 0.0679 - val_output_1_loss: 0.0649 - val_output_2_loss: 9.6709e-06 - lr: 0.0010\n",
            "Epoch 143/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0401 - output_1_loss: 0.0370 - output_2_loss: 9.6389e-06\n",
            "Epoch 143: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0394 - output_1_loss: 0.0364 - output_2_loss: 9.6383e-06 - val_loss: 0.0765 - val_output_1_loss: 0.0735 - val_output_2_loss: 9.6101e-06 - lr: 0.0010\n",
            "Epoch 144/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0323 - output_1_loss: 0.0293 - output_2_loss: 9.5759e-06\n",
            "Epoch 144: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0327 - output_1_loss: 0.0297 - output_2_loss: 9.5726e-06 - val_loss: 0.0719 - val_output_1_loss: 0.0689 - val_output_2_loss: 9.5307e-06 - lr: 0.0010\n",
            "Epoch 145/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0581 - output_1_loss: 0.0551 - output_2_loss: 9.5041e-06\n",
            "Epoch 145: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0572 - output_1_loss: 0.0542 - output_2_loss: 9.5033e-06 - val_loss: 0.0747 - val_output_1_loss: 0.0717 - val_output_2_loss: 9.4583e-06 - lr: 0.0010\n",
            "Epoch 146/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0325 - output_1_loss: 0.0295 - output_2_loss: 9.4349e-06\n",
            "Epoch 146: val_loss did not improve from 0.06543\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0325 - output_1_loss: 0.0295 - output_2_loss: 9.4349e-06 - val_loss: 0.0794 - val_output_1_loss: 0.0764 - val_output_2_loss: 9.3938e-06 - lr: 0.0010\n",
            "Epoch 147/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0393 - output_1_loss: 0.0364 - output_2_loss: 9.3682e-06\n",
            "Epoch 147: val_loss improved from 0.06543 to 0.06467, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0380 - output_1_loss: 0.0350 - output_2_loss: 9.3662e-06 - val_loss: 0.0647 - val_output_1_loss: 0.0617 - val_output_2_loss: 9.3242e-06 - lr: 0.0010\n",
            "Epoch 148/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0353 - output_1_loss: 0.0323 - output_2_loss: 9.2958e-06\n",
            "Epoch 148: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0350 - output_1_loss: 0.0321 - output_2_loss: 9.2949e-06 - val_loss: 0.0686 - val_output_1_loss: 0.0656 - val_output_2_loss: 9.2567e-06 - lr: 0.0010\n",
            "Epoch 149/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0331 - output_1_loss: 0.0302 - output_2_loss: 9.2295e-06\n",
            "Epoch 149: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0328 - output_1_loss: 0.0299 - output_2_loss: 9.2259e-06 - val_loss: 0.0788 - val_output_1_loss: 0.0758 - val_output_2_loss: 9.1873e-06 - lr: 0.0010\n",
            "Epoch 150/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0367 - output_1_loss: 0.0338 - output_2_loss: 9.1589e-06\n",
            "Epoch 150: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0345 - output_1_loss: 0.0316 - output_2_loss: 9.1553e-06 - val_loss: 0.0696 - val_output_1_loss: 0.0667 - val_output_2_loss: 9.1242e-06 - lr: 0.0010\n",
            "Epoch 151/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0304 - output_1_loss: 0.0275 - output_2_loss: 9.0874e-06\n",
            "Epoch 151: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0329 - output_1_loss: 0.0300 - output_2_loss: 9.0851e-06 - val_loss: 0.0852 - val_output_1_loss: 0.0823 - val_output_2_loss: 9.0468e-06 - lr: 0.0010\n",
            "Epoch 152/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0271 - output_1_loss: 0.0242 - output_2_loss: 9.0169e-06\n",
            "Epoch 152: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0270 - output_1_loss: 0.0241 - output_2_loss: 9.0133e-06 - val_loss: 0.0855 - val_output_1_loss: 0.0827 - val_output_2_loss: 8.9686e-06 - lr: 0.0010\n",
            "Epoch 153/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0354 - output_1_loss: 0.0325 - output_2_loss: 8.9397e-06\n",
            "Epoch 153: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0354 - output_1_loss: 0.0325 - output_2_loss: 8.9397e-06 - val_loss: 0.0774 - val_output_1_loss: 0.0745 - val_output_2_loss: 8.9074e-06 - lr: 0.0010\n",
            "Epoch 154/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0315 - output_1_loss: 0.0286 - output_2_loss: 8.8724e-06\n",
            "Epoch 154: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0335 - output_1_loss: 0.0306 - output_2_loss: 8.8686e-06 - val_loss: 0.0760 - val_output_1_loss: 0.0731 - val_output_2_loss: 8.8343e-06 - lr: 0.0010\n",
            "Epoch 155/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0343 - output_1_loss: 0.0314 - output_2_loss: 8.7991e-06\n",
            "Epoch 155: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0326 - output_1_loss: 0.0298 - output_2_loss: 8.7966e-06 - val_loss: 0.0752 - val_output_1_loss: 0.0724 - val_output_2_loss: 8.7545e-06 - lr: 0.0010\n",
            "Epoch 156/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0324 - output_1_loss: 0.0296 - output_2_loss: 8.7248e-06\n",
            "Epoch 156: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0326 - output_1_loss: 0.0298 - output_2_loss: 8.7217e-06 - val_loss: 0.0687 - val_output_1_loss: 0.0658 - val_output_2_loss: 8.6844e-06 - lr: 0.0010\n",
            "Epoch 157/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0320 - output_1_loss: 0.0292 - output_2_loss: 8.6498e-06\n",
            "Epoch 157: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0329 - output_1_loss: 0.0301 - output_2_loss: 8.6486e-06 - val_loss: 0.0754 - val_output_1_loss: 0.0725 - val_output_2_loss: 8.6121e-06 - lr: 0.0010\n",
            "Epoch 158/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0260 - output_1_loss: 0.0232 - output_2_loss: 8.5769e-06\n",
            "Epoch 158: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0276 - output_1_loss: 0.0248 - output_2_loss: 8.5746e-06 - val_loss: 0.0668 - val_output_1_loss: 0.0640 - val_output_2_loss: 8.5353e-06 - lr: 0.0010\n",
            "Epoch 159/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0441 - output_1_loss: 0.0413 - output_2_loss: 8.5019e-06\n",
            "Epoch 159: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0495 - output_1_loss: 0.0467 - output_2_loss: 8.5000e-06 - val_loss: 0.0952 - val_output_1_loss: 0.0925 - val_output_2_loss: 8.4580e-06 - lr: 0.0010\n",
            "Epoch 160/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0300 - output_1_loss: 0.0273 - output_2_loss: 8.4262e-06\n",
            "Epoch 160: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0289 - output_1_loss: 0.0261 - output_2_loss: 8.4236e-06 - val_loss: 0.0717 - val_output_1_loss: 0.0689 - val_output_2_loss: 8.3941e-06 - lr: 0.0010\n",
            "Epoch 161/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0496 - output_1_loss: 0.0468 - output_2_loss: 8.3529e-06\n",
            "Epoch 161: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0488 - output_1_loss: 0.0461 - output_2_loss: 8.3522e-06 - val_loss: 0.0765 - val_output_1_loss: 0.0738 - val_output_2_loss: 8.3082e-06 - lr: 0.0010\n",
            "Epoch 162/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0316 - output_1_loss: 0.0289 - output_2_loss: 8.2772e-06\n",
            "Epoch 162: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0332 - output_1_loss: 0.0304 - output_2_loss: 8.2745e-06 - val_loss: 0.1031 - val_output_1_loss: 0.1004 - val_output_2_loss: 8.2391e-06 - lr: 0.0010\n",
            "Epoch 163/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0365 - output_1_loss: 0.0337 - output_2_loss: 8.2016e-06\n",
            "Epoch 163: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0355 - output_1_loss: 0.0328 - output_2_loss: 8.2002e-06 - val_loss: 0.0685 - val_output_1_loss: 0.0658 - val_output_2_loss: 8.1608e-06 - lr: 0.0010\n",
            "Epoch 164/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0313 - output_1_loss: 0.0286 - output_2_loss: 8.1279e-06\n",
            "Epoch 164: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0349 - output_1_loss: 0.0322 - output_2_loss: 8.1248e-06 - val_loss: 0.1005 - val_output_1_loss: 0.0977 - val_output_2_loss: 8.0832e-06 - lr: 0.0010\n",
            "Epoch 165/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0362 - output_1_loss: 0.0335 - output_2_loss: 8.0511e-06\n",
            "Epoch 165: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0349 - output_1_loss: 0.0322 - output_2_loss: 8.0491e-06 - val_loss: 0.0770 - val_output_1_loss: 0.0743 - val_output_2_loss: 8.0110e-06 - lr: 0.0010\n",
            "Epoch 166/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0297 - output_1_loss: 0.0270 - output_2_loss: 7.9745e-06\n",
            "Epoch 166: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0295 - output_1_loss: 0.0268 - output_2_loss: 7.9735e-06 - val_loss: 0.0773 - val_output_1_loss: 0.0746 - val_output_2_loss: 7.9364e-06 - lr: 0.0010\n",
            "Epoch 167/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0456 - output_1_loss: 0.0429 - output_2_loss: 7.8980e-06\n",
            "Epoch 167: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0456 - output_1_loss: 0.0429 - output_2_loss: 7.8980e-06 - val_loss: 0.0918 - val_output_1_loss: 0.0891 - val_output_2_loss: 7.8577e-06 - lr: 0.0010\n",
            "Epoch 168/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0318 - output_1_loss: 0.0291 - output_2_loss: 7.8230e-06\n",
            "Epoch 168: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0316 - output_1_loss: 0.0289 - output_2_loss: 7.8224e-06 - val_loss: 0.0888 - val_output_1_loss: 0.0862 - val_output_2_loss: 7.7828e-06 - lr: 0.0010\n",
            "Epoch 169/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0317 - output_1_loss: 0.0291 - output_2_loss: 7.7504e-06\n",
            "Epoch 169: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0397 - output_1_loss: 0.0371 - output_2_loss: 7.7468e-06 - val_loss: 0.1693 - val_output_1_loss: 0.1667 - val_output_2_loss: 7.7068e-06 - lr: 0.0010\n",
            "Epoch 170/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0585 - output_1_loss: 0.0559 - output_2_loss: 7.6729e-06\n",
            "Epoch 170: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0587 - output_1_loss: 0.0560 - output_2_loss: 7.6714e-06 - val_loss: 0.0747 - val_output_1_loss: 0.0721 - val_output_2_loss: 7.6334e-06 - lr: 0.0010\n",
            "Epoch 171/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0291 - output_1_loss: 0.0264 - output_2_loss: 7.6008e-06\n",
            "Epoch 171: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 10ms/step - loss: 0.0312 - output_1_loss: 0.0286 - output_2_loss: 7.5971e-06 - val_loss: 0.0993 - val_output_1_loss: 0.0967 - val_output_2_loss: 7.5557e-06 - lr: 0.0010\n",
            "Epoch 172/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0430 - output_1_loss: 0.0403 - output_2_loss: 7.5249e-06\n",
            "Epoch 172: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0401 - output_1_loss: 0.0375 - output_2_loss: 7.5209e-06 - val_loss: 0.0812 - val_output_1_loss: 0.0786 - val_output_2_loss: 7.4843e-06 - lr: 0.0010\n",
            "Epoch 173/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0424 - output_1_loss: 0.0398 - output_2_loss: 7.4516e-06\n",
            "Epoch 173: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0405 - output_1_loss: 0.0379 - output_2_loss: 7.4476e-06 - val_loss: 0.0695 - val_output_1_loss: 0.0669 - val_output_2_loss: 7.4053e-06 - lr: 0.0010\n",
            "Epoch 174/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0258 - output_1_loss: 0.0232 - output_2_loss: 7.3728e-06\n",
            "Epoch 174: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0264 - output_1_loss: 0.0238 - output_2_loss: 7.3721e-06 - val_loss: 0.0768 - val_output_1_loss: 0.0743 - val_output_2_loss: 7.3346e-06 - lr: 0.0010\n",
            "Epoch 175/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0315 - output_1_loss: 0.0289 - output_2_loss: 7.2986e-06\n",
            "Epoch 175: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0315 - output_1_loss: 0.0289 - output_2_loss: 7.2986e-06 - val_loss: 0.0880 - val_output_1_loss: 0.0854 - val_output_2_loss: 7.2623e-06 - lr: 0.0010\n",
            "Epoch 176/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0260 - output_1_loss: 0.0234 - output_2_loss: 7.2289e-06\n",
            "Epoch 176: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0287 - output_1_loss: 0.0261 - output_2_loss: 7.2249e-06 - val_loss: 0.0694 - val_output_1_loss: 0.0669 - val_output_2_loss: 7.1859e-06 - lr: 0.0010\n",
            "Epoch 177/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0464 - output_1_loss: 0.0439 - output_2_loss: 7.1525e-06\n",
            "Epoch 177: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0455 - output_1_loss: 0.0429 - output_2_loss: 7.1514e-06 - val_loss: 0.0734 - val_output_1_loss: 0.0708 - val_output_2_loss: 7.1129e-06 - lr: 0.0010\n",
            "Epoch 178/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0335 - output_1_loss: 0.0310 - output_2_loss: 7.0796e-06\n",
            "Epoch 178: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0332 - output_1_loss: 0.0306 - output_2_loss: 7.0782e-06 - val_loss: 0.0749 - val_output_1_loss: 0.0724 - val_output_2_loss: 7.0406e-06 - lr: 0.0010\n",
            "Epoch 179/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0304 - output_1_loss: 0.0279 - output_2_loss: 7.0064e-06\n",
            "Epoch 179: val_loss did not improve from 0.06467\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0308 - output_1_loss: 0.0283 - output_2_loss: 7.0056e-06 - val_loss: 0.0668 - val_output_1_loss: 0.0643 - val_output_2_loss: 6.9680e-06 - lr: 0.0010\n",
            "Epoch 180/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0394 - output_1_loss: 0.0369 - output_2_loss: 6.9358e-06\n",
            "Epoch 180: val_loss improved from 0.06467 to 0.06241, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0384 - output_1_loss: 0.0359 - output_2_loss: 6.9345e-06 - val_loss: 0.0624 - val_output_1_loss: 0.0599 - val_output_2_loss: 6.8944e-06 - lr: 0.0010\n",
            "Epoch 181/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0417 - output_1_loss: 0.0392 - output_2_loss: 6.8623e-06\n",
            "Epoch 181: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 9ms/step - loss: 0.0412 - output_1_loss: 0.0387 - output_2_loss: 6.8614e-06 - val_loss: 0.0794 - val_output_1_loss: 0.0769 - val_output_2_loss: 6.8250e-06 - lr: 0.0010\n",
            "Epoch 182/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0320 - output_1_loss: 0.0295 - output_2_loss: 6.7952e-06\n",
            "Epoch 182: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0318 - output_1_loss: 0.0293 - output_2_loss: 6.7915e-06 - val_loss: 0.0919 - val_output_1_loss: 0.0894 - val_output_2_loss: 6.7541e-06 - lr: 0.0010\n",
            "Epoch 183/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0382 - output_1_loss: 0.0357 - output_2_loss: 6.7220e-06\n",
            "Epoch 183: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0376 - output_1_loss: 0.0351 - output_2_loss: 6.7206e-06 - val_loss: 0.0688 - val_output_1_loss: 0.0664 - val_output_2_loss: 6.6832e-06 - lr: 0.0010\n",
            "Epoch 184/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0315 - output_1_loss: 0.0290 - output_2_loss: 6.6505e-06\n",
            "Epoch 184: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0315 - output_1_loss: 0.0290 - output_2_loss: 6.6505e-06 - val_loss: 0.0720 - val_output_1_loss: 0.0695 - val_output_2_loss: 6.6144e-06 - lr: 0.0010\n",
            "Epoch 185/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0384 - output_1_loss: 0.0360 - output_2_loss: 6.5817e-06\n",
            "Epoch 185: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0384 - output_1_loss: 0.0360 - output_2_loss: 6.5817e-06 - val_loss: 0.0661 - val_output_1_loss: 0.0636 - val_output_2_loss: 6.5453e-06 - lr: 0.0010\n",
            "Epoch 186/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0238 - output_1_loss: 0.0213 - output_2_loss: 6.5159e-06\n",
            "Epoch 186: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0270 - output_1_loss: 0.0245 - output_2_loss: 6.5124e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0604 - val_output_2_loss: 6.4777e-06 - lr: 0.0010\n",
            "Epoch 187/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0400 - output_1_loss: 0.0376 - output_2_loss: 6.4459e-06\n",
            "Epoch 187: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0392 - output_1_loss: 0.0368 - output_2_loss: 6.4444e-06 - val_loss: 0.0787 - val_output_1_loss: 0.0763 - val_output_2_loss: 6.4074e-06 - lr: 0.0010\n",
            "Epoch 188/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0323 - output_1_loss: 0.0299 - output_2_loss: 6.3786e-06\n",
            "Epoch 188: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0319 - output_1_loss: 0.0295 - output_2_loss: 6.3768e-06 - val_loss: 0.1029 - val_output_1_loss: 0.1005 - val_output_2_loss: 6.3426e-06 - lr: 0.0010\n",
            "Epoch 189/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0425 - output_1_loss: 0.0401 - output_2_loss: 6.3117e-06\n",
            "Epoch 189: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0420 - output_1_loss: 0.0396 - output_2_loss: 6.3098e-06 - val_loss: 0.0896 - val_output_1_loss: 0.0872 - val_output_2_loss: 6.2744e-06 - lr: 0.0010\n",
            "Epoch 190/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0379 - output_1_loss: 0.0355 - output_2_loss: 6.2461e-06\n",
            "Epoch 190: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0355 - output_1_loss: 0.0332 - output_2_loss: 6.2436e-06 - val_loss: 0.0750 - val_output_1_loss: 0.0727 - val_output_2_loss: 6.2104e-06 - lr: 0.0010\n",
            "Epoch 191/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0363 - output_1_loss: 0.0340 - output_2_loss: 6.1808e-06\n",
            "Epoch 191: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0350 - output_1_loss: 0.0327 - output_2_loss: 6.1779e-06 - val_loss: 0.0670 - val_output_1_loss: 0.0647 - val_output_2_loss: 6.1432e-06 - lr: 0.0010\n",
            "Epoch 192/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0299 - output_1_loss: 0.0275 - output_2_loss: 6.1141e-06\n",
            "Epoch 192: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0295 - output_1_loss: 0.0271 - output_2_loss: 6.1128e-06 - val_loss: 0.0740 - val_output_1_loss: 0.0716 - val_output_2_loss: 6.0804e-06 - lr: 0.0010\n",
            "Epoch 193/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0323 - output_1_loss: 0.0300 - output_2_loss: 6.0496e-06\n",
            "Epoch 193: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0320 - output_1_loss: 0.0297 - output_2_loss: 6.0489e-06 - val_loss: 0.0853 - val_output_1_loss: 0.0830 - val_output_2_loss: 6.0154e-06 - lr: 0.0010\n",
            "Epoch 194/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0285 - output_1_loss: 0.0261 - output_2_loss: 5.9864e-06\n",
            "Epoch 194: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0282 - output_1_loss: 0.0259 - output_2_loss: 5.9850e-06 - val_loss: 0.0711 - val_output_1_loss: 0.0688 - val_output_2_loss: 5.9524e-06 - lr: 0.0010\n",
            "Epoch 195/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0335 - output_1_loss: 0.0312 - output_2_loss: 5.9236e-06\n",
            "Epoch 195: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0325 - output_1_loss: 0.0302 - output_2_loss: 5.9223e-06 - val_loss: 0.0700 - val_output_1_loss: 0.0676 - val_output_2_loss: 5.8906e-06 - lr: 0.0010\n",
            "Epoch 196/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0283 - output_1_loss: 0.0260 - output_2_loss: 5.8616e-06\n",
            "Epoch 196: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0275 - output_1_loss: 0.0252 - output_2_loss: 5.8601e-06 - val_loss: 0.0756 - val_output_1_loss: 0.0733 - val_output_2_loss: 5.8275e-06 - lr: 0.0010\n",
            "Epoch 197/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0434 - output_1_loss: 0.0411 - output_2_loss: 5.7985e-06\n",
            "Epoch 197: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0427 - output_1_loss: 0.0404 - output_2_loss: 5.7980e-06 - val_loss: 0.0684 - val_output_1_loss: 0.0661 - val_output_2_loss: 5.7684e-06 - lr: 0.0010\n",
            "Epoch 198/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0284 - output_1_loss: 0.0261 - output_2_loss: 5.7407e-06\n",
            "Epoch 198: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0286 - output_1_loss: 0.0264 - output_2_loss: 5.7376e-06 - val_loss: 0.0866 - val_output_1_loss: 0.0844 - val_output_2_loss: 5.7057e-06 - lr: 0.0010\n",
            "Epoch 199/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0389 - output_1_loss: 0.0366 - output_2_loss: 5.6794e-06\n",
            "Epoch 199: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0414 - output_1_loss: 0.0391 - output_2_loss: 5.6775e-06 - val_loss: 0.0642 - val_output_1_loss: 0.0619 - val_output_2_loss: 5.6446e-06 - lr: 0.0010\n",
            "Epoch 200/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0315 - output_1_loss: 0.0293 - output_2_loss: 5.6178e-06\n",
            "Epoch 200: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0315 - output_1_loss: 0.0293 - output_2_loss: 5.6178e-06 - val_loss: 0.0639 - val_output_1_loss: 0.0617 - val_output_2_loss: 5.5870e-06 - lr: 0.0010\n",
            "Epoch 201/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0334 - output_1_loss: 0.0312 - output_2_loss: 5.5618e-06\n",
            "Epoch 201: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0309 - output_1_loss: 0.0286 - output_2_loss: 5.5586e-06 - val_loss: 0.0665 - val_output_1_loss: 0.0642 - val_output_2_loss: 5.5296e-06 - lr: 0.0010\n",
            "Epoch 202/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0301 - output_1_loss: 0.0279 - output_2_loss: 5.5037e-06\n",
            "Epoch 202: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0303 - output_1_loss: 0.0281 - output_2_loss: 5.5010e-06 - val_loss: 0.0855 - val_output_1_loss: 0.0833 - val_output_2_loss: 5.4714e-06 - lr: 0.0010\n",
            "Epoch 203/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0509 - output_1_loss: 0.0487 - output_2_loss: 5.4465e-06\n",
            "Epoch 203: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0507 - output_1_loss: 0.0484 - output_2_loss: 5.4437e-06 - val_loss: 0.0689 - val_output_1_loss: 0.0667 - val_output_2_loss: 5.4133e-06 - lr: 0.0010\n",
            "Epoch 204/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0516 - output_1_loss: 0.0494 - output_2_loss: 5.3880e-06\n",
            "Epoch 204: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0500 - output_1_loss: 0.0478 - output_2_loss: 5.3869e-06 - val_loss: 0.0689 - val_output_1_loss: 0.0667 - val_output_2_loss: 5.3563e-06 - lr: 0.0010\n",
            "Epoch 205/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0253 - output_1_loss: 0.0231 - output_2_loss: 5.3330e-06\n",
            "Epoch 205: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0269 - output_1_loss: 0.0247 - output_2_loss: 5.3304e-06 - val_loss: 0.0924 - val_output_1_loss: 0.0903 - val_output_2_loss: 5.3015e-06 - lr: 0.0010\n",
            "Epoch 206/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0204 - output_1_loss: 0.0182 - output_2_loss: 5.2779e-06\n",
            "Epoch 206: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - output_1_loss: 0.0202 - output_2_loss: 5.2756e-06 - val_loss: 0.0672 - val_output_1_loss: 0.0650 - val_output_2_loss: 5.2473e-06 - lr: 0.0010\n",
            "Epoch 207/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0359 - output_1_loss: 0.0337 - output_2_loss: 5.2226e-06\n",
            "Epoch 207: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0407 - output_1_loss: 0.0385 - output_2_loss: 5.2208e-06 - val_loss: 0.1704 - val_output_1_loss: 0.1683 - val_output_2_loss: 5.1921e-06 - lr: 0.0010\n",
            "Epoch 208/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0501 - output_1_loss: 0.0480 - output_2_loss: 5.1679e-06\n",
            "Epoch 208: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0502 - output_1_loss: 0.0481 - output_2_loss: 5.1668e-06 - val_loss: 0.0706 - val_output_1_loss: 0.0684 - val_output_2_loss: 5.1387e-06 - lr: 0.0010\n",
            "Epoch 209/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0333 - output_1_loss: 0.0312 - output_2_loss: 5.1141e-06\n",
            "Epoch 209: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0326 - output_1_loss: 0.0305 - output_2_loss: 5.1131e-06 - val_loss: 0.0653 - val_output_1_loss: 0.0632 - val_output_2_loss: 5.0868e-06 - lr: 0.0010\n",
            "Epoch 210/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0232 - output_1_loss: 0.0211 - output_2_loss: 5.0634e-06\n",
            "Epoch 210: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0236 - output_1_loss: 0.0215 - output_2_loss: 5.0608e-06 - val_loss: 0.0766 - val_output_1_loss: 0.0745 - val_output_2_loss: 5.0341e-06 - lr: 0.0010\n",
            "Epoch 211/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0502 - output_1_loss: 0.0481 - output_2_loss: 5.0098e-06\n",
            "Epoch 211: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0489 - output_1_loss: 0.0467 - output_2_loss: 5.0088e-06 - val_loss: 0.0682 - val_output_1_loss: 0.0661 - val_output_2_loss: 4.9820e-06 - lr: 0.0010\n",
            "Epoch 212/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0422 - output_1_loss: 0.0401 - output_2_loss: 4.9604e-06\n",
            "Epoch 212: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0393 - output_1_loss: 0.0372 - output_2_loss: 4.9575e-06 - val_loss: 0.0661 - val_output_1_loss: 0.0640 - val_output_2_loss: 4.9311e-06 - lr: 0.0010\n",
            "Epoch 213/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0339 - output_1_loss: 0.0318 - output_2_loss: 4.9082e-06\n",
            "Epoch 213: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0337 - output_1_loss: 0.0316 - output_2_loss: 4.9066e-06 - val_loss: 0.0647 - val_output_1_loss: 0.0626 - val_output_2_loss: 4.8795e-06 - lr: 0.0010\n",
            "Epoch 214/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0271 - output_1_loss: 0.0250 - output_2_loss: 4.8595e-06\n",
            "Epoch 214: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0309 - output_1_loss: 0.0289 - output_2_loss: 4.8571e-06 - val_loss: 0.0630 - val_output_1_loss: 0.0609 - val_output_2_loss: 4.8301e-06 - lr: 0.0010\n",
            "Epoch 215/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0168 - output_1_loss: 0.0147 - output_2_loss: 4.8083e-06\n",
            "Epoch 215: val_loss did not improve from 0.06241\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0199 - output_1_loss: 0.0179 - output_2_loss: 4.8068e-06 - val_loss: 0.0739 - val_output_1_loss: 0.0719 - val_output_2_loss: 4.7821e-06 - lr: 0.0010\n",
            "Epoch 216/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0308 - output_1_loss: 0.0288 - output_2_loss: 4.7588e-06\n",
            "Epoch 216: val_loss improved from 0.06241 to 0.06150, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0308 - output_1_loss: 0.0288 - output_2_loss: 4.7588e-06 - val_loss: 0.0615 - val_output_1_loss: 0.0595 - val_output_2_loss: 4.7329e-06 - lr: 0.0010\n",
            "Epoch 217/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0294 - output_1_loss: 0.0273 - output_2_loss: 4.7108e-06\n",
            "Epoch 217: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0296 - output_1_loss: 0.0276 - output_2_loss: 4.7104e-06 - val_loss: 0.0977 - val_output_1_loss: 0.0957 - val_output_2_loss: 4.6853e-06 - lr: 0.0010\n",
            "Epoch 218/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0348 - output_1_loss: 0.0328 - output_2_loss: 4.6635e-06\n",
            "Epoch 218: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0340 - output_1_loss: 0.0320 - output_2_loss: 4.6625e-06 - val_loss: 0.0742 - val_output_1_loss: 0.0722 - val_output_2_loss: 4.6385e-06 - lr: 0.0010\n",
            "Epoch 219/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0511 - output_1_loss: 0.0490 - output_2_loss: 4.6171e-06\n",
            "Epoch 219: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0504 - output_1_loss: 0.0484 - output_2_loss: 4.6165e-06 - val_loss: 0.1046 - val_output_1_loss: 0.1026 - val_output_2_loss: 4.5905e-06 - lr: 0.0010\n",
            "Epoch 220/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0268 - output_1_loss: 0.0248 - output_2_loss: 4.5706e-06\n",
            "Epoch 220: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0257 - output_1_loss: 0.0236 - output_2_loss: 4.5695e-06 - val_loss: 0.0878 - val_output_1_loss: 0.0858 - val_output_2_loss: 4.5465e-06 - lr: 0.0010\n",
            "Epoch 221/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0417 - output_1_loss: 0.0397 - output_2_loss: 4.5242e-06\n",
            "Epoch 221: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0417 - output_1_loss: 0.0397 - output_2_loss: 4.5242e-06 - val_loss: 0.0825 - val_output_1_loss: 0.0805 - val_output_2_loss: 4.5002e-06 - lr: 0.0010\n",
            "Epoch 222/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0275 - output_1_loss: 0.0256 - output_2_loss: 4.4798e-06\n",
            "Epoch 222: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - output_1_loss: 0.0280 - output_2_loss: 4.4789e-06 - val_loss: 0.0762 - val_output_1_loss: 0.0742 - val_output_2_loss: 4.4560e-06 - lr: 0.0010\n",
            "Epoch 223/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0378 - output_1_loss: 0.0359 - output_2_loss: 4.4365e-06\n",
            "Epoch 223: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0372 - output_1_loss: 0.0353 - output_2_loss: 4.4347e-06 - val_loss: 0.1110 - val_output_1_loss: 0.1091 - val_output_2_loss: 4.4107e-06 - lr: 0.0010\n",
            "Epoch 224/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0465 - output_1_loss: 0.0446 - output_2_loss: 4.3912e-06\n",
            "Epoch 224: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0465 - output_1_loss: 0.0445 - output_2_loss: 4.3907e-06 - val_loss: 0.0850 - val_output_1_loss: 0.0830 - val_output_2_loss: 4.3679e-06 - lr: 0.0010\n",
            "Epoch 225/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0288 - output_1_loss: 0.0268 - output_2_loss: 4.3470e-06\n",
            "Epoch 225: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0288 - output_1_loss: 0.0268 - output_2_loss: 4.3470e-06 - val_loss: 0.0730 - val_output_1_loss: 0.0710 - val_output_2_loss: 4.3250e-06 - lr: 0.0010\n",
            "Epoch 226/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0228 - output_1_loss: 0.0208 - output_2_loss: 4.3064e-06\n",
            "Epoch 226: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0215 - output_1_loss: 0.0196 - output_2_loss: 4.3046e-06 - val_loss: 0.0740 - val_output_1_loss: 0.0720 - val_output_2_loss: 4.2827e-06 - lr: 0.0010\n",
            "Epoch 227/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0347 - output_1_loss: 0.0328 - output_2_loss: 4.2637e-06\n",
            "Epoch 227: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 10ms/step - loss: 0.0335 - output_1_loss: 0.0316 - output_2_loss: 4.2625e-06 - val_loss: 0.0732 - val_output_1_loss: 0.0713 - val_output_2_loss: 4.2408e-06 - lr: 0.0010\n",
            "Epoch 228/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0355 - output_1_loss: 0.0336 - output_2_loss: 4.2212e-06\n",
            "Epoch 228: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0354 - output_1_loss: 0.0335 - output_2_loss: 4.2208e-06 - val_loss: 0.0659 - val_output_1_loss: 0.0640 - val_output_2_loss: 4.1996e-06 - lr: 0.0010\n",
            "Epoch 229/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0237 - output_1_loss: 0.0218 - output_2_loss: 4.1817e-06\n",
            "Epoch 229: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0245 - output_1_loss: 0.0226 - output_2_loss: 4.1797e-06 - val_loss: 0.0846 - val_output_1_loss: 0.0826 - val_output_2_loss: 4.1583e-06 - lr: 0.0010\n",
            "Epoch 230/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0312 - output_1_loss: 0.0293 - output_2_loss: 4.1394e-06\n",
            "Epoch 230: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0308 - output_1_loss: 0.0289 - output_2_loss: 4.1390e-06 - val_loss: 0.0639 - val_output_1_loss: 0.0620 - val_output_2_loss: 4.1185e-06 - lr: 0.0010\n",
            "Epoch 231/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0295 - output_1_loss: 0.0276 - output_2_loss: 4.1010e-06\n",
            "Epoch 231: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0278 - output_1_loss: 0.0259 - output_2_loss: 4.0994e-06 - val_loss: 0.0698 - val_output_1_loss: 0.0679 - val_output_2_loss: 4.0785e-06 - lr: 0.0010\n",
            "Epoch 232/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0215 - output_1_loss: 0.0196 - output_2_loss: 4.0600e-06\n",
            "Epoch 232: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 9ms/step - loss: 0.0212 - output_1_loss: 0.0193 - output_2_loss: 4.0597e-06 - val_loss: 0.0786 - val_output_1_loss: 0.0767 - val_output_2_loss: 4.0393e-06 - lr: 0.0010\n",
            "Epoch 233/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0427 - output_1_loss: 0.0408 - output_2_loss: 4.0225e-06\n",
            "Epoch 233: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0406 - output_1_loss: 0.0387 - output_2_loss: 4.0208e-06 - val_loss: 0.0720 - val_output_1_loss: 0.0701 - val_output_2_loss: 4.0007e-06 - lr: 0.0010\n",
            "Epoch 234/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0357 - output_1_loss: 0.0338 - output_2_loss: 3.9823e-06\n",
            "Epoch 234: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0357 - output_1_loss: 0.0338 - output_2_loss: 3.9823e-06 - val_loss: 0.0793 - val_output_1_loss: 0.0774 - val_output_2_loss: 3.9624e-06 - lr: 0.0010\n",
            "Epoch 235/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0298 - output_1_loss: 0.0279 - output_2_loss: 3.9459e-06\n",
            "Epoch 235: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - output_1_loss: 0.0282 - output_2_loss: 3.9446e-06 - val_loss: 0.0647 - val_output_1_loss: 0.0628 - val_output_2_loss: 3.9249e-06 - lr: 0.0010\n",
            "Epoch 236/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0334 - output_1_loss: 0.0315 - output_2_loss: 3.9076e-06\n",
            "Epoch 236: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0333 - output_1_loss: 0.0315 - output_2_loss: 3.9072e-06 - val_loss: 0.1270 - val_output_1_loss: 0.1252 - val_output_2_loss: 3.8873e-06 - lr: 0.0010\n",
            "Epoch 237/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0330 - output_1_loss: 0.0312 - output_2_loss: 3.8718e-06\n",
            "Epoch 237: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0317 - output_1_loss: 0.0299 - output_2_loss: 3.8701e-06 - val_loss: 0.0659 - val_output_1_loss: 0.0640 - val_output_2_loss: 3.8508e-06 - lr: 0.0010\n",
            "Epoch 238/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0327 - output_1_loss: 0.0308 - output_2_loss: 3.8346e-06\n",
            "Epoch 238: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0318 - output_1_loss: 0.0300 - output_2_loss: 3.8341e-06 - val_loss: 0.0764 - val_output_1_loss: 0.0746 - val_output_2_loss: 3.8149e-06 - lr: 0.0010\n",
            "Epoch 239/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0335 - output_1_loss: 0.0317 - output_2_loss: 3.7986e-06\n",
            "Epoch 239: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0344 - output_1_loss: 0.0326 - output_2_loss: 3.7975e-06 - val_loss: 0.1021 - val_output_1_loss: 0.1003 - val_output_2_loss: 3.7794e-06 - lr: 0.0010\n",
            "Epoch 240/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0349 - output_1_loss: 0.0331 - output_2_loss: 3.7631e-06\n",
            "Epoch 240: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0367 - output_1_loss: 0.0349 - output_2_loss: 3.7628e-06 - val_loss: 0.0743 - val_output_1_loss: 0.0725 - val_output_2_loss: 3.7438e-06 - lr: 0.0010\n",
            "Epoch 241/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0340 - output_1_loss: 0.0322 - output_2_loss: 3.7276e-06\n",
            "Epoch 241: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0346 - output_1_loss: 0.0329 - output_2_loss: 3.7273e-06 - val_loss: 0.0708 - val_output_1_loss: 0.0690 - val_output_2_loss: 3.7093e-06 - lr: 0.0010\n",
            "Epoch 242/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0212 - output_1_loss: 0.0194 - output_2_loss: 3.6938e-06\n",
            "Epoch 242: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0211 - output_1_loss: 0.0194 - output_2_loss: 3.6928e-06 - val_loss: 0.0690 - val_output_1_loss: 0.0672 - val_output_2_loss: 3.6749e-06 - lr: 0.0010\n",
            "Epoch 243/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0308 - output_1_loss: 0.0290 - output_2_loss: 3.6590e-06\n",
            "Epoch 243: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0308 - output_1_loss: 0.0290 - output_2_loss: 3.6590e-06 - val_loss: 0.0861 - val_output_1_loss: 0.0844 - val_output_2_loss: 3.6414e-06 - lr: 0.0010\n",
            "Epoch 244/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0396 - output_1_loss: 0.0379 - output_2_loss: 3.6252e-06\n",
            "Epoch 244: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0396 - output_1_loss: 0.0379 - output_2_loss: 3.6252e-06 - val_loss: 0.0825 - val_output_1_loss: 0.0807 - val_output_2_loss: 3.6079e-06 - lr: 0.0010\n",
            "Epoch 245/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0265 - output_1_loss: 0.0247 - output_2_loss: 3.5923e-06\n",
            "Epoch 245: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0265 - output_1_loss: 0.0247 - output_2_loss: 3.5923e-06 - val_loss: 0.0875 - val_output_1_loss: 0.0858 - val_output_2_loss: 3.5748e-06 - lr: 0.0010\n",
            "Epoch 246/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0319 - output_1_loss: 0.0302 - output_2_loss: 3.5598e-06\n",
            "Epoch 246: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0316 - output_1_loss: 0.0298 - output_2_loss: 3.5592e-06 - val_loss: 0.0814 - val_output_1_loss: 0.0797 - val_output_2_loss: 3.5428e-06 - lr: 0.0010\n",
            "Epoch 247/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0290 - output_1_loss: 0.0273 - output_2_loss: 3.5278e-06\n",
            "Epoch 247: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0282 - output_1_loss: 0.0265 - output_2_loss: 3.5273e-06 - val_loss: 0.0748 - val_output_1_loss: 0.0731 - val_output_2_loss: 3.5102e-06 - lr: 0.0010\n",
            "Epoch 248/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0351 - output_1_loss: 0.0334 - output_2_loss: 3.4953e-06\n",
            "Epoch 248: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0350 - output_1_loss: 0.0333 - output_2_loss: 3.4950e-06 - val_loss: 0.0671 - val_output_1_loss: 0.0654 - val_output_2_loss: 3.4790e-06 - lr: 0.0010\n",
            "Epoch 249/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0225 - output_1_loss: 0.0208 - output_2_loss: 3.4637e-06\n",
            "Epoch 249: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - output_1_loss: 0.0208 - output_2_loss: 3.4637e-06 - val_loss: 0.0665 - val_output_1_loss: 0.0648 - val_output_2_loss: 3.4481e-06 - lr: 0.0010\n",
            "Epoch 250/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0380 - output_1_loss: 0.0363 - output_2_loss: 3.4339e-06\n",
            "Epoch 250: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0409 - output_1_loss: 0.0392 - output_2_loss: 3.4330e-06 - val_loss: 0.0638 - val_output_1_loss: 0.0621 - val_output_2_loss: 3.4169e-06 - lr: 0.0010\n",
            "Epoch 251/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0193 - output_1_loss: 0.0176 - output_2_loss: 3.4032e-06\n",
            "Epoch 251: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 10ms/step - loss: 0.0187 - output_1_loss: 0.0170 - output_2_loss: 3.4022e-06 - val_loss: 0.0667 - val_output_1_loss: 0.0650 - val_output_2_loss: 3.3865e-06 - lr: 0.0010\n",
            "Epoch 252/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0263 - output_1_loss: 0.0246 - output_2_loss: 3.3720e-06\n",
            "Epoch 252: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0263 - output_1_loss: 0.0246 - output_2_loss: 3.3720e-06 - val_loss: 0.0673 - val_output_1_loss: 0.0657 - val_output_2_loss: 3.3562e-06 - lr: 0.0010\n",
            "Epoch 253/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0337 - output_1_loss: 0.0321 - output_2_loss: 3.3432e-06\n",
            "Epoch 253: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0365 - output_1_loss: 0.0348 - output_2_loss: 3.3422e-06 - val_loss: 0.0848 - val_output_1_loss: 0.0831 - val_output_2_loss: 3.3269e-06 - lr: 0.0010\n",
            "Epoch 254/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0423 - output_1_loss: 0.0406 - output_2_loss: 3.3135e-06\n",
            "Epoch 254: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0413 - output_1_loss: 0.0396 - output_2_loss: 3.3129e-06 - val_loss: 0.0693 - val_output_1_loss: 0.0677 - val_output_2_loss: 3.2974e-06 - lr: 0.0010\n",
            "Epoch 255/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0423 - output_1_loss: 0.0406 - output_2_loss: 3.2837e-06\n",
            "Epoch 255: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0423 - output_1_loss: 0.0406 - output_2_loss: 3.2837e-06 - val_loss: 0.0745 - val_output_1_loss: 0.0728 - val_output_2_loss: 3.2685e-06 - lr: 0.0010\n",
            "Epoch 256/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0238 - output_1_loss: 0.0222 - output_2_loss: 3.2551e-06\n",
            "Epoch 256: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0238 - output_1_loss: 0.0222 - output_2_loss: 3.2551e-06 - val_loss: 0.0770 - val_output_1_loss: 0.0754 - val_output_2_loss: 3.2403e-06 - lr: 0.0010\n",
            "Epoch 257/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0432 - output_1_loss: 0.0415 - output_2_loss: 3.2280e-06\n",
            "Epoch 257: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0420 - output_1_loss: 0.0403 - output_2_loss: 3.2268e-06 - val_loss: 0.0802 - val_output_1_loss: 0.0786 - val_output_2_loss: 3.2122e-06 - lr: 0.0010\n",
            "Epoch 258/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0360 - output_1_loss: 0.0343 - output_2_loss: 3.2001e-06\n",
            "Epoch 258: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0353 - output_1_loss: 0.0336 - output_2_loss: 3.1992e-06 - val_loss: 0.0823 - val_output_1_loss: 0.0807 - val_output_2_loss: 3.1841e-06 - lr: 0.0010\n",
            "Epoch 259/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0327 - output_1_loss: 0.0310 - output_2_loss: 3.1721e-06\n",
            "Epoch 259: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0316 - output_1_loss: 0.0300 - output_2_loss: 3.1714e-06 - val_loss: 0.0624 - val_output_1_loss: 0.0608 - val_output_2_loss: 3.1569e-06 - lr: 0.0010\n",
            "Epoch 260/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0379 - output_1_loss: 0.0363 - output_2_loss: 3.1445e-06\n",
            "Epoch 260: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0389 - output_1_loss: 0.0373 - output_2_loss: 3.1439e-06 - val_loss: 0.0647 - val_output_1_loss: 0.0631 - val_output_2_loss: 3.1300e-06 - lr: 0.0010\n",
            "Epoch 261/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0287 - output_1_loss: 0.0271 - output_2_loss: 3.1181e-06\n",
            "Epoch 261: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0294 - output_1_loss: 0.0278 - output_2_loss: 3.1173e-06 - val_loss: 0.0695 - val_output_1_loss: 0.0679 - val_output_2_loss: 3.1037e-06 - lr: 0.0010\n",
            "Epoch 262/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0264 - output_1_loss: 0.0248 - output_2_loss: 3.0917e-06\n",
            "Epoch 262: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0271 - output_1_loss: 0.0255 - output_2_loss: 3.0911e-06 - val_loss: 0.0653 - val_output_1_loss: 0.0637 - val_output_2_loss: 3.0771e-06 - lr: 0.0010\n",
            "Epoch 263/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0271 - output_1_loss: 0.0255 - output_2_loss: 3.0654e-06\n",
            "Epoch 263: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0270 - output_1_loss: 0.0254 - output_2_loss: 3.0649e-06 - val_loss: 0.0726 - val_output_1_loss: 0.0710 - val_output_2_loss: 3.0512e-06 - lr: 0.0010\n",
            "Epoch 264/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0408 - output_1_loss: 0.0392 - output_2_loss: 3.0393e-06\n",
            "Epoch 264: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0402 - output_1_loss: 0.0387 - output_2_loss: 3.0390e-06 - val_loss: 0.0683 - val_output_1_loss: 0.0668 - val_output_2_loss: 3.0255e-06 - lr: 0.0010\n",
            "Epoch 265/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0324 - output_1_loss: 0.0308 - output_2_loss: 3.0144e-06\n",
            "Epoch 265: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0313 - output_1_loss: 0.0297 - output_2_loss: 3.0134e-06 - val_loss: 0.0643 - val_output_1_loss: 0.0628 - val_output_2_loss: 3.0004e-06 - lr: 0.0010\n",
            "Epoch 266/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0346 - output_1_loss: 0.0331 - output_2_loss: 2.9898e-06\n",
            "Epoch 266: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0322 - output_1_loss: 0.0307 - output_2_loss: 2.9885e-06 - val_loss: 0.0822 - val_output_1_loss: 0.0807 - val_output_2_loss: 2.9755e-06 - lr: 0.0010\n",
            "Epoch 267/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0334 - output_1_loss: 0.0318 - output_2_loss: 2.9643e-06\n",
            "Epoch 267: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0319 - output_1_loss: 0.0303 - output_2_loss: 2.9636e-06 - val_loss: 0.0709 - val_output_1_loss: 0.0693 - val_output_2_loss: 2.9510e-06 - lr: 0.0010\n",
            "Epoch 268/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0264 - output_1_loss: 0.0249 - output_2_loss: 2.9402e-06\n",
            "Epoch 268: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0254 - output_1_loss: 0.0239 - output_2_loss: 2.9393e-06 - val_loss: 0.0668 - val_output_1_loss: 0.0652 - val_output_2_loss: 2.9266e-06 - lr: 0.0010\n",
            "Epoch 269/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0269 - output_1_loss: 0.0254 - output_2_loss: 2.9151e-06\n",
            "Epoch 269: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0269 - output_1_loss: 0.0254 - output_2_loss: 2.9151e-06 - val_loss: 0.0673 - val_output_1_loss: 0.0658 - val_output_2_loss: 2.9025e-06 - lr: 0.0010\n",
            "Epoch 270/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0275 - output_1_loss: 0.0260 - output_2_loss: 2.8914e-06\n",
            "Epoch 270: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0271 - output_1_loss: 0.0256 - output_2_loss: 2.8911e-06 - val_loss: 0.0664 - val_output_1_loss: 0.0649 - val_output_2_loss: 2.8787e-06 - lr: 0.0010\n",
            "Epoch 271/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0352 - output_1_loss: 0.0337 - output_2_loss: 2.8677e-06\n",
            "Epoch 271: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0352 - output_1_loss: 0.0337 - output_2_loss: 2.8675e-06 - val_loss: 0.0687 - val_output_1_loss: 0.0672 - val_output_2_loss: 2.8555e-06 - lr: 0.0010\n",
            "Epoch 272/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0310 - output_1_loss: 0.0295 - output_2_loss: 2.8447e-06\n",
            "Epoch 272: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 16ms/step - loss: 0.0312 - output_1_loss: 0.0297 - output_2_loss: 2.8444e-06 - val_loss: 0.0726 - val_output_1_loss: 0.0711 - val_output_2_loss: 2.8323e-06 - lr: 0.0010\n",
            "Epoch 273/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0307 - output_1_loss: 0.0292 - output_2_loss: 2.8215e-06\n",
            "Epoch 273: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0307 - output_1_loss: 0.0292 - output_2_loss: 2.8215e-06 - val_loss: 0.0748 - val_output_1_loss: 0.0733 - val_output_2_loss: 2.8096e-06 - lr: 0.0010\n",
            "Epoch 274/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0274 - output_1_loss: 0.0259 - output_2_loss: 2.7996e-06\n",
            "Epoch 274: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0262 - output_1_loss: 0.0247 - output_2_loss: 2.7989e-06 - val_loss: 0.0710 - val_output_1_loss: 0.0695 - val_output_2_loss: 2.7869e-06 - lr: 0.0010\n",
            "Epoch 275/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0416 - output_1_loss: 0.0401 - output_2_loss: 2.7762e-06\n",
            "Epoch 275: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0416 - output_1_loss: 0.0401 - output_2_loss: 2.7762e-06 - val_loss: 0.1017 - val_output_1_loss: 0.1002 - val_output_2_loss: 2.7648e-06 - lr: 0.0010\n",
            "Epoch 276/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0432 - output_1_loss: 0.0417 - output_2_loss: 2.7555e-06\n",
            "Epoch 276: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0407 - output_1_loss: 0.0392 - output_2_loss: 2.7544e-06 - val_loss: 0.0624 - val_output_1_loss: 0.0609 - val_output_2_loss: 2.7429e-06 - lr: 0.0010\n",
            "Epoch 277/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0409 - output_1_loss: 0.0394 - output_2_loss: 2.7338e-06\n",
            "Epoch 277: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0374 - output_1_loss: 0.0359 - output_2_loss: 2.7327e-06 - val_loss: 0.0681 - val_output_1_loss: 0.0666 - val_output_2_loss: 2.7212e-06 - lr: 0.0010\n",
            "Epoch 278/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0297 - output_1_loss: 0.0282 - output_2_loss: 2.7123e-06\n",
            "Epoch 278: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0315 - output_1_loss: 0.0300 - output_2_loss: 2.7108e-06 - val_loss: 0.0646 - val_output_1_loss: 0.0631 - val_output_2_loss: 2.6999e-06 - lr: 0.0010\n",
            "Epoch 279/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0262 - output_1_loss: 0.0248 - output_2_loss: 2.6901e-06\n",
            "Epoch 279: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0253 - output_1_loss: 0.0239 - output_2_loss: 2.6897e-06 - val_loss: 0.0688 - val_output_1_loss: 0.0674 - val_output_2_loss: 2.6789e-06 - lr: 0.0010\n",
            "Epoch 280/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0254 - output_1_loss: 0.0239 - output_2_loss: 2.6692e-06\n",
            "Epoch 280: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0246 - output_1_loss: 0.0231 - output_2_loss: 2.6688e-06 - val_loss: 0.0640 - val_output_1_loss: 0.0625 - val_output_2_loss: 2.6581e-06 - lr: 0.0010\n",
            "Epoch 281/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0253 - output_1_loss: 0.0239 - output_2_loss: 2.6485e-06\n",
            "Epoch 281: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0250 - output_1_loss: 0.0235 - output_2_loss: 2.6483e-06 - val_loss: 0.0766 - val_output_1_loss: 0.0752 - val_output_2_loss: 2.6372e-06 - lr: 0.0010\n",
            "Epoch 282/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0265 - output_1_loss: 0.0251 - output_2_loss: 2.6283e-06\n",
            "Epoch 282: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0256 - output_1_loss: 0.0242 - output_2_loss: 2.6276e-06 - val_loss: 0.0672 - val_output_1_loss: 0.0657 - val_output_2_loss: 2.6170e-06 - lr: 0.0010\n",
            "Epoch 283/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0257 - output_1_loss: 0.0243 - output_2_loss: 2.6074e-06\n",
            "Epoch 283: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0257 - output_1_loss: 0.0243 - output_2_loss: 2.6074e-06 - val_loss: 0.0741 - val_output_1_loss: 0.0727 - val_output_2_loss: 2.5970e-06 - lr: 0.0010\n",
            "Epoch 284/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0293 - output_1_loss: 0.0279 - output_2_loss: 2.5875e-06\n",
            "Epoch 284: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0293 - output_1_loss: 0.0279 - output_2_loss: 2.5875e-06 - val_loss: 0.0727 - val_output_1_loss: 0.0712 - val_output_2_loss: 2.5771e-06 - lr: 0.0010\n",
            "Epoch 285/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0362 - output_1_loss: 0.0348 - output_2_loss: 2.5685e-06\n",
            "Epoch 285: val_loss did not improve from 0.06150\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0354 - output_1_loss: 0.0340 - output_2_loss: 2.5677e-06 - val_loss: 0.0815 - val_output_1_loss: 0.0801 - val_output_2_loss: 2.5575e-06 - lr: 0.0010\n",
            "Epoch 286/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0450 - output_1_loss: 0.0436 - output_2_loss: 2.5489e-06\n",
            "Epoch 286: val_loss improved from 0.06150 to 0.06076, saving model to best_model.h5\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0460 - output_1_loss: 0.0446 - output_2_loss: 2.5483e-06 - val_loss: 0.0608 - val_output_1_loss: 0.0594 - val_output_2_loss: 2.5383e-06 - lr: 0.0010\n",
            "Epoch 287/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0308 - output_1_loss: 0.0295 - output_2_loss: 2.5294e-06\n",
            "Epoch 287: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0306 - output_1_loss: 0.0292 - output_2_loss: 2.5292e-06 - val_loss: 0.0752 - val_output_1_loss: 0.0738 - val_output_2_loss: 2.5191e-06 - lr: 0.0010\n",
            "Epoch 288/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0295 - output_1_loss: 0.0281 - output_2_loss: 2.5108e-06\n",
            "Epoch 288: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0285 - output_1_loss: 0.0271 - output_2_loss: 2.5101e-06 - val_loss: 0.0643 - val_output_1_loss: 0.0629 - val_output_2_loss: 2.5003e-06 - lr: 0.0010\n",
            "Epoch 289/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0292 - output_1_loss: 0.0278 - output_2_loss: 2.4920e-06\n",
            "Epoch 289: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0284 - output_1_loss: 0.0270 - output_2_loss: 2.4915e-06 - val_loss: 0.0789 - val_output_1_loss: 0.0775 - val_output_2_loss: 2.4816e-06 - lr: 0.0010\n",
            "Epoch 290/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0309 - output_1_loss: 0.0295 - output_2_loss: 2.4728e-06\n",
            "Epoch 290: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 0s 10ms/step - loss: 0.0309 - output_1_loss: 0.0295 - output_2_loss: 2.4728e-06 - val_loss: 0.0639 - val_output_1_loss: 0.0625 - val_output_2_loss: 2.4633e-06 - lr: 0.0010\n",
            "Epoch 291/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0226 - output_1_loss: 0.0212 - output_2_loss: 2.4546e-06\n",
            "Epoch 291: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0226 - output_1_loss: 0.0212 - output_2_loss: 2.4546e-06 - val_loss: 0.0661 - val_output_1_loss: 0.0648 - val_output_2_loss: 2.4452e-06 - lr: 0.0010\n",
            "Epoch 292/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0332 - output_1_loss: 0.0319 - output_2_loss: 2.4366e-06\n",
            "Epoch 292: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0332 - output_1_loss: 0.0319 - output_2_loss: 2.4366e-06 - val_loss: 0.0625 - val_output_1_loss: 0.0612 - val_output_2_loss: 2.4273e-06 - lr: 0.0010\n",
            "Epoch 293/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0302 - output_1_loss: 0.0288 - output_2_loss: 2.4189e-06\n",
            "Epoch 293: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0319 - output_1_loss: 0.0305 - output_2_loss: 2.4186e-06 - val_loss: 0.0735 - val_output_1_loss: 0.0722 - val_output_2_loss: 2.4097e-06 - lr: 0.0010\n",
            "Epoch 294/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0160 - output_1_loss: 0.0146 - output_2_loss: 2.4016e-06\n",
            "Epoch 294: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0156 - output_1_loss: 0.0143 - output_2_loss: 2.4013e-06 - val_loss: 0.0657 - val_output_1_loss: 0.0643 - val_output_2_loss: 2.3921e-06 - lr: 0.0010\n",
            "Epoch 295/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0349 - output_1_loss: 0.0335 - output_2_loss: 2.3842e-06\n",
            "Epoch 295: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0351 - output_1_loss: 0.0338 - output_2_loss: 2.3839e-06 - val_loss: 0.1354 - val_output_1_loss: 0.1341 - val_output_2_loss: 2.3746e-06 - lr: 0.0010\n",
            "Epoch 296/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0393 - output_1_loss: 0.0380 - output_2_loss: 2.3666e-06\n",
            "Epoch 296: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0393 - output_1_loss: 0.0380 - output_2_loss: 2.3663e-06 - val_loss: 0.1206 - val_output_1_loss: 0.1193 - val_output_2_loss: 2.3578e-06 - lr: 0.0010\n",
            "Epoch 297/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0355 - output_1_loss: 0.0341 - output_2_loss: 2.3498e-06\n",
            "Epoch 297: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0355 - output_1_loss: 0.0341 - output_2_loss: 2.3498e-06 - val_loss: 0.0667 - val_output_1_loss: 0.0653 - val_output_2_loss: 2.3407e-06 - lr: 0.0010\n",
            "Epoch 298/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0306 - output_1_loss: 0.0293 - output_2_loss: 2.3332e-06\n",
            "Epoch 298: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0298 - output_1_loss: 0.0285 - output_2_loss: 2.3327e-06 - val_loss: 0.0683 - val_output_1_loss: 0.0670 - val_output_2_loss: 2.3241e-06 - lr: 0.0010\n",
            "Epoch 299/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0272 - output_1_loss: 0.0259 - output_2_loss: 2.3166e-06\n",
            "Epoch 299: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0272 - output_1_loss: 0.0258 - output_2_loss: 2.3163e-06 - val_loss: 0.0649 - val_output_1_loss: 0.0635 - val_output_2_loss: 2.3077e-06 - lr: 0.0010\n",
            "Epoch 300/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0277 - output_1_loss: 0.0264 - output_2_loss: 2.3006e-06\n",
            "Epoch 300: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0343 - output_1_loss: 0.0330 - output_2_loss: 2.2998e-06 - val_loss: 0.0759 - val_output_1_loss: 0.0746 - val_output_2_loss: 2.2914e-06 - lr: 0.0010\n",
            "Epoch 301/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0186 - output_1_loss: 0.0173 - output_2_loss: 2.2838e-06\n",
            "Epoch 301: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0186 - output_1_loss: 0.0173 - output_2_loss: 2.2837e-06 - val_loss: 0.0731 - val_output_1_loss: 0.0718 - val_output_2_loss: 2.2754e-06 - lr: 0.0010\n",
            "Epoch 302/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0310 - output_1_loss: 0.0297 - output_2_loss: 2.2679e-06\n",
            "Epoch 302: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0307 - output_1_loss: 0.0294 - output_2_loss: 2.2677e-06 - val_loss: 0.0741 - val_output_1_loss: 0.0728 - val_output_2_loss: 2.2595e-06 - lr: 0.0010\n",
            "Epoch 303/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0381 - output_1_loss: 0.0368 - output_2_loss: 2.2528e-06\n",
            "Epoch 303: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0388 - output_1_loss: 0.0375 - output_2_loss: 2.2521e-06 - val_loss: 0.0644 - val_output_1_loss: 0.0632 - val_output_2_loss: 2.2437e-06 - lr: 0.0010\n",
            "Epoch 304/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0320 - output_1_loss: 0.0308 - output_2_loss: 2.2368e-06\n",
            "Epoch 304: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0334 - output_1_loss: 0.0321 - output_2_loss: 2.2362e-06 - val_loss: 0.0682 - val_output_1_loss: 0.0669 - val_output_2_loss: 2.2283e-06 - lr: 0.0010\n",
            "Epoch 305/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0185 - output_1_loss: 0.0172 - output_2_loss: 2.2211e-06\n",
            "Epoch 305: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0217 - output_1_loss: 0.0204 - output_2_loss: 2.2209e-06 - val_loss: 0.0887 - val_output_1_loss: 0.0875 - val_output_2_loss: 2.2130e-06 - lr: 0.0010\n",
            "Epoch 306/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0439 - output_1_loss: 0.0427 - output_2_loss: 2.2061e-06\n",
            "Epoch 306: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0439 - output_1_loss: 0.0426 - output_2_loss: 2.2058e-06 - val_loss: 0.0692 - val_output_1_loss: 0.0680 - val_output_2_loss: 2.1978e-06 - lr: 0.0010\n",
            "Epoch 307/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0311 - output_1_loss: 0.0298 - output_2_loss: 2.1908e-06\n",
            "Epoch 307: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0306 - output_1_loss: 0.0293 - output_2_loss: 2.1907e-06 - val_loss: 0.0748 - val_output_1_loss: 0.0735 - val_output_2_loss: 2.1829e-06 - lr: 0.0010\n",
            "Epoch 308/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0524 - output_1_loss: 0.0512 - output_2_loss: 2.1761e-06\n",
            "Epoch 308: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0514 - output_1_loss: 0.0502 - output_2_loss: 2.1758e-06 - val_loss: 0.0817 - val_output_1_loss: 0.0805 - val_output_2_loss: 2.1681e-06 - lr: 0.0010\n",
            "Epoch 309/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0307 - output_1_loss: 0.0294 - output_2_loss: 2.1612e-06\n",
            "Epoch 309: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0312 - output_1_loss: 0.0299 - output_2_loss: 2.1611e-06 - val_loss: 0.0996 - val_output_1_loss: 0.0984 - val_output_2_loss: 2.1536e-06 - lr: 0.0010\n",
            "Epoch 310/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0278 - output_1_loss: 0.0266 - output_2_loss: 2.1466e-06\n",
            "Epoch 310: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 16ms/step - loss: 0.0278 - output_1_loss: 0.0266 - output_2_loss: 2.1466e-06 - val_loss: 0.0640 - val_output_1_loss: 0.0627 - val_output_2_loss: 2.1391e-06 - lr: 0.0010\n",
            "Epoch 311/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0297 - output_1_loss: 0.0284 - output_2_loss: 2.1325e-06\n",
            "Epoch 311: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0296 - output_1_loss: 0.0284 - output_2_loss: 2.1323e-06 - val_loss: 0.0672 - val_output_1_loss: 0.0660 - val_output_2_loss: 2.1248e-06 - lr: 0.0010\n",
            "Epoch 312/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0230 - output_1_loss: 0.0218 - output_2_loss: 2.1188e-06\n",
            "Epoch 312: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 0s 10ms/step - loss: 0.0219 - output_1_loss: 0.0207 - output_2_loss: 2.1182e-06 - val_loss: 0.0639 - val_output_1_loss: 0.0627 - val_output_2_loss: 2.1107e-06 - lr: 0.0010\n",
            "Epoch 313/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0284 - output_1_loss: 0.0272 - output_2_loss: 2.1043e-06\n",
            "Epoch 313: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0274 - output_1_loss: 0.0262 - output_2_loss: 2.1040e-06 - val_loss: 0.0661 - val_output_1_loss: 0.0649 - val_output_2_loss: 2.0967e-06 - lr: 0.0010\n",
            "Epoch 314/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0268 - output_1_loss: 0.0256 - output_2_loss: 2.0910e-06\n",
            "Epoch 314: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0249 - output_1_loss: 0.0237 - output_2_loss: 2.0902e-06 - val_loss: 0.0650 - val_output_1_loss: 0.0638 - val_output_2_loss: 2.0830e-06 - lr: 0.0010\n",
            "Epoch 315/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0251 - output_1_loss: 0.0239 - output_2_loss: 2.0773e-06\n",
            "Epoch 315: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0291 - output_1_loss: 0.0278 - output_2_loss: 2.0766e-06 - val_loss: 0.0620 - val_output_1_loss: 0.0608 - val_output_2_loss: 2.0694e-06 - lr: 0.0010\n",
            "Epoch 316/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0234 - output_1_loss: 0.0222 - output_2_loss: 2.0632e-06\n",
            "Epoch 316: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0251 - output_1_loss: 0.0239 - output_2_loss: 2.0630e-06 - val_loss: 0.0662 - val_output_1_loss: 0.0650 - val_output_2_loss: 2.0560e-06 - lr: 0.0010\n",
            "Epoch 317/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0242 - output_1_loss: 0.0230 - output_2_loss: 2.0495e-06\n",
            "Epoch 317: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0242 - output_1_loss: 0.0230 - output_2_loss: 2.0495e-06 - val_loss: 0.0669 - val_output_1_loss: 0.0657 - val_output_2_loss: 2.0427e-06 - lr: 0.0010\n",
            "Epoch 318/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0346 - output_1_loss: 0.0334 - output_2_loss: 2.0368e-06\n",
            "Epoch 318: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0348 - output_1_loss: 0.0336 - output_2_loss: 2.0366e-06 - val_loss: 0.0764 - val_output_1_loss: 0.0752 - val_output_2_loss: 2.0295e-06 - lr: 0.0010\n",
            "Epoch 319/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0246 - output_1_loss: 0.0234 - output_2_loss: 2.0234e-06\n",
            "Epoch 319: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0247 - output_1_loss: 0.0236 - output_2_loss: 2.0233e-06 - val_loss: 0.0681 - val_output_1_loss: 0.0669 - val_output_2_loss: 2.0166e-06 - lr: 0.0010\n",
            "Epoch 320/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0278 - output_1_loss: 0.0266 - output_2_loss: 2.0105e-06\n",
            "Epoch 320: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0273 - output_1_loss: 0.0261 - output_2_loss: 2.0104e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0616 - val_output_2_loss: 2.0037e-06 - lr: 0.0010\n",
            "Epoch 321/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0270 - output_1_loss: 0.0258 - output_2_loss: 1.9977e-06\n",
            "Epoch 321: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0267 - output_1_loss: 0.0255 - output_2_loss: 1.9976e-06 - val_loss: 0.0655 - val_output_1_loss: 0.0643 - val_output_2_loss: 1.9910e-06 - lr: 0.0010\n",
            "Epoch 322/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0248 - output_1_loss: 0.0237 - output_2_loss: 1.9850e-06\n",
            "Epoch 322: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0248 - output_1_loss: 0.0237 - output_2_loss: 1.9850e-06 - val_loss: 0.0803 - val_output_1_loss: 0.0791 - val_output_2_loss: 1.9785e-06 - lr: 0.0010\n",
            "Epoch 323/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0244 - output_1_loss: 0.0232 - output_2_loss: 1.9728e-06\n",
            "Epoch 323: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0235 - output_1_loss: 0.0224 - output_2_loss: 1.9725e-06 - val_loss: 0.0625 - val_output_1_loss: 0.0613 - val_output_2_loss: 1.9660e-06 - lr: 0.0010\n",
            "Epoch 324/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0276 - output_1_loss: 0.0265 - output_2_loss: 1.9606e-06\n",
            "Epoch 324: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0292 - output_1_loss: 0.0280 - output_2_loss: 1.9601e-06 - val_loss: 0.0674 - val_output_1_loss: 0.0662 - val_output_2_loss: 1.9537e-06 - lr: 0.0010\n",
            "Epoch 325/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0249 - output_1_loss: 0.0238 - output_2_loss: 1.9482e-06\n",
            "Epoch 325: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0248 - output_1_loss: 0.0237 - output_2_loss: 1.9480e-06 - val_loss: 0.0786 - val_output_1_loss: 0.0774 - val_output_2_loss: 1.9417e-06 - lr: 0.0010\n",
            "Epoch 326/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0413 - output_1_loss: 0.0401 - output_2_loss: 1.9363e-06\n",
            "Epoch 326: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0405 - output_1_loss: 0.0394 - output_2_loss: 1.9359e-06 - val_loss: 0.1064 - val_output_1_loss: 0.1053 - val_output_2_loss: 1.9296e-06 - lr: 0.0010\n",
            "Epoch 327/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0344 - output_1_loss: 0.0332 - output_2_loss: 1.9245e-06\n",
            "Epoch 327: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0316 - output_1_loss: 0.0305 - output_2_loss: 1.9240e-06 - val_loss: 0.0632 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.9178e-06 - lr: 0.0010\n",
            "Epoch 328/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0280 - output_1_loss: 0.0268 - output_2_loss: 1.9127e-06\n",
            "Epoch 328: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0349 - output_1_loss: 0.0337 - output_2_loss: 1.9121e-06 - val_loss: 0.0682 - val_output_1_loss: 0.0671 - val_output_2_loss: 1.9061e-06 - lr: 0.0010\n",
            "Epoch 329/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0560 - output_1_loss: 0.0549 - output_2_loss: 1.9007e-06\n",
            "Epoch 329: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0550 - output_1_loss: 0.0539 - output_2_loss: 1.9006e-06 - val_loss: 0.0670 - val_output_1_loss: 0.0659 - val_output_2_loss: 1.8944e-06 - lr: 0.0010\n",
            "Epoch 330/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0242 - output_1_loss: 0.0231 - output_2_loss: 1.8892e-06\n",
            "Epoch 330: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0234 - output_1_loss: 0.0223 - output_2_loss: 1.8889e-06 - val_loss: 0.0668 - val_output_1_loss: 0.0657 - val_output_2_loss: 1.8830e-06 - lr: 0.0010\n",
            "Epoch 331/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0313 - output_1_loss: 0.0302 - output_2_loss: 1.8778e-06\n",
            "Epoch 331: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0304 - output_1_loss: 0.0292 - output_2_loss: 1.8776e-06 - val_loss: 0.0645 - val_output_1_loss: 0.0634 - val_output_2_loss: 1.8717e-06 - lr: 0.0010\n",
            "Epoch 332/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0290 - output_1_loss: 0.0279 - output_2_loss: 1.8665e-06\n",
            "Epoch 332: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0287 - output_1_loss: 0.0276 - output_2_loss: 1.8663e-06 - val_loss: 0.0632 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.8604e-06 - lr: 0.0010\n",
            "Epoch 333/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0279 - output_1_loss: 0.0268 - output_2_loss: 1.8551e-06\n",
            "Epoch 333: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0279 - output_1_loss: 0.0268 - output_2_loss: 1.8551e-06 - val_loss: 0.0949 - val_output_1_loss: 0.0938 - val_output_2_loss: 1.8493e-06 - lr: 0.0010\n",
            "Epoch 334/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0286 - output_1_loss: 0.0275 - output_2_loss: 1.8444e-06\n",
            "Epoch 334: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0280 - output_1_loss: 0.0269 - output_2_loss: 1.8441e-06 - val_loss: 0.0749 - val_output_1_loss: 0.0738 - val_output_2_loss: 1.8384e-06 - lr: 0.0010\n",
            "Epoch 335/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0182 - output_1_loss: 0.0171 - output_2_loss: 1.8337e-06\n",
            "Epoch 335: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0231 - output_1_loss: 0.0220 - output_2_loss: 1.8332e-06 - val_loss: 0.0719 - val_output_1_loss: 0.0708 - val_output_2_loss: 1.8275e-06 - lr: 0.0010\n",
            "Epoch 336/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0226 - output_1_loss: 0.0215 - output_2_loss: 1.8227e-06\n",
            "Epoch 336: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0269 - output_1_loss: 0.0258 - output_2_loss: 1.8224e-06 - val_loss: 0.0878 - val_output_1_loss: 0.0867 - val_output_2_loss: 1.8168e-06 - lr: 0.0010\n",
            "Epoch 337/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0287 - output_1_loss: 0.0276 - output_2_loss: 1.8119e-06\n",
            "Epoch 337: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0281 - output_1_loss: 0.0270 - output_2_loss: 1.8117e-06 - val_loss: 0.0653 - val_output_1_loss: 0.0642 - val_output_2_loss: 1.8061e-06 - lr: 0.0010\n",
            "Epoch 338/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0344 - output_1_loss: 0.0333 - output_2_loss: 1.8016e-06\n",
            "Epoch 338: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0313 - output_1_loss: 0.0303 - output_2_loss: 1.8012e-06 - val_loss: 0.0663 - val_output_1_loss: 0.0652 - val_output_2_loss: 1.7956e-06 - lr: 0.0010\n",
            "Epoch 339/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0233 - output_1_loss: 0.0222 - output_2_loss: 1.7908e-06\n",
            "Epoch 339: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - output_1_loss: 0.0219 - output_2_loss: 1.7906e-06 - val_loss: 0.0651 - val_output_1_loss: 0.0640 - val_output_2_loss: 1.7853e-06 - lr: 0.0010\n",
            "Epoch 340/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0228 - output_1_loss: 0.0218 - output_2_loss: 1.7807e-06\n",
            "Epoch 340: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0264 - output_1_loss: 0.0253 - output_2_loss: 1.7803e-06 - val_loss: 0.0644 - val_output_1_loss: 0.0633 - val_output_2_loss: 1.7750e-06 - lr: 0.0010\n",
            "Epoch 341/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0302 - output_1_loss: 0.0292 - output_2_loss: 1.7703e-06\n",
            "Epoch 341: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0320 - output_1_loss: 0.0309 - output_2_loss: 1.7701e-06 - val_loss: 0.0631 - val_output_1_loss: 0.0621 - val_output_2_loss: 1.7648e-06 - lr: 0.0010\n",
            "Epoch 342/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0282 - output_1_loss: 0.0271 - output_2_loss: 1.7603e-06\n",
            "Epoch 342: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0267 - output_1_loss: 0.0257 - output_2_loss: 1.7600e-06 - val_loss: 0.0635 - val_output_1_loss: 0.0625 - val_output_2_loss: 1.7548e-06 - lr: 0.0010\n",
            "Epoch 343/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0276 - output_1_loss: 0.0265 - output_2_loss: 1.7502e-06\n",
            "Epoch 343: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0276 - output_1_loss: 0.0266 - output_2_loss: 1.7500e-06 - val_loss: 0.0783 - val_output_1_loss: 0.0773 - val_output_2_loss: 1.7448e-06 - lr: 0.0010\n",
            "Epoch 344/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0295 - output_1_loss: 0.0284 - output_2_loss: 1.7404e-06\n",
            "Epoch 344: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0282 - output_1_loss: 0.0272 - output_2_loss: 1.7401e-06 - val_loss: 0.0643 - val_output_1_loss: 0.0632 - val_output_2_loss: 1.7349e-06 - lr: 0.0010\n",
            "Epoch 345/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0251 - output_1_loss: 0.0241 - output_2_loss: 1.7303e-06\n",
            "Epoch 345: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0251 - output_1_loss: 0.0241 - output_2_loss: 1.7303e-06 - val_loss: 0.0646 - val_output_1_loss: 0.0636 - val_output_2_loss: 1.7252e-06 - lr: 0.0010\n",
            "Epoch 346/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0223 - output_1_loss: 0.0212 - output_2_loss: 1.7208e-06\n",
            "Epoch 346: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0223 - output_1_loss: 0.0213 - output_2_loss: 1.7206e-06 - val_loss: 0.0609 - val_output_1_loss: 0.0599 - val_output_2_loss: 1.7156e-06 - lr: 0.0010\n",
            "Epoch 347/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0258 - output_1_loss: 0.0248 - output_2_loss: 1.7110e-06\n",
            "Epoch 347: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0258 - output_1_loss: 0.0248 - output_2_loss: 1.7110e-06 - val_loss: 0.0753 - val_output_1_loss: 0.0743 - val_output_2_loss: 1.7061e-06 - lr: 0.0010\n",
            "Epoch 348/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0297 - output_1_loss: 0.0286 - output_2_loss: 1.7017e-06\n",
            "Epoch 348: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0295 - output_1_loss: 0.0285 - output_2_loss: 1.7015e-06 - val_loss: 0.0707 - val_output_1_loss: 0.0697 - val_output_2_loss: 1.6966e-06 - lr: 0.0010\n",
            "Epoch 349/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0268 - output_1_loss: 0.0258 - output_2_loss: 1.6925e-06\n",
            "Epoch 349: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0283 - output_1_loss: 0.0273 - output_2_loss: 1.6922e-06 - val_loss: 0.0641 - val_output_1_loss: 0.0631 - val_output_2_loss: 1.6872e-06 - lr: 0.0010\n",
            "Epoch 350/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0186 - output_1_loss: 0.0175 - output_2_loss: 1.6830e-06\n",
            "Epoch 350: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0204 - output_1_loss: 0.0194 - output_2_loss: 1.6828e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6780e-06 - lr: 0.0010\n",
            "Epoch 351/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0200 - output_1_loss: 0.0190 - output_2_loss: 1.6777e-06\n",
            "Epoch 351: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0209 - output_1_loss: 0.0199 - output_2_loss: 1.6777e-06 - val_loss: 0.0635 - val_output_1_loss: 0.0625 - val_output_2_loss: 1.6771e-06 - lr: 1.0000e-04\n",
            "Epoch 352/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0223 - output_1_loss: 0.0213 - output_2_loss: 1.6769e-06\n",
            "Epoch 352: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0212 - output_1_loss: 0.0202 - output_2_loss: 1.6768e-06 - val_loss: 0.0645 - val_output_1_loss: 0.0635 - val_output_2_loss: 1.6762e-06 - lr: 1.0000e-04\n",
            "Epoch 353/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0151 - output_1_loss: 0.0141 - output_2_loss: 1.6758e-06\n",
            "Epoch 353: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - output_1_loss: 0.0141 - output_2_loss: 1.6758e-06 - val_loss: 0.0632 - val_output_1_loss: 0.0622 - val_output_2_loss: 1.6752e-06 - lr: 1.0000e-04\n",
            "Epoch 354/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0218 - output_1_loss: 0.0208 - output_2_loss: 1.6749e-06\n",
            "Epoch 354: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0233 - output_1_loss: 0.0223 - output_2_loss: 1.6749e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6743e-06 - lr: 1.0000e-04\n",
            "Epoch 355/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0215 - output_1_loss: 0.0204 - output_2_loss: 1.6741e-06\n",
            "Epoch 355: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0234 - output_1_loss: 0.0224 - output_2_loss: 1.6740e-06 - val_loss: 0.0630 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.6734e-06 - lr: 1.0000e-04\n",
            "Epoch 356/500\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0211 - output_1_loss: 0.0201 - output_2_loss: 1.6731e-06\n",
            "Epoch 356: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0206 - output_1_loss: 0.0195 - output_2_loss: 1.6731e-06 - val_loss: 0.0643 - val_output_1_loss: 0.0633 - val_output_2_loss: 1.6725e-06 - lr: 1.0000e-04\n",
            "Epoch 357/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0157 - output_1_loss: 0.0147 - output_2_loss: 1.6722e-06\n",
            "Epoch 357: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0160 - output_1_loss: 0.0150 - output_2_loss: 1.6722e-06 - val_loss: 0.0632 - val_output_1_loss: 0.0621 - val_output_2_loss: 1.6716e-06 - lr: 1.0000e-04\n",
            "Epoch 358/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0267 - output_1_loss: 0.0257 - output_2_loss: 1.6712e-06\n",
            "Epoch 358: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0280 - output_1_loss: 0.0270 - output_2_loss: 1.6712e-06 - val_loss: 0.0634 - val_output_1_loss: 0.0623 - val_output_2_loss: 1.6706e-06 - lr: 1.0000e-04\n",
            "Epoch 359/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0166 - output_1_loss: 0.0156 - output_2_loss: 1.6703e-06\n",
            "Epoch 359: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0200 - output_1_loss: 0.0190 - output_2_loss: 1.6703e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6697e-06 - lr: 1.0000e-04\n",
            "Epoch 360/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0178 - output_1_loss: 0.0168 - output_2_loss: 1.6694e-06\n",
            "Epoch 360: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0178 - output_1_loss: 0.0168 - output_2_loss: 1.6694e-06 - val_loss: 0.0638 - val_output_1_loss: 0.0628 - val_output_2_loss: 1.6688e-06 - lr: 1.0000e-04\n",
            "Epoch 361/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0154 - output_1_loss: 0.0144 - output_2_loss: 1.6685e-06\n",
            "Epoch 361: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0154 - output_1_loss: 0.0144 - output_2_loss: 1.6685e-06 - val_loss: 0.0625 - val_output_1_loss: 0.0615 - val_output_2_loss: 1.6679e-06 - lr: 1.0000e-04\n",
            "Epoch 362/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0286 - output_1_loss: 0.0275 - output_2_loss: 1.6675e-06\n",
            "Epoch 362: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0275 - output_1_loss: 0.0265 - output_2_loss: 1.6675e-06 - val_loss: 0.0631 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.6669e-06 - lr: 1.0000e-04\n",
            "Epoch 363/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0195 - output_1_loss: 0.0185 - output_2_loss: 1.6666e-06\n",
            "Epoch 363: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0180 - output_1_loss: 0.0170 - output_2_loss: 1.6666e-06 - val_loss: 0.0639 - val_output_1_loss: 0.0629 - val_output_2_loss: 1.6660e-06 - lr: 1.0000e-04\n",
            "Epoch 364/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0161 - output_1_loss: 0.0151 - output_2_loss: 1.6658e-06\n",
            "Epoch 364: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0152 - output_1_loss: 0.0141 - output_2_loss: 1.6657e-06 - val_loss: 0.0622 - val_output_1_loss: 0.0612 - val_output_2_loss: 1.6651e-06 - lr: 1.0000e-04\n",
            "Epoch 365/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0271 - output_1_loss: 0.0261 - output_2_loss: 1.6647e-06\n",
            "Epoch 365: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0274 - output_1_loss: 0.0264 - output_2_loss: 1.6647e-06 - val_loss: 0.0616 - val_output_1_loss: 0.0606 - val_output_2_loss: 1.6642e-06 - lr: 1.0000e-04\n",
            "Epoch 366/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0224 - output_1_loss: 0.0213 - output_2_loss: 1.6640e-06\n",
            "Epoch 366: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0216 - output_1_loss: 0.0206 - output_2_loss: 1.6639e-06 - val_loss: 0.0624 - val_output_1_loss: 0.0614 - val_output_2_loss: 1.6632e-06 - lr: 1.0000e-04\n",
            "Epoch 367/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0171 - output_1_loss: 0.0161 - output_2_loss: 1.6629e-06\n",
            "Epoch 367: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0171 - output_1_loss: 0.0161 - output_2_loss: 1.6629e-06 - val_loss: 0.0660 - val_output_1_loss: 0.0650 - val_output_2_loss: 1.6623e-06 - lr: 1.0000e-04\n",
            "Epoch 368/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0150 - output_1_loss: 0.0140 - output_2_loss: 1.6620e-06\n",
            "Epoch 368: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0150 - output_1_loss: 0.0140 - output_2_loss: 1.6620e-06 - val_loss: 0.0623 - val_output_1_loss: 0.0613 - val_output_2_loss: 1.6614e-06 - lr: 1.0000e-04\n",
            "Epoch 369/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0208 - output_1_loss: 0.0198 - output_2_loss: 1.6611e-06\n",
            "Epoch 369: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0208 - output_1_loss: 0.0198 - output_2_loss: 1.6610e-06 - val_loss: 0.0650 - val_output_1_loss: 0.0640 - val_output_2_loss: 1.6604e-06 - lr: 1.0000e-04\n",
            "Epoch 370/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0233 - output_1_loss: 0.0223 - output_2_loss: 1.6601e-06\n",
            "Epoch 370: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0229 - output_1_loss: 0.0218 - output_2_loss: 1.6601e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6595e-06 - lr: 1.0000e-04\n",
            "Epoch 371/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0218 - output_1_loss: 0.0208 - output_2_loss: 1.6593e-06\n",
            "Epoch 371: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0266 - output_1_loss: 0.0256 - output_2_loss: 1.6593e-06 - val_loss: 0.0630 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.6586e-06 - lr: 1.0000e-04\n",
            "Epoch 372/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0182 - output_1_loss: 0.0172 - output_2_loss: 1.6582e-06\n",
            "Epoch 372: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0182 - output_1_loss: 0.0172 - output_2_loss: 1.6582e-06 - val_loss: 0.0631 - val_output_1_loss: 0.0621 - val_output_2_loss: 1.6577e-06 - lr: 1.0000e-04\n",
            "Epoch 373/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0189 - output_1_loss: 0.0179 - output_2_loss: 1.6574e-06\n",
            "Epoch 373: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 16ms/step - loss: 0.0187 - output_1_loss: 0.0177 - output_2_loss: 1.6573e-06 - val_loss: 0.0623 - val_output_1_loss: 0.0613 - val_output_2_loss: 1.6567e-06 - lr: 1.0000e-04\n",
            "Epoch 374/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0197 - output_1_loss: 0.0187 - output_2_loss: 1.6565e-06\n",
            "Epoch 374: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0197 - output_1_loss: 0.0187 - output_2_loss: 1.6565e-06 - val_loss: 0.0626 - val_output_1_loss: 0.0616 - val_output_2_loss: 1.6558e-06 - lr: 1.0000e-04\n",
            "Epoch 375/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0268 - output_1_loss: 0.0258 - output_2_loss: 1.6555e-06\n",
            "Epoch 375: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0268 - output_1_loss: 0.0258 - output_2_loss: 1.6555e-06 - val_loss: 0.0777 - val_output_1_loss: 0.0767 - val_output_2_loss: 1.6549e-06 - lr: 1.0000e-04\n",
            "Epoch 376/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0176 - output_1_loss: 0.0166 - output_2_loss: 1.6545e-06\n",
            "Epoch 376: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0173 - output_1_loss: 0.0163 - output_2_loss: 1.6545e-06 - val_loss: 0.0626 - val_output_1_loss: 0.0616 - val_output_2_loss: 1.6540e-06 - lr: 1.0000e-04\n",
            "Epoch 377/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0207 - output_1_loss: 0.0197 - output_2_loss: 1.6537e-06\n",
            "Epoch 377: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0205 - output_1_loss: 0.0195 - output_2_loss: 1.6537e-06 - val_loss: 0.0648 - val_output_1_loss: 0.0638 - val_output_2_loss: 1.6530e-06 - lr: 1.0000e-04\n",
            "Epoch 378/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0228 - output_1_loss: 0.0218 - output_2_loss: 1.6527e-06\n",
            "Epoch 378: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0216 - output_1_loss: 0.0206 - output_2_loss: 1.6527e-06 - val_loss: 0.0644 - val_output_1_loss: 0.0634 - val_output_2_loss: 1.6521e-06 - lr: 1.0000e-04\n",
            "Epoch 379/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0246 - output_1_loss: 0.0236 - output_2_loss: 1.6518e-06\n",
            "Epoch 379: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0232 - output_1_loss: 0.0222 - output_2_loss: 1.6518e-06 - val_loss: 0.0615 - val_output_1_loss: 0.0605 - val_output_2_loss: 1.6512e-06 - lr: 1.0000e-04\n",
            "Epoch 380/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0194 - output_1_loss: 0.0184 - output_2_loss: 1.6510e-06\n",
            "Epoch 380: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0194 - output_1_loss: 0.0184 - output_2_loss: 1.6510e-06 - val_loss: 0.0623 - val_output_1_loss: 0.0613 - val_output_2_loss: 1.6502e-06 - lr: 1.0000e-04\n",
            "Epoch 381/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0153 - output_1_loss: 0.0143 - output_2_loss: 1.6499e-06\n",
            "Epoch 381: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0140 - output_1_loss: 0.0131 - output_2_loss: 1.6499e-06 - val_loss: 0.0643 - val_output_1_loss: 0.0633 - val_output_2_loss: 1.6493e-06 - lr: 1.0000e-04\n",
            "Epoch 382/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0243 - output_1_loss: 0.0233 - output_2_loss: 1.6490e-06\n",
            "Epoch 382: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0243 - output_1_loss: 0.0233 - output_2_loss: 1.6490e-06 - val_loss: 0.0620 - val_output_1_loss: 0.0610 - val_output_2_loss: 1.6484e-06 - lr: 1.0000e-04\n",
            "Epoch 383/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0212 - output_1_loss: 0.0202 - output_2_loss: 1.6481e-06\n",
            "Epoch 383: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0216 - output_1_loss: 0.0206 - output_2_loss: 1.6481e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6475e-06 - lr: 1.0000e-04\n",
            "Epoch 384/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0216 - output_1_loss: 0.0206 - output_2_loss: 1.6472e-06\n",
            "Epoch 384: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0216 - output_1_loss: 0.0206 - output_2_loss: 1.6472e-06 - val_loss: 0.0621 - val_output_1_loss: 0.0611 - val_output_2_loss: 1.6466e-06 - lr: 1.0000e-04\n",
            "Epoch 385/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0182 - output_1_loss: 0.0172 - output_2_loss: 1.6463e-06\n",
            "Epoch 385: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0172 - output_1_loss: 0.0162 - output_2_loss: 1.6463e-06 - val_loss: 0.0630 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.6456e-06 - lr: 1.0000e-04\n",
            "Epoch 386/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0258 - output_1_loss: 0.0248 - output_2_loss: 1.6453e-06\n",
            "Epoch 386: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0246 - output_1_loss: 0.0236 - output_2_loss: 1.6453e-06 - val_loss: 0.0672 - val_output_1_loss: 0.0662 - val_output_2_loss: 1.6447e-06 - lr: 1.0000e-04\n",
            "Epoch 387/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0184 - output_1_loss: 0.0174 - output_2_loss: 1.6444e-06\n",
            "Epoch 387: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0195 - output_1_loss: 0.0185 - output_2_loss: 1.6444e-06 - val_loss: 0.0634 - val_output_1_loss: 0.0625 - val_output_2_loss: 1.6438e-06 - lr: 1.0000e-04\n",
            "Epoch 388/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0148 - output_1_loss: 0.0138 - output_2_loss: 1.6434e-06\n",
            "Epoch 388: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0148 - output_1_loss: 0.0138 - output_2_loss: 1.6434e-06 - val_loss: 0.0625 - val_output_1_loss: 0.0616 - val_output_2_loss: 1.6429e-06 - lr: 1.0000e-04\n",
            "Epoch 389/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0213 - output_1_loss: 0.0203 - output_2_loss: 1.6425e-06\n",
            "Epoch 389: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0216 - output_1_loss: 0.0206 - output_2_loss: 1.6425e-06 - val_loss: 0.0633 - val_output_1_loss: 0.0623 - val_output_2_loss: 1.6420e-06 - lr: 1.0000e-04\n",
            "Epoch 390/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0235 - output_1_loss: 0.0225 - output_2_loss: 1.6418e-06\n",
            "Epoch 390: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0221 - output_1_loss: 0.0212 - output_2_loss: 1.6417e-06 - val_loss: 0.0683 - val_output_1_loss: 0.0673 - val_output_2_loss: 1.6410e-06 - lr: 1.0000e-04\n",
            "Epoch 391/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0178 - output_1_loss: 0.0168 - output_2_loss: 1.6407e-06\n",
            "Epoch 391: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0175 - output_1_loss: 0.0165 - output_2_loss: 1.6407e-06 - val_loss: 0.0649 - val_output_1_loss: 0.0639 - val_output_2_loss: 1.6401e-06 - lr: 1.0000e-04\n",
            "Epoch 392/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0259 - output_1_loss: 0.0249 - output_2_loss: 1.6397e-06\n",
            "Epoch 392: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0259 - output_1_loss: 0.0249 - output_2_loss: 1.6397e-06 - val_loss: 0.0639 - val_output_1_loss: 0.0630 - val_output_2_loss: 1.6392e-06 - lr: 1.0000e-04\n",
            "Epoch 393/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0234 - output_1_loss: 0.0224 - output_2_loss: 1.6390e-06\n",
            "Epoch 393: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0223 - output_1_loss: 0.0213 - output_2_loss: 1.6389e-06 - val_loss: 0.0724 - val_output_1_loss: 0.0714 - val_output_2_loss: 1.6383e-06 - lr: 1.0000e-04\n",
            "Epoch 394/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0194 - output_1_loss: 0.0184 - output_2_loss: 1.6380e-06\n",
            "Epoch 394: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.0191 - output_1_loss: 0.0181 - output_2_loss: 1.6380e-06 - val_loss: 0.0646 - val_output_1_loss: 0.0636 - val_output_2_loss: 1.6374e-06 - lr: 1.0000e-04\n",
            "Epoch 395/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0211 - output_1_loss: 0.0201 - output_2_loss: 1.6370e-06\n",
            "Epoch 395: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 16ms/step - loss: 0.0200 - output_1_loss: 0.0190 - output_2_loss: 1.6370e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6364e-06 - lr: 1.0000e-04\n",
            "Epoch 396/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0184 - output_1_loss: 0.0175 - output_2_loss: 1.6362e-06\n",
            "Epoch 396: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0199 - output_1_loss: 0.0190 - output_2_loss: 1.6361e-06 - val_loss: 0.0626 - val_output_1_loss: 0.0616 - val_output_2_loss: 1.6355e-06 - lr: 1.0000e-04\n",
            "Epoch 397/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0241 - output_1_loss: 0.0231 - output_2_loss: 1.6352e-06\n",
            "Epoch 397: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.0241 - output_1_loss: 0.0231 - output_2_loss: 1.6352e-06 - val_loss: 0.0635 - val_output_1_loss: 0.0625 - val_output_2_loss: 1.6346e-06 - lr: 1.0000e-04\n",
            "Epoch 398/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0164 - output_1_loss: 0.0154 - output_2_loss: 1.6343e-06\n",
            "Epoch 398: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0159 - output_1_loss: 0.0149 - output_2_loss: 1.6342e-06 - val_loss: 0.0653 - val_output_1_loss: 0.0643 - val_output_2_loss: 1.6337e-06 - lr: 1.0000e-04\n",
            "Epoch 399/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0253 - output_1_loss: 0.0243 - output_2_loss: 1.6334e-06\n",
            "Epoch 399: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0245 - output_1_loss: 0.0235 - output_2_loss: 1.6334e-06 - val_loss: 0.0620 - val_output_1_loss: 0.0610 - val_output_2_loss: 1.6328e-06 - lr: 1.0000e-04\n",
            "Epoch 400/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0202 - output_1_loss: 0.0193 - output_2_loss: 1.6324e-06\n",
            "Epoch 400: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0199 - output_1_loss: 0.0189 - output_2_loss: 1.6324e-06 - val_loss: 0.0617 - val_output_1_loss: 0.0607 - val_output_2_loss: 1.6319e-06 - lr: 1.0000e-04\n",
            "Epoch 401/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0174 - output_1_loss: 0.0165 - output_2_loss: 1.6321e-06\n",
            "Epoch 401: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0162 - output_1_loss: 0.0152 - output_2_loss: 1.6321e-06 - val_loss: 0.0622 - val_output_1_loss: 0.0612 - val_output_2_loss: 1.6318e-06 - lr: 1.0000e-05\n",
            "Epoch 402/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0230 - output_1_loss: 0.0220 - output_2_loss: 1.6321e-06\n",
            "Epoch 402: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0226 - output_1_loss: 0.0216 - output_2_loss: 1.6320e-06 - val_loss: 0.0639 - val_output_1_loss: 0.0629 - val_output_2_loss: 1.6317e-06 - lr: 1.0000e-05\n",
            "Epoch 403/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0135 - output_1_loss: 0.0125 - output_2_loss: 1.6318e-06\n",
            "Epoch 403: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0135 - output_1_loss: 0.0125 - output_2_loss: 1.6318e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6316e-06 - lr: 1.0000e-05\n",
            "Epoch 404/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0269 - output_1_loss: 0.0259 - output_2_loss: 1.6316e-06\n",
            "Epoch 404: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0259 - output_1_loss: 0.0249 - output_2_loss: 1.6316e-06 - val_loss: 0.0625 - val_output_1_loss: 0.0615 - val_output_2_loss: 1.6315e-06 - lr: 1.0000e-05\n",
            "Epoch 405/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0203 - output_1_loss: 0.0193 - output_2_loss: 1.6316e-06\n",
            "Epoch 405: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0210 - output_1_loss: 0.0200 - output_2_loss: 1.6316e-06 - val_loss: 0.0631 - val_output_1_loss: 0.0621 - val_output_2_loss: 1.6314e-06 - lr: 1.0000e-05\n",
            "Epoch 406/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0076 - output_1_loss: 0.0066 - output_2_loss: 1.6316e-06\n",
            "Epoch 406: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0110 - output_1_loss: 0.0100 - output_2_loss: 1.6315e-06 - val_loss: 0.0633 - val_output_1_loss: 0.0623 - val_output_2_loss: 1.6313e-06 - lr: 1.0000e-05\n",
            "Epoch 407/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0251 - output_1_loss: 0.0242 - output_2_loss: 1.6315e-06\n",
            "Epoch 407: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0237 - output_1_loss: 0.0227 - output_2_loss: 1.6314e-06 - val_loss: 0.0633 - val_output_1_loss: 0.0623 - val_output_2_loss: 1.6312e-06 - lr: 1.0000e-05\n",
            "Epoch 408/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0190 - output_1_loss: 0.0180 - output_2_loss: 1.6314e-06\n",
            "Epoch 408: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0186 - output_1_loss: 0.0177 - output_2_loss: 1.6314e-06 - val_loss: 0.0636 - val_output_1_loss: 0.0626 - val_output_2_loss: 1.6312e-06 - lr: 1.0000e-05\n",
            "Epoch 409/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0236 - output_1_loss: 0.0226 - output_2_loss: 1.6312e-06\n",
            "Epoch 409: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0224 - output_1_loss: 0.0214 - output_2_loss: 1.6312e-06 - val_loss: 0.0623 - val_output_1_loss: 0.0613 - val_output_2_loss: 1.6311e-06 - lr: 1.0000e-05\n",
            "Epoch 410/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0187 - output_1_loss: 0.0177 - output_2_loss: 1.6312e-06\n",
            "Epoch 410: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0174 - output_1_loss: 0.0164 - output_2_loss: 1.6312e-06 - val_loss: 0.0623 - val_output_1_loss: 0.0613 - val_output_2_loss: 1.6310e-06 - lr: 1.0000e-05\n",
            "Epoch 411/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0233 - output_1_loss: 0.0224 - output_2_loss: 1.6312e-06\n",
            "Epoch 411: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0229 - output_1_loss: 0.0219 - output_2_loss: 1.6312e-06 - val_loss: 0.0624 - val_output_1_loss: 0.0615 - val_output_2_loss: 1.6309e-06 - lr: 1.0000e-05\n",
            "Epoch 412/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0126 - output_1_loss: 0.0116 - output_2_loss: 1.6309e-06\n",
            "Epoch 412: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0126 - output_1_loss: 0.0116 - output_2_loss: 1.6309e-06 - val_loss: 0.0634 - val_output_1_loss: 0.0624 - val_output_2_loss: 1.6308e-06 - lr: 1.0000e-05\n",
            "Epoch 413/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0213 - output_1_loss: 0.0204 - output_2_loss: 1.6308e-06\n",
            "Epoch 413: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0221 - output_1_loss: 0.0211 - output_2_loss: 1.6309e-06 - val_loss: 0.0631 - val_output_1_loss: 0.0621 - val_output_2_loss: 1.6307e-06 - lr: 1.0000e-05\n",
            "Epoch 414/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0192 - output_1_loss: 0.0182 - output_2_loss: 1.6308e-06\n",
            "Epoch 414: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0192 - output_1_loss: 0.0182 - output_2_loss: 1.6308e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6306e-06 - lr: 1.0000e-05\n",
            "Epoch 415/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0198 - output_1_loss: 0.0188 - output_2_loss: 1.6306e-06\n",
            "Epoch 415: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0191 - output_1_loss: 0.0181 - output_2_loss: 1.6307e-06 - val_loss: 0.0641 - val_output_1_loss: 0.0631 - val_output_2_loss: 1.6305e-06 - lr: 1.0000e-05\n",
            "Epoch 416/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0216 - output_1_loss: 0.0207 - output_2_loss: 1.6306e-06\n",
            "Epoch 416: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0219 - output_1_loss: 0.0209 - output_2_loss: 1.6306e-06 - val_loss: 0.0633 - val_output_1_loss: 0.0623 - val_output_2_loss: 1.6304e-06 - lr: 1.0000e-05\n",
            "Epoch 417/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0196 - output_1_loss: 0.0186 - output_2_loss: 1.6304e-06\n",
            "Epoch 417: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 16ms/step - loss: 0.0195 - output_1_loss: 0.0185 - output_2_loss: 1.6304e-06 - val_loss: 0.0638 - val_output_1_loss: 0.0629 - val_output_2_loss: 1.6303e-06 - lr: 1.0000e-05\n",
            "Epoch 418/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0189 - output_1_loss: 0.0179 - output_2_loss: 1.6305e-06\n",
            "Epoch 418: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0189 - output_1_loss: 0.0179 - output_2_loss: 1.6305e-06 - val_loss: 0.0623 - val_output_1_loss: 0.0614 - val_output_2_loss: 1.6302e-06 - lr: 1.0000e-05\n",
            "Epoch 419/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0189 - output_1_loss: 0.0180 - output_2_loss: 1.6303e-06\n",
            "Epoch 419: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 16ms/step - loss: 0.0186 - output_1_loss: 0.0176 - output_2_loss: 1.6303e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6302e-06 - lr: 1.0000e-05\n",
            "Epoch 420/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0201 - output_1_loss: 0.0191 - output_2_loss: 1.6303e-06\n",
            "Epoch 420: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0197 - output_1_loss: 0.0187 - output_2_loss: 1.6303e-06 - val_loss: 0.0630 - val_output_1_loss: 0.0621 - val_output_2_loss: 1.6301e-06 - lr: 1.0000e-05\n",
            "Epoch 421/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0211 - output_1_loss: 0.0202 - output_2_loss: 1.6300e-06\n",
            "Epoch 421: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 16ms/step - loss: 0.0201 - output_1_loss: 0.0191 - output_2_loss: 1.6301e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6300e-06 - lr: 1.0000e-05\n",
            "Epoch 422/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0247 - output_1_loss: 0.0237 - output_2_loss: 1.6301e-06\n",
            "Epoch 422: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0241 - output_1_loss: 0.0231 - output_2_loss: 1.6301e-06 - val_loss: 0.0623 - val_output_1_loss: 0.0613 - val_output_2_loss: 1.6299e-06 - lr: 1.0000e-05\n",
            "Epoch 423/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0126 - output_1_loss: 0.0116 - output_2_loss: 1.6300e-06\n",
            "Epoch 423: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0132 - output_1_loss: 0.0122 - output_2_loss: 1.6300e-06 - val_loss: 0.0630 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.6298e-06 - lr: 1.0000e-05\n",
            "Epoch 424/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0263 - output_1_loss: 0.0253 - output_2_loss: 1.6299e-06\n",
            "Epoch 424: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0258 - output_1_loss: 0.0248 - output_2_loss: 1.6299e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6297e-06 - lr: 1.0000e-05\n",
            "Epoch 425/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0179 - output_1_loss: 0.0169 - output_2_loss: 1.6297e-06\n",
            "Epoch 425: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0179 - output_1_loss: 0.0169 - output_2_loss: 1.6297e-06 - val_loss: 0.0626 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6296e-06 - lr: 1.0000e-05\n",
            "Epoch 426/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0151 - output_1_loss: 0.0142 - output_2_loss: 1.6297e-06\n",
            "Epoch 426: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0144 - output_1_loss: 0.0134 - output_2_loss: 1.6297e-06 - val_loss: 0.0636 - val_output_1_loss: 0.0627 - val_output_2_loss: 1.6295e-06 - lr: 1.0000e-05\n",
            "Epoch 427/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0210 - output_1_loss: 0.0200 - output_2_loss: 1.6295e-06\n",
            "Epoch 427: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0210 - output_1_loss: 0.0200 - output_2_loss: 1.6295e-06 - val_loss: 0.0644 - val_output_1_loss: 0.0634 - val_output_2_loss: 1.6294e-06 - lr: 1.0000e-05\n",
            "Epoch 428/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0258 - output_1_loss: 0.0248 - output_2_loss: 1.6296e-06\n",
            "Epoch 428: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0249 - output_1_loss: 0.0239 - output_2_loss: 1.6296e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6293e-06 - lr: 1.0000e-05\n",
            "Epoch 429/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0212 - output_1_loss: 0.0203 - output_2_loss: 1.6294e-06\n",
            "Epoch 429: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0212 - output_1_loss: 0.0203 - output_2_loss: 1.6294e-06 - val_loss: 0.0631 - val_output_1_loss: 0.0621 - val_output_2_loss: 1.6292e-06 - lr: 1.0000e-05\n",
            "Epoch 430/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0186 - output_1_loss: 0.0177 - output_2_loss: 1.6294e-06\n",
            "Epoch 430: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0186 - output_1_loss: 0.0177 - output_2_loss: 1.6294e-06 - val_loss: 0.0621 - val_output_1_loss: 0.0611 - val_output_2_loss: 1.6292e-06 - lr: 1.0000e-05\n",
            "Epoch 431/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0176 - output_1_loss: 0.0166 - output_2_loss: 1.6292e-06\n",
            "Epoch 431: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0176 - output_1_loss: 0.0166 - output_2_loss: 1.6292e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6291e-06 - lr: 1.0000e-05\n",
            "Epoch 432/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0211 - output_1_loss: 0.0201 - output_2_loss: 1.6291e-06\n",
            "Epoch 432: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0211 - output_1_loss: 0.0201 - output_2_loss: 1.6291e-06 - val_loss: 0.0622 - val_output_1_loss: 0.0612 - val_output_2_loss: 1.6290e-06 - lr: 1.0000e-05\n",
            "Epoch 433/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0186 - output_1_loss: 0.0177 - output_2_loss: 1.6292e-06\n",
            "Epoch 433: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0185 - output_1_loss: 0.0176 - output_2_loss: 1.6291e-06 - val_loss: 0.0632 - val_output_1_loss: 0.0623 - val_output_2_loss: 1.6289e-06 - lr: 1.0000e-05\n",
            "Epoch 434/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0169 - output_1_loss: 0.0159 - output_2_loss: 1.6290e-06\n",
            "Epoch 434: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0161 - output_1_loss: 0.0151 - output_2_loss: 1.6290e-06 - val_loss: 0.0638 - val_output_1_loss: 0.0628 - val_output_2_loss: 1.6288e-06 - lr: 1.0000e-05\n",
            "Epoch 435/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0220 - output_1_loss: 0.0211 - output_2_loss: 1.6289e-06\n",
            "Epoch 435: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0220 - output_1_loss: 0.0211 - output_2_loss: 1.6289e-06 - val_loss: 0.0630 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.6287e-06 - lr: 1.0000e-05\n",
            "Epoch 436/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0147 - output_1_loss: 0.0137 - output_2_loss: 1.6288e-06\n",
            "Epoch 436: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0144 - output_1_loss: 0.0134 - output_2_loss: 1.6288e-06 - val_loss: 0.0634 - val_output_1_loss: 0.0625 - val_output_2_loss: 1.6286e-06 - lr: 1.0000e-05\n",
            "Epoch 437/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0200 - output_1_loss: 0.0190 - output_2_loss: 1.6286e-06\n",
            "Epoch 437: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0200 - output_1_loss: 0.0190 - output_2_loss: 1.6286e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6285e-06 - lr: 1.0000e-05\n",
            "Epoch 438/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0191 - output_1_loss: 0.0182 - output_2_loss: 1.6287e-06\n",
            "Epoch 438: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0188 - output_1_loss: 0.0178 - output_2_loss: 1.6287e-06 - val_loss: 0.0633 - val_output_1_loss: 0.0624 - val_output_2_loss: 1.6284e-06 - lr: 1.0000e-05\n",
            "Epoch 439/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0244 - output_1_loss: 0.0234 - output_2_loss: 1.6285e-06\n",
            "Epoch 439: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0237 - output_1_loss: 0.0228 - output_2_loss: 1.6285e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6283e-06 - lr: 1.0000e-05\n",
            "Epoch 440/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0205 - output_1_loss: 0.0195 - output_2_loss: 1.6284e-06\n",
            "Epoch 440: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0206 - output_1_loss: 0.0196 - output_2_loss: 1.6284e-06 - val_loss: 0.0642 - val_output_1_loss: 0.0632 - val_output_2_loss: 1.6283e-06 - lr: 1.0000e-05\n",
            "Epoch 441/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0204 - output_1_loss: 0.0195 - output_2_loss: 1.6283e-06\n",
            "Epoch 441: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0201 - output_1_loss: 0.0191 - output_2_loss: 1.6283e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6282e-06 - lr: 1.0000e-05\n",
            "Epoch 442/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0181 - output_1_loss: 0.0172 - output_2_loss: 1.6283e-06\n",
            "Epoch 442: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0230 - output_1_loss: 0.0221 - output_2_loss: 1.6283e-06 - val_loss: 0.0625 - val_output_1_loss: 0.0615 - val_output_2_loss: 1.6281e-06 - lr: 1.0000e-05\n",
            "Epoch 443/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0170 - output_1_loss: 0.0160 - output_2_loss: 1.6281e-06\n",
            "Epoch 443: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0164 - output_1_loss: 0.0154 - output_2_loss: 1.6281e-06 - val_loss: 0.0632 - val_output_1_loss: 0.0623 - val_output_2_loss: 1.6280e-06 - lr: 1.0000e-05\n",
            "Epoch 444/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0175 - output_1_loss: 0.0166 - output_2_loss: 1.6281e-06\n",
            "Epoch 444: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0195 - output_1_loss: 0.0185 - output_2_loss: 1.6280e-06 - val_loss: 0.0632 - val_output_1_loss: 0.0622 - val_output_2_loss: 1.6279e-06 - lr: 1.0000e-05\n",
            "Epoch 445/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0180 - output_1_loss: 0.0170 - output_2_loss: 1.6279e-06\n",
            "Epoch 445: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0180 - output_1_loss: 0.0170 - output_2_loss: 1.6279e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6278e-06 - lr: 1.0000e-05\n",
            "Epoch 446/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0182 - output_1_loss: 0.0172 - output_2_loss: 1.6278e-06\n",
            "Epoch 446: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0198 - output_1_loss: 0.0188 - output_2_loss: 1.6278e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6277e-06 - lr: 1.0000e-05\n",
            "Epoch 447/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0200 - output_1_loss: 0.0190 - output_2_loss: 1.6278e-06\n",
            "Epoch 447: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0213 - output_1_loss: 0.0203 - output_2_loss: 1.6278e-06 - val_loss: 0.0625 - val_output_1_loss: 0.0615 - val_output_2_loss: 1.6276e-06 - lr: 1.0000e-05\n",
            "Epoch 448/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0148 - output_1_loss: 0.0138 - output_2_loss: 1.6277e-06\n",
            "Epoch 448: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0140 - output_1_loss: 0.0130 - output_2_loss: 1.6277e-06 - val_loss: 0.0634 - val_output_1_loss: 0.0624 - val_output_2_loss: 1.6275e-06 - lr: 1.0000e-05\n",
            "Epoch 449/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0179 - output_1_loss: 0.0169 - output_2_loss: 1.6275e-06\n",
            "Epoch 449: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0179 - output_1_loss: 0.0169 - output_2_loss: 1.6275e-06 - val_loss: 0.0637 - val_output_1_loss: 0.0628 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-05\n",
            "Epoch 450/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0238 - output_1_loss: 0.0228 - output_2_loss: 1.6276e-06\n",
            "Epoch 450: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0238 - output_1_loss: 0.0228 - output_2_loss: 1.6276e-06 - val_loss: 0.0623 - val_output_1_loss: 0.0613 - val_output_2_loss: 1.6273e-06 - lr: 1.0000e-05\n",
            "Epoch 451/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0215 - output_1_loss: 0.0205 - output_2_loss: 1.6275e-06\n",
            "Epoch 451: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0215 - output_1_loss: 0.0205 - output_2_loss: 1.6275e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6273e-06 - lr: 1.0000e-06\n",
            "Epoch 452/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0145 - output_1_loss: 0.0135 - output_2_loss: 1.6275e-06\n",
            "Epoch 452: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0142 - output_1_loss: 0.0133 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6273e-06 - lr: 1.0000e-06\n",
            "Epoch 453/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0250 - output_1_loss: 0.0240 - output_2_loss: 1.6275e-06\n",
            "Epoch 453: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0230 - output_1_loss: 0.0220 - output_2_loss: 1.6275e-06 - val_loss: 0.0626 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6273e-06 - lr: 1.0000e-06\n",
            "Epoch 454/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0157 - output_1_loss: 0.0147 - output_2_loss: 1.6275e-06\n",
            "Epoch 454: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0153 - output_1_loss: 0.0143 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6273e-06 - lr: 1.0000e-06\n",
            "Epoch 455/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0265 - output_1_loss: 0.0255 - output_2_loss: 1.6275e-06\n",
            "Epoch 455: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0251 - output_1_loss: 0.0241 - output_2_loss: 1.6275e-06 - val_loss: 0.0626 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6273e-06 - lr: 1.0000e-06\n",
            "Epoch 456/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0147 - output_1_loss: 0.0138 - output_2_loss: 1.6275e-06\n",
            "Epoch 456: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0142 - output_1_loss: 0.0133 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6273e-06 - lr: 1.0000e-06\n",
            "Epoch 457/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0211 - output_1_loss: 0.0202 - output_2_loss: 1.6274e-06\n",
            "Epoch 457: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0201 - output_1_loss: 0.0191 - output_2_loss: 1.6274e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6273e-06 - lr: 1.0000e-06\n",
            "Epoch 458/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0226 - output_1_loss: 0.0216 - output_2_loss: 1.6276e-06\n",
            "Epoch 458: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0222 - output_1_loss: 0.0212 - output_2_loss: 1.6275e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6273e-06 - lr: 1.0000e-06\n",
            "Epoch 459/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0198 - output_1_loss: 0.0189 - output_2_loss: 1.6274e-06\n",
            "Epoch 459: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0184 - output_1_loss: 0.0174 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 460/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0180 - output_1_loss: 0.0170 - output_2_loss: 1.6274e-06\n",
            "Epoch 460: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0217 - output_1_loss: 0.0208 - output_2_loss: 1.6274e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 461/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0165 - output_1_loss: 0.0155 - output_2_loss: 1.6275e-06\n",
            "Epoch 461: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0169 - output_1_loss: 0.0160 - output_2_loss: 1.6275e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 462/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0194 - output_1_loss: 0.0184 - output_2_loss: 1.6276e-06\n",
            "Epoch 462: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0177 - output_1_loss: 0.0167 - output_2_loss: 1.6276e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 463/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0220 - output_1_loss: 0.0210 - output_2_loss: 1.6275e-06\n",
            "Epoch 463: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0220 - output_1_loss: 0.0210 - output_2_loss: 1.6275e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 464/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0211 - output_1_loss: 0.0201 - output_2_loss: 1.6274e-06\n",
            "Epoch 464: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0233 - output_1_loss: 0.0224 - output_2_loss: 1.6274e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 465/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0133 - output_1_loss: 0.0123 - output_2_loss: 1.6275e-06\n",
            "Epoch 465: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0132 - output_1_loss: 0.0123 - output_2_loss: 1.6275e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 466/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0244 - output_1_loss: 0.0234 - output_2_loss: 1.6274e-06\n",
            "Epoch 466: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 16ms/step - loss: 0.0244 - output_1_loss: 0.0234 - output_2_loss: 1.6274e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 467/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0139 - output_1_loss: 0.0129 - output_2_loss: 1.6275e-06\n",
            "Epoch 467: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0139 - output_1_loss: 0.0129 - output_2_loss: 1.6275e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 468/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0222 - output_1_loss: 0.0213 - output_2_loss: 1.6275e-06\n",
            "Epoch 468: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0211 - output_1_loss: 0.0201 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 469/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0245 - output_1_loss: 0.0235 - output_2_loss: 1.6275e-06\n",
            "Epoch 469: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0235 - output_1_loss: 0.0225 - output_2_loss: 1.6275e-06 - val_loss: 0.0626 - val_output_1_loss: 0.0616 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 470/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0150 - output_1_loss: 0.0140 - output_2_loss: 1.6275e-06\n",
            "Epoch 470: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0150 - output_1_loss: 0.0140 - output_2_loss: 1.6275e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 471/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0227 - output_1_loss: 0.0217 - output_2_loss: 1.6274e-06\n",
            "Epoch 471: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0218 - output_1_loss: 0.0209 - output_2_loss: 1.6275e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 472/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0168 - output_1_loss: 0.0158 - output_2_loss: 1.6275e-06\n",
            "Epoch 472: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0167 - output_1_loss: 0.0157 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 473/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0208 - output_1_loss: 0.0199 - output_2_loss: 1.6275e-06\n",
            "Epoch 473: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0205 - output_1_loss: 0.0195 - output_2_loss: 1.6275e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 474/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0190 - output_1_loss: 0.0180 - output_2_loss: 1.6274e-06\n",
            "Epoch 474: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0189 - output_1_loss: 0.0179 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 475/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0202 - output_1_loss: 0.0192 - output_2_loss: 1.6275e-06\n",
            "Epoch 475: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0195 - output_1_loss: 0.0186 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 476/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0197 - output_1_loss: 0.0188 - output_2_loss: 1.6275e-06\n",
            "Epoch 476: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0184 - output_1_loss: 0.0174 - output_2_loss: 1.6274e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 477/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0242 - output_1_loss: 0.0233 - output_2_loss: 1.6275e-06\n",
            "Epoch 477: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0242 - output_1_loss: 0.0233 - output_2_loss: 1.6275e-06 - val_loss: 0.0626 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 478/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0160 - output_1_loss: 0.0151 - output_2_loss: 1.6275e-06\n",
            "Epoch 478: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0160 - output_1_loss: 0.0151 - output_2_loss: 1.6275e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 479/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0198 - output_1_loss: 0.0188 - output_2_loss: 1.6275e-06\n",
            "Epoch 479: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0220 - output_1_loss: 0.0211 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 480/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0180 - output_1_loss: 0.0171 - output_2_loss: 1.6275e-06\n",
            "Epoch 480: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0171 - output_1_loss: 0.0161 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 481/500\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0198 - output_1_loss: 0.0189 - output_2_loss: 1.6275e-06\n",
            "Epoch 481: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0191 - output_1_loss: 0.0181 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 482/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0197 - output_1_loss: 0.0187 - output_2_loss: 1.6275e-06\n",
            "Epoch 482: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0190 - output_1_loss: 0.0180 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 483/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0187 - output_1_loss: 0.0177 - output_2_loss: 1.6275e-06\n",
            "Epoch 483: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0187 - output_1_loss: 0.0177 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 484/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0223 - output_1_loss: 0.0213 - output_2_loss: 1.6275e-06\n",
            "Epoch 484: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0215 - output_1_loss: 0.0205 - output_2_loss: 1.6275e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 485/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0211 - output_1_loss: 0.0201 - output_2_loss: 1.6275e-06\n",
            "Epoch 485: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0209 - output_1_loss: 0.0200 - output_2_loss: 1.6275e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 486/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0167 - output_1_loss: 0.0157 - output_2_loss: 1.6275e-06\n",
            "Epoch 486: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0158 - output_1_loss: 0.0149 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 487/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0234 - output_1_loss: 0.0224 - output_2_loss: 1.6275e-06\n",
            "Epoch 487: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0234 - output_1_loss: 0.0224 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 488/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0169 - output_1_loss: 0.0159 - output_2_loss: 1.6276e-06\n",
            "Epoch 488: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0169 - output_1_loss: 0.0159 - output_2_loss: 1.6276e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 489/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0198 - output_1_loss: 0.0188 - output_2_loss: 1.6275e-06\n",
            "Epoch 489: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 17ms/step - loss: 0.0207 - output_1_loss: 0.0197 - output_2_loss: 1.6274e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 490/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0198 - output_1_loss: 0.0188 - output_2_loss: 1.6275e-06\n",
            "Epoch 490: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0191 - output_1_loss: 0.0181 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 491/500\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0178 - output_1_loss: 0.0169 - output_2_loss: 1.6275e-06\n",
            "Epoch 491: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0175 - output_1_loss: 0.0166 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 492/500\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0174 - output_1_loss: 0.0165 - output_2_loss: 1.6275e-06\n",
            "Epoch 492: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0206 - output_1_loss: 0.0196 - output_2_loss: 1.6275e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0620 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 493/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0191 - output_1_loss: 0.0182 - output_2_loss: 1.6275e-06\n",
            "Epoch 493: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0189 - output_1_loss: 0.0179 - output_2_loss: 1.6275e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 494/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0204 - output_1_loss: 0.0194 - output_2_loss: 1.6275e-06\n",
            "Epoch 494: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 15ms/step - loss: 0.0204 - output_1_loss: 0.0194 - output_2_loss: 1.6275e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0617 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 495/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0193 - output_1_loss: 0.0183 - output_2_loss: 1.6275e-06\n",
            "Epoch 495: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 14ms/step - loss: 0.0186 - output_1_loss: 0.0176 - output_2_loss: 1.6275e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 496/500\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0199 - output_1_loss: 0.0190 - output_2_loss: 1.6275e-06\n",
            "Epoch 496: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0193 - output_1_loss: 0.0183 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 497/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0194 - output_1_loss: 0.0185 - output_2_loss: 1.6275e-06\n",
            "Epoch 497: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0194 - output_1_loss: 0.0185 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 498/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0201 - output_1_loss: 0.0191 - output_2_loss: 1.6275e-06\n",
            "Epoch 498: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0201 - output_1_loss: 0.0191 - output_2_loss: 1.6275e-06 - val_loss: 0.0628 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 499/500\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0189 - output_1_loss: 0.0180 - output_2_loss: 1.6275e-06\n",
            "Epoch 499: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.0189 - output_1_loss: 0.0180 - output_2_loss: 1.6275e-06 - val_loss: 0.0629 - val_output_1_loss: 0.0619 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n",
            "Epoch 500/500\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0197 - output_1_loss: 0.0188 - output_2_loss: 1.6275e-06\n",
            "Epoch 500: val_loss did not improve from 0.06076\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0198 - output_1_loss: 0.0188 - output_2_loss: 1.6275e-06 - val_loss: 0.0627 - val_output_1_loss: 0.0618 - val_output_2_loss: 1.6274e-06 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Define input layer\n",
        "input_layer_1 = Input(shape=(3,), name = 'original')\n",
        "input_layer_2 = Input(shape=(3,), name = 'noisy')\n",
        "\n",
        "# Define the shared layers for the two branches\n",
        "shared_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(input_layer_1)\n",
        "#shared_layer_1 = Dropout(0.01)(shared_layer_1)\n",
        "shared_layer_2 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_1)\n",
        "#shared_layer_2 = Dropout(0.01)(shared_layer_2)\n",
        "shared_layer_3 = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_2)\n",
        "#shared_layer_3 = Dropout(0.01)(shared_layer_3)\n",
        "shared_layer_4 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_3)\n",
        "#shared_layer_4 = Dropout(0.01)(shared_layer_4)\n",
        "shared_layer_5 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_4)\n",
        "#shared_layer_5 = Dropout(0.01)(shared_layer_5)\n",
        "shared_layer_6 = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_5)\n",
        "#shared_layer_6 = Dropout(0.01)(shared_layer_6)\n",
        "\n",
        "# Define the first branch\n",
        "output_1 = Dense(1, name='output_1')(shared_layer_6)\n",
        "\n",
        "# Define the shared layers for the two branches\n",
        "shared_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(input_layer_2)\n",
        "#shared_layer_1 = Dropout(0.01)(shared_layer_1)\n",
        "shared_layer_2 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_1)\n",
        "#shared_layer_2 = Dropout(0.01)(shared_layer_2)\n",
        "shared_layer_3 = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_2)\n",
        "#shared_layer_3 = Dropout(0.01)(shared_layer_3)\n",
        "shared_layer_4 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_3)\n",
        "#shared_layer_4 = Dropout(0.01)(shared_layer_4)\n",
        "shared_layer_5 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_4)\n",
        "#shared_layer_5 = Dropout(0.01)(shared_layer_5)\n",
        "shared_layer_6 = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_5)\n",
        "#shared_layer_6 = Dropout(0.01)(shared_layer_6)\n",
        "\n",
        "# Define the second branch (identical to the first)\n",
        "output_2 = Dense(1, name='output_2')(shared_layer_6)\n",
        "\n",
        "# Create a model that takes input and has two outputs\n",
        "model = Model(inputs=[input_layer_1, input_layer_2], outputs=[output_1, output_2])\n",
        "\n",
        "# Define custom loss function\n",
        "def custom_loss(y_true, y_pred):\n",
        "    output_1 = y_pred[0]  # Unpack the predictions\n",
        "    output_2 = y_pred[1]\n",
        "\n",
        "    epsilon = 10e-5\n",
        "\n",
        "    # Define your custom loss calculation here\n",
        "    loss_1 = tf.losses.mean_squared_error(y_true, output_1)\n",
        "    loss_2 = 1/(tf.losses.mean_squared_error(y_true, output_2)+epsilon)\n",
        "\n",
        "    #print('MSE_original', loss_1)\n",
        "\n",
        "    # Combine the losses as per your requirements\n",
        "    total_loss = loss_1 + loss_2\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "# Define custom loss function\n",
        "def custom_loss_contrastive(y_true, y_pred):\n",
        "    epsilon = 10e-6\n",
        "    loss_2 = 1/(tf.losses.mean_squared_error(y_true, y_pred)+epsilon)\n",
        "\n",
        "    #print('MSE_original', loss_1)\n",
        "    # Combine the losses as per your requirements\n",
        "    # total_loss = loss_1 + loss_2\n",
        "\n",
        "    return loss_2\n",
        "\n",
        "def learning_rate_schedule(epoch, initial_lr=0.001):\n",
        "    \"\"\"\n",
        "    Custom learning rate schedule. Adjust the learning rate based on the current epoch.\n",
        "    You can customize this function to fit your needs.\n",
        "    \"\"\"\n",
        "    if epoch < 300 or epoch == 300:\n",
        "        return initial_lr  # Keep the initial learning rate for the first 50 epochs\n",
        "    elif epoch > 300 and epoch%50 == 0:\n",
        "        initial_lr = initial_lr * 0.1  # Reduce the learning rate by a factor of 10 after epoch 50\n",
        "        return initial_lr\n",
        "    else:\n",
        "        return initial_lr\n",
        "\n",
        "# Create the Adam optimizer with the initial learning rate\n",
        "initial_learning_rate = 0.001\n",
        "adam_optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
        "\n",
        "# Compile the model (specify loss for each output)\n",
        "model.compile(optimizer=adam_optimizer, loss={'output_1': 'mean_squared_error', 'output_2': custom_loss_contrastive}) #{'output_1': 'mean_squared_error', 'output_2': 'mean_squared_error'} custom_loss\n",
        "model.summary()\n",
        "\n",
        "# Define a ModelCheckpoint callback to save the best model\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "# Calculate the number of steps\n",
        "num_steps = len(X_train) // batch_size\n",
        "val_steps = len(X_test) // batch_size\n",
        "\n",
        "# Train the model using your custom data loading pipeline\n",
        "# You need to replace X_train and y_train with your custom data loading logic\n",
        "# You can use tf.data.Dataset for custom data loading pipelines\n",
        "history = model.fit(\n",
        "    custom_data_loader(X_train, y_train, batch_size),  # Replace with your custom data loader\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    steps_per_epoch=num_steps,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=[checkpoint, lr_scheduler]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e-QANi1GAZ_",
        "outputId": "a08ab117-0c7f-4dee-f30d-9d6cb5f20618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 0s 2ms/step - loss: 0.0601 - output_1_loss: 0.0586 - output_2_loss: 2.1248e-06\n",
            "Test Loss: [0.06010081619024277, 0.058605000376701355, 2.1248254142847145e-06]\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model('/content/best_model.h5', custom_objects={'custom_loss_contrastive': custom_loss_contrastive})\n",
        "\n",
        "losses = model.evaluate(val_dataset, steps=val_steps)\n",
        "print(\"Test Loss:\", losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FTTgfotYJzH",
        "outputId": "390d81c9-d3ce-4f07-cdab-1c60b0825830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions_1 = model.predict(val_dataset, steps=val_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c85FrszvYNlx",
        "outputId": "615bdc11-4263-4a1a-d7cd-1281008e759b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(208, 1)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions_1[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkYrolW5Z6S0",
        "outputId": "a11a0e01-7430-4e3c-b4f1-d2dfde681fd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(208,)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test[:208].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb1b3OXddf5S",
        "outputId": "6eff9040-8942-430b-e0c0-7b9461f59b14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(208, 2)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_array = np.column_stack((predictions_1[0].reshape(-1), y_test[:208]))\n",
        "results_array.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvaPBDVWeULc",
        "outputId": "4bdcbbb5-f0dd-4ee7-fa7d-f308921bc542"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(171, 2)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find unique rows using numpy's unique function and get the mapping\n",
        "unique_rows, unique_indices, inverse_indices = np.unique(results_array, axis=0, return_index=True, return_inverse=True)\n",
        "\n",
        "# Create a new array with the unique rows using the inverse mapping\n",
        "filtered_data = results_array[unique_indices]\n",
        "filtered_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY4sGttkhjeY",
        "outputId": "293e0a62-cf2c-4d0b-8d01-83580a5ecf72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(171,)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_data[:, 0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQe211IRbAf8"
      },
      "outputs": [],
      "source": [
        "predictions_real = np.array([[ 9.572701 ],\n",
        "       [10.237632 ],\n",
        "       [ 9.885076 ],\n",
        "       [ 9.275073 ],\n",
        "       [ 9.314901 ],\n",
        "       [ 9.385312 ],\n",
        "       [ 9.540867 ],\n",
        "       [ 9.310527 ],\n",
        "       [ 9.338502 ],\n",
        "       [ 9.595572 ],\n",
        "       [ 9.52753  ],\n",
        "       [ 9.339528 ],\n",
        "       [ 9.275298 ],\n",
        "       [ 9.408086 ],\n",
        "       [ 9.335758 ],\n",
        "       [ 9.343392 ],\n",
        "       [ 9.569541 ],\n",
        "       [ 9.326131 ],\n",
        "       [ 9.400727 ],\n",
        "       [ 9.5417   ],\n",
        "       [ 9.358058 ],\n",
        "       [10.191965 ],\n",
        "       [ 9.365769 ],\n",
        "       [ 9.470035 ],\n",
        "       [ 9.53802  ],\n",
        "       [ 9.560929 ],\n",
        "       [ 9.39619  ],\n",
        "       [ 9.533076 ],\n",
        "       [ 9.380445 ],\n",
        "       [ 9.3827915],\n",
        "       [10.407908 ],\n",
        "       [10.137826 ]])\n",
        "\n",
        "y_test_real = np.array([[ 9.452525 ],\n",
        "       [10.24755  ],\n",
        "       [ 9.86985  ],\n",
        "       [ 9.27835  ],\n",
        "       [ 9.375575 ],\n",
        "       [ 9.411249 ],\n",
        "       [ 9.522651 ],\n",
        "       [ 9.297275 ],\n",
        "       [ 9.32445  ],\n",
        "       [ 9.631625 ],\n",
        "       [ 9.556875 ],\n",
        "       [ 9.313825 ],\n",
        "       [ 9.2693   ],\n",
        "       [ 9.429001 ],\n",
        "       [ 9.288775 ],\n",
        "       [ 9.32205  ],\n",
        "       [ 9.575425 ],\n",
        "       [ 9.336725 ],\n",
        "       [ 9.37265  ],\n",
        "       [ 9.54095  ],\n",
        "       [ 9.373224 ],\n",
        "       [10.212126 ],\n",
        "       [ 9.377    ],\n",
        "       [ 9.466499 ],\n",
        "       [ 9.559549 ],\n",
        "       [ 9.5870495],\n",
        "       [ 9.38165  ],\n",
        "       [ 9.551425 ],\n",
        "       [ 9.37595  ],\n",
        "       [ 9.420401 ],\n",
        "       [10.4125   ],\n",
        "       [10.1695   ]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2veY5kpcbS7S",
        "outputId": "891df7c6-9c9d-47ee-f0d9-ae27a1d74c09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(32, 1)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions_real.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "7gzZVYRwaDDM",
        "outputId": "e62b1717-0ee1-49b5-9eff-f32022c00b16"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Slice the first 32 elements from y_test\n",
        "y_test_subset = y_test[:208]\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(8, 6))  # Set the figure size to your desired dimensions\n",
        "plt.scatter(y_test_real, predictions_real, alpha=0.7, c='r', edgecolors='k', s=100, label='Real Data')\n",
        "plt.scatter(filtered_data[:,1], filtered_data[:, 0], alpha=0.7, c='b', edgecolors='k', s=100, label='Simulation Data')  # Adjust marker properties as needed\n",
        "plt.title('')\n",
        "plt.xlabel('Experimental Permittivity Values')\n",
        "plt.ylabel('Predicted Permittivity Values')\n",
        "plt.grid(True)\n",
        "\n",
        "# # Fit a first-degree (linear) polynomial\n",
        "# coefficients = np.polyfit(y_test_subset, predictions_1[0], 1)\n",
        "# polynomial = np.poly1d(coefficients)\n",
        "\n",
        "# # Plot the regression line\n",
        "# plt.plot(y_test_subset, polynomial(y_test_subset), color='r', linestyle='--', linewidth=2)\n",
        "# Set the limits for both axes\n",
        "plt.xlim(9, 12)\n",
        "plt.ylim(9, 12)\n",
        "plt.legend()\n",
        "\n",
        "# Save the plot as a high-quality image (e.g., PNG or PDF)\n",
        "plt.savefig('/content/drive/MyDrive/IEEE EMBS SMP Project/results/scatter_plot.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmVdbPIkcrrc",
        "outputId": "bb27073e-1f4c-4edf-b91d-9303f4e0b523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pearson Correlation Coefficient: 0.9946190956767478\n",
            "Spearman Rank Correlation Coefficient: 0.9527126099706745\n",
            "Kendall Tau Correlation Coefficient: 0.8346774193548387\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "\n",
        "# Calculate different correlation coefficients\n",
        "pearson_corr, _ = pearsonr(y_test_real.flatten(), predictions_real.flatten())         # Pearson correlation coefficient\n",
        "spearman_corr, _ = spearmanr(y_test_real.flatten(), predictions_real.flatten())       # Spearman rank correlation coefficient\n",
        "kendall_corr, _ = kendalltau(y_test_real.flatten(), predictions_real.flatten())       # Kendall Tau correlation coefficient\n",
        "\n",
        "print(f\"Pearson Correlation Coefficient: {pearson_corr}\")\n",
        "print(f\"Spearman Rank Correlation Coefficient: {spearman_corr}\")\n",
        "print(f\"Kendall Tau Correlation Coefficient: {kendall_corr}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0dif_fGheZ6",
        "outputId": "3ed756f8-38fe-4527-8b57-4a68cede7a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pearson Correlation Coefficient: 0.9271943131334863\n",
            "Spearman Rank Correlation Coefficient: 0.9866253230366461\n",
            "Kendall Tau Correlation Coefficient: 0.9381727622543549\n"
          ]
        }
      ],
      "source": [
        "# Calculate different correlation coefficients\n",
        "pearson_corr, _ = pearsonr(filtered_data[:, 1], filtered_data[:, 0])         # Pearson correlation coefficient\n",
        "spearman_corr, _ = spearmanr(filtered_data[:, 1], filtered_data[:, 0])       # Spearman rank correlation coefficient\n",
        "kendall_corr, _ = kendalltau(filtered_data[:, 1], filtered_data[:, 0])       # Kendall Tau correlation coefficient\n",
        "\n",
        "print(f\"Pearson Correlation Coefficient: {pearson_corr}\")\n",
        "print(f\"Spearman Rank Correlation Coefficient: {spearman_corr}\")\n",
        "print(f\"Kendall Tau Correlation Coefficient: {kendall_corr}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AiHhWGGkSgS"
      },
      "outputs": [],
      "source": [
        "errors_real = np.array([[0.12017632],\n",
        "       [0.00991821],\n",
        "       [0.01522541],\n",
        "       [0.00327682],\n",
        "       [0.06067371],\n",
        "       [0.02593708],\n",
        "       [0.01821613],\n",
        "       [0.01325226],\n",
        "       [0.01405239],\n",
        "       [0.0360527 ],\n",
        "       [0.02934551],\n",
        "       [0.02570343],\n",
        "       [0.00599766],\n",
        "       [0.02091503],\n",
        "       [0.04698277],\n",
        "       [0.02134228],\n",
        "       [0.00588417],\n",
        "       [0.01059437],\n",
        "       [0.02807713],\n",
        "       [0.00075054],\n",
        "       [0.01516628],\n",
        "       [0.02016068],\n",
        "       [0.01123047],\n",
        "       [0.00353527],\n",
        "       [0.0215292 ],\n",
        "       [0.02612019],\n",
        "       [0.01453972],\n",
        "       [0.01834869],\n",
        "       [0.00449467],\n",
        "       [0.0376091 ],\n",
        "       [0.00459194],\n",
        "       [0.03167439]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIUXHcDRkZvI"
      },
      "outputs": [],
      "source": [
        "errors_simulation = np.abs(filtered_data[:, 0] - filtered_data[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVBD9BvSh327",
        "outputId": "0cc1f11f-95a4-441e-af55-59dc506122b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.009411014952849954"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_per_simulation = np.mean(errors_simulation/filtered_data[:, 1])\n",
        "error_per_simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex6WV9GSiH-m",
        "outputId": "d24f242f-b7c5-4374-a160-41f620a451cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0023718109339161747"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_per_real = np.mean(errors_real/y_test_real)\n",
        "error_per_real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "XJgk7zW5k9-4",
        "outputId": "1d034dae-a309-4621-c752-0133c59f54ec"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2400x1500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(8, 5), dpi=300)\n",
        "\n",
        "# Plot the data with customizations\n",
        "plt.scatter(y_test_real, errors_real, alpha=0.7, c='r', edgecolors='k', s=50, label='Real Data')\n",
        "plt.scatter(filtered_data[:,1], errors_simulation, alpha=0.7, c='b', edgecolors='k', s=50, label='Simulation Data')\n",
        "plt.xlabel('Experimental Permittivity Values', fontsize=12)\n",
        "plt.ylabel('Error', fontsize=12)\n",
        "plt.title('', fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Customize the axis labels and ticks (adjust as needed)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.xlim(9, 12)\n",
        "plt.ylim(-0.01, 0.125)\n",
        "\n",
        "# Save the plot as a high-resolution image (adjust the format as needed)\n",
        "plt.savefig('/content/drive/MyDrive/IEEE EMBS SMP Project/results/simulation_real_error_plot.png', format='png', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XddvyW6Isro_"
      },
      "source": [
        "#Siamese"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDLGpbjhstTO",
        "outputId": "d3ff84fe-3995-4ab2-cc38-94dc3eb8134f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " original (InputLayer)       [(None, 4)]                  0         []                            \n",
            "                                                                                                  \n",
            " noisy (InputLayer)          [(None, 4)]                  0         []                            \n",
            "                                                                                                  \n",
            " model (Functional)          (None, 32)                   150688    ['original[0][0]',            \n",
            "                                                                     'noisy[0][0]']               \n",
            "                                                                                                  \n",
            " output_original (Dense)     (None, 1)                    33        ['model[0][0]']               \n",
            "                                                                                                  \n",
            " output_noisy (Dense)        (None, 1)                    33        ['model[1][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 150754 (588.88 KB)\n",
            "Trainable params: 150754 (588.88 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "1\n",
            "128\n",
            "(396, 1)\n",
            "(396, 4)\n",
            "Epoch 1/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 26.7435 - output_original_loss: 26.4522 - output_noisy_loss: 0.2850\n",
            "Epoch 1: val_loss improved from inf to 23.39763, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 5s 443ms/step - loss: 26.7435 - output_original_loss: 26.4522 - output_noisy_loss: 0.2850 - val_loss: 23.3976 - val_output_original_loss: 23.0932 - val_output_noisy_loss: 0.2982 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 22.9663 - output_original_loss: 22.6929 - output_noisy_loss: 0.2671\n",
            "Epoch 2: val_loss improved from 23.39763 to 18.28887, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 119ms/step - loss: 22.9663 - output_original_loss: 22.6929 - output_noisy_loss: 0.2671 - val_loss: 18.2889 - val_output_original_loss: 18.0044 - val_output_noisy_loss: 0.2782 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 18.4021 - output_original_loss: 18.1347 - output_noisy_loss: 0.2612\n",
            "Epoch 3: val_loss improved from 18.28887 to 11.53229, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 16.2636 - output_original_loss: 16.0021 - output_noisy_loss: 0.2552 - val_loss: 11.5323 - val_output_original_loss: 11.2771 - val_output_noisy_loss: 0.2489 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 12.5361 - output_original_loss: 12.3028 - output_noisy_loss: 0.2271\n",
            "Epoch 4: val_loss improved from 11.53229 to 9.31711, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 10.1571 - output_original_loss: 9.9291 - output_noisy_loss: 0.2218 - val_loss: 9.3171 - val_output_original_loss: 9.0977 - val_output_noisy_loss: 0.2132 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 8.6659 - output_original_loss: 8.4627 - output_noisy_loss: 0.1970\n",
            "Epoch 5: val_loss did not improve from 9.31711\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 8.6659 - output_original_loss: 8.4627 - output_noisy_loss: 0.1970 - val_loss: 10.7753 - val_output_original_loss: 10.5689 - val_output_noisy_loss: 0.2003 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 9.0871 - output_original_loss: 8.8974 - output_noisy_loss: 0.1835\n",
            "Epoch 6: val_loss improved from 9.31711 to 6.06044, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 7.4327 - output_original_loss: 7.2366 - output_noisy_loss: 0.1899 - val_loss: 6.0604 - val_output_original_loss: 5.8442 - val_output_noisy_loss: 0.2101 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.0442 - output_original_loss: 4.8353 - output_noisy_loss: 0.2027\n",
            "Epoch 7: val_loss improved from 6.06044 to 4.55940, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 4.6220 - output_original_loss: 4.4103 - output_noisy_loss: 0.2055 - val_loss: 4.5594 - val_output_original_loss: 4.3349 - val_output_noisy_loss: 0.2184 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.1853 - output_original_loss: 3.9709 - output_noisy_loss: 0.2083\n",
            "Epoch 8: val_loss improved from 4.55940 to 3.72398, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 172ms/step - loss: 4.1853 - output_original_loss: 3.9709 - output_noisy_loss: 0.2083 - val_loss: 3.7240 - val_output_original_loss: 3.4991 - val_output_noisy_loss: 0.2188 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.7172 - output_original_loss: 3.5128 - output_noisy_loss: 0.1984\n",
            "Epoch 9: val_loss improved from 3.72398 to 2.81515, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 3.1287 - output_original_loss: 2.9152 - output_noisy_loss: 0.2075 - val_loss: 2.8152 - val_output_original_loss: 2.5977 - val_output_noisy_loss: 0.2114 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4310 - output_original_loss: 2.2245 - output_noisy_loss: 0.2005\n",
            "Epoch 10: val_loss improved from 2.81515 to 2.55469, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 238ms/step - loss: 2.4310 - output_original_loss: 2.2245 - output_noisy_loss: 0.2005 - val_loss: 2.5547 - val_output_original_loss: 2.3445 - val_output_noisy_loss: 0.2042 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3981 - output_original_loss: 2.2064 - output_noisy_loss: 0.1857\n",
            "Epoch 11: val_loss improved from 2.55469 to 1.70760, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 138ms/step - loss: 2.1430 - output_original_loss: 1.9415 - output_noisy_loss: 0.1955 - val_loss: 1.7076 - val_output_original_loss: 1.4965 - val_output_noisy_loss: 0.2052 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.2946 - output_original_loss: 1.0934 - output_noisy_loss: 0.1952\n",
            "Epoch 12: val_loss improved from 1.70760 to 1.17885, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 169ms/step - loss: 1.2946 - output_original_loss: 1.0934 - output_noisy_loss: 0.1952 - val_loss: 1.1789 - val_output_original_loss: 0.9636 - val_output_noisy_loss: 0.2094 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.0504 - output_original_loss: 0.8409 - output_noisy_loss: 0.2036\n",
            "Epoch 13: val_loss improved from 1.17885 to 0.99015, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 195ms/step - loss: 1.0504 - output_original_loss: 0.8409 - output_noisy_loss: 0.2036 - val_loss: 0.9901 - val_output_original_loss: 0.7747 - val_output_noisy_loss: 0.2095 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9365 - output_original_loss: 0.7307 - output_noisy_loss: 0.1998\n",
            "Epoch 14: val_loss improved from 0.99015 to 0.83412, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 194ms/step - loss: 0.9365 - output_original_loss: 0.7307 - output_noisy_loss: 0.1998 - val_loss: 0.8341 - val_output_original_loss: 0.6224 - val_output_noisy_loss: 0.2058 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7295 - output_original_loss: 0.5280 - output_noisy_loss: 0.1956\n",
            "Epoch 15: val_loss improved from 0.83412 to 0.79949, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.7295 - output_original_loss: 0.5280 - output_noisy_loss: 0.1956 - val_loss: 0.7995 - val_output_original_loss: 0.5910 - val_output_noisy_loss: 0.2026 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7614 - output_original_loss: 0.5543 - output_noisy_loss: 0.2012\n",
            "Epoch 16: val_loss improved from 0.79949 to 0.74432, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 1s 280ms/step - loss: 0.7614 - output_original_loss: 0.5543 - output_noisy_loss: 0.2012 - val_loss: 0.7443 - val_output_original_loss: 0.5363 - val_output_noisy_loss: 0.2022 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7272 - output_original_loss: 0.5189 - output_noisy_loss: 0.2025\n",
            "Epoch 17: val_loss improved from 0.74432 to 0.72355, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 1s 281ms/step - loss: 0.7272 - output_original_loss: 0.5189 - output_noisy_loss: 0.2025 - val_loss: 0.7236 - val_output_original_loss: 0.5187 - val_output_noisy_loss: 0.1990 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6381 - output_original_loss: 0.4510 - output_noisy_loss: 0.1813\n",
            "Epoch 18: val_loss improved from 0.72355 to 0.65973, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.6381 - output_original_loss: 0.4510 - output_noisy_loss: 0.1813 - val_loss: 0.6597 - val_output_original_loss: 0.4613 - val_output_noisy_loss: 0.1927 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5760 - output_original_loss: 0.3783 - output_noisy_loss: 0.1919\n",
            "Epoch 19: val_loss improved from 0.65973 to 0.61877, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 199ms/step - loss: 0.5760 - output_original_loss: 0.3783 - output_noisy_loss: 0.1919 - val_loss: 0.6188 - val_output_original_loss: 0.4261 - val_output_noisy_loss: 0.1869 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5413 - output_original_loss: 0.3525 - output_noisy_loss: 0.1830\n",
            "Epoch 20: val_loss improved from 0.61877 to 0.58686, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 117ms/step - loss: 0.5413 - output_original_loss: 0.3525 - output_noisy_loss: 0.1830 - val_loss: 0.5869 - val_output_original_loss: 0.3978 - val_output_noisy_loss: 0.1833 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5450 - output_original_loss: 0.3607 - output_noisy_loss: 0.1786\n",
            "Epoch 21: val_loss improved from 0.58686 to 0.56174, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 124ms/step - loss: 0.5555 - output_original_loss: 0.3819 - output_noisy_loss: 0.1679 - val_loss: 0.5617 - val_output_original_loss: 0.3777 - val_output_noisy_loss: 0.1783 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4163 - output_original_loss: 0.2272 - output_noisy_loss: 0.1834\n",
            "Epoch 22: val_loss improved from 0.56174 to 0.54433, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.4509 - output_original_loss: 0.2752 - output_noisy_loss: 0.1701 - val_loss: 0.5443 - val_output_original_loss: 0.3654 - val_output_noisy_loss: 0.1732 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4977 - output_original_loss: 0.3131 - output_noisy_loss: 0.1789\n",
            "Epoch 23: val_loss improved from 0.54433 to 0.52599, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.4557 - output_original_loss: 0.2822 - output_noisy_loss: 0.1678 - val_loss: 0.5260 - val_output_original_loss: 0.3500 - val_output_noisy_loss: 0.1703 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4662 - output_original_loss: 0.2967 - output_noisy_loss: 0.1639\n",
            "Epoch 24: val_loss improved from 0.52599 to 0.51269, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4276 - output_original_loss: 0.2530 - output_noisy_loss: 0.1689 - val_loss: 0.5127 - val_output_original_loss: 0.3399 - val_output_noisy_loss: 0.1671 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4081 - output_original_loss: 0.2492 - output_noisy_loss: 0.1532\n",
            "Epoch 25: val_loss improved from 0.51269 to 0.49709, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.4387 - output_original_loss: 0.2747 - output_noisy_loss: 0.1583 - val_loss: 0.4971 - val_output_original_loss: 0.3291 - val_output_noisy_loss: 0.1624 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3766 - output_original_loss: 0.2139 - output_noisy_loss: 0.1570\n",
            "Epoch 26: val_loss improved from 0.49709 to 0.48312, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3981 - output_original_loss: 0.2361 - output_noisy_loss: 0.1563 - val_loss: 0.4831 - val_output_original_loss: 0.3186 - val_output_noisy_loss: 0.1589 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3337 - output_original_loss: 0.1762 - output_noisy_loss: 0.1519\n",
            "Epoch 27: val_loss improved from 0.48312 to 0.47406, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3915 - output_original_loss: 0.2361 - output_noisy_loss: 0.1498 - val_loss: 0.4741 - val_output_original_loss: 0.3137 - val_output_noisy_loss: 0.1547 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2921 - output_original_loss: 0.1448 - output_noisy_loss: 0.1417\n",
            "Epoch 28: val_loss improved from 0.47406 to 0.46626, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3515 - output_original_loss: 0.2016 - output_noisy_loss: 0.1442 - val_loss: 0.4663 - val_output_original_loss: 0.3111 - val_output_noisy_loss: 0.1496 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3879 - output_original_loss: 0.2380 - output_noisy_loss: 0.1443\n",
            "Epoch 29: val_loss improved from 0.46626 to 0.45262, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3879 - output_original_loss: 0.2380 - output_noisy_loss: 0.1443 - val_loss: 0.4526 - val_output_original_loss: 0.3017 - val_output_noisy_loss: 0.1453 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4186 - output_original_loss: 0.2753 - output_noisy_loss: 0.1377\n",
            "Epoch 30: val_loss improved from 0.45262 to 0.43689, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3760 - output_original_loss: 0.2311 - output_noisy_loss: 0.1393 - val_loss: 0.4369 - val_output_original_loss: 0.2907 - val_output_noisy_loss: 0.1406 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3384 - output_original_loss: 0.1978 - output_noisy_loss: 0.1350\n",
            "Epoch 31: val_loss improved from 0.43689 to 0.42320, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3384 - output_original_loss: 0.1978 - output_noisy_loss: 0.1350 - val_loss: 0.4232 - val_output_original_loss: 0.2810 - val_output_noisy_loss: 0.1366 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3610 - output_original_loss: 0.2338 - output_noisy_loss: 0.1217\n",
            "Epoch 32: val_loss improved from 0.42320 to 0.41429, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3306 - output_original_loss: 0.1943 - output_noisy_loss: 0.1307 - val_loss: 0.4143 - val_output_original_loss: 0.2758 - val_output_noisy_loss: 0.1329 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3169 - output_original_loss: 0.1792 - output_noisy_loss: 0.1321\n",
            "Epoch 33: val_loss improved from 0.41429 to 0.40257, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3331 - output_original_loss: 0.2008 - output_noisy_loss: 0.1267 - val_loss: 0.4026 - val_output_original_loss: 0.2694 - val_output_noisy_loss: 0.1276 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3229 - output_original_loss: 0.1951 - output_noisy_loss: 0.1223\n",
            "Epoch 34: val_loss improved from 0.40257 to 0.38778, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.3229 - output_original_loss: 0.1951 - output_noisy_loss: 0.1223 - val_loss: 0.3878 - val_output_original_loss: 0.2585 - val_output_noisy_loss: 0.1238 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3010 - output_original_loss: 0.1781 - output_noisy_loss: 0.1174\n",
            "Epoch 35: val_loss improved from 0.38778 to 0.37624, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3010 - output_original_loss: 0.1781 - output_noisy_loss: 0.1174 - val_loss: 0.3762 - val_output_original_loss: 0.2513 - val_output_noisy_loss: 0.1194 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2523 - output_original_loss: 0.1193 - output_noisy_loss: 0.1275\n",
            "Epoch 36: val_loss improved from 0.37624 to 0.36790, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.2951 - output_original_loss: 0.1742 - output_noisy_loss: 0.1153 - val_loss: 0.3679 - val_output_original_loss: 0.2476 - val_output_noisy_loss: 0.1148 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2962 - output_original_loss: 0.1887 - output_noisy_loss: 0.1020\n",
            "Epoch 37: val_loss improved from 0.36790 to 0.35642, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.2893 - output_original_loss: 0.1753 - output_noisy_loss: 0.1084 - val_loss: 0.3564 - val_output_original_loss: 0.2400 - val_output_noisy_loss: 0.1109 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3199 - output_original_loss: 0.2053 - output_noisy_loss: 0.1091\n",
            "Epoch 38: val_loss improved from 0.35642 to 0.34379, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 126ms/step - loss: 0.2741 - output_original_loss: 0.1615 - output_noisy_loss: 0.1071 - val_loss: 0.3438 - val_output_original_loss: 0.2316 - val_output_noisy_loss: 0.1066 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2740 - output_original_loss: 0.1672 - output_noisy_loss: 0.1013\n",
            "Epoch 39: val_loss improved from 0.34379 to 0.33525, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 164ms/step - loss: 0.2740 - output_original_loss: 0.1672 - output_noisy_loss: 0.1013 - val_loss: 0.3352 - val_output_original_loss: 0.2273 - val_output_noisy_loss: 0.1024 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2762 - output_original_loss: 0.1736 - output_noisy_loss: 0.0971\n",
            "Epoch 40: val_loss improved from 0.33525 to 0.32601, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.2762 - output_original_loss: 0.1736 - output_noisy_loss: 0.0971 - val_loss: 0.3260 - val_output_original_loss: 0.2220 - val_output_noisy_loss: 0.0985 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2357 - output_original_loss: 0.1353 - output_noisy_loss: 0.0948\n",
            "Epoch 41: val_loss improved from 0.32601 to 0.31862, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 125ms/step - loss: 0.2357 - output_original_loss: 0.1353 - output_noisy_loss: 0.0948 - val_loss: 0.3186 - val_output_original_loss: 0.2183 - val_output_noisy_loss: 0.0947 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2670 - output_original_loss: 0.1757 - output_noisy_loss: 0.0858\n",
            "Epoch 42: val_loss improved from 0.31862 to 0.30980, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.2573 - output_original_loss: 0.1631 - output_noisy_loss: 0.0886 - val_loss: 0.3098 - val_output_original_loss: 0.2121 - val_output_noisy_loss: 0.0921 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2423 - output_original_loss: 0.1456 - output_noisy_loss: 0.0912\n",
            "Epoch 43: val_loss improved from 0.30980 to 0.30244, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.2401 - output_original_loss: 0.1461 - output_noisy_loss: 0.0885 - val_loss: 0.3024 - val_output_original_loss: 0.2083 - val_output_noisy_loss: 0.0886 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2168 - output_original_loss: 0.1265 - output_noisy_loss: 0.0848\n",
            "Epoch 44: val_loss improved from 0.30244 to 0.30015, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.2168 - output_original_loss: 0.1265 - output_noisy_loss: 0.0848 - val_loss: 0.3002 - val_output_original_loss: 0.2083 - val_output_noisy_loss: 0.0863 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2671 - output_original_loss: 0.1803 - output_noisy_loss: 0.0813\n",
            "Epoch 45: val_loss improved from 0.30015 to 0.29117, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.2462 - output_original_loss: 0.1606 - output_noisy_loss: 0.0801 - val_loss: 0.2912 - val_output_original_loss: 0.2027 - val_output_noisy_loss: 0.0829 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2034 - output_original_loss: 0.1171 - output_noisy_loss: 0.0808\n",
            "Epoch 46: val_loss improved from 0.29117 to 0.28180, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.2034 - output_original_loss: 0.1171 - output_noisy_loss: 0.0808 - val_loss: 0.2818 - val_output_original_loss: 0.1949 - val_output_noisy_loss: 0.0814 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2104 - output_original_loss: 0.1278 - output_noisy_loss: 0.0770\n",
            "Epoch 47: val_loss improved from 0.28180 to 0.27363, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 118ms/step - loss: 0.2104 - output_original_loss: 0.1278 - output_noisy_loss: 0.0770 - val_loss: 0.2736 - val_output_original_loss: 0.1896 - val_output_noisy_loss: 0.0785 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1819 - output_original_loss: 0.1027 - output_noisy_loss: 0.0736\n",
            "Epoch 48: val_loss improved from 0.27363 to 0.26069, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.2190 - output_original_loss: 0.1401 - output_noisy_loss: 0.0733 - val_loss: 0.2607 - val_output_original_loss: 0.1783 - val_output_noisy_loss: 0.0769 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1661 - output_original_loss: 0.0869 - output_noisy_loss: 0.0736\n",
            "Epoch 49: val_loss improved from 0.26069 to 0.25175, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.1661 - output_original_loss: 0.0869 - output_noisy_loss: 0.0736 - val_loss: 0.2517 - val_output_original_loss: 0.1719 - val_output_noisy_loss: 0.0743 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1884 - output_original_loss: 0.1127 - output_noisy_loss: 0.0702\n",
            "Epoch 50: val_loss improved from 0.25175 to 0.24241, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.1911 - output_original_loss: 0.1149 - output_noisy_loss: 0.0707 - val_loss: 0.2424 - val_output_original_loss: 0.1645 - val_output_noisy_loss: 0.0724 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1879 - output_original_loss: 0.1130 - output_noisy_loss: 0.0694\n",
            "Epoch 51: val_loss improved from 0.24241 to 0.23218, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.1738 - output_original_loss: 0.1002 - output_noisy_loss: 0.0680 - val_loss: 0.2322 - val_output_original_loss: 0.1567 - val_output_noisy_loss: 0.0700 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1600 - output_original_loss: 0.0890 - output_noisy_loss: 0.0654\n",
            "Epoch 52: val_loss improved from 0.23218 to 0.22424, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.1904 - output_original_loss: 0.1190 - output_noisy_loss: 0.0658 - val_loss: 0.2242 - val_output_original_loss: 0.1512 - val_output_noisy_loss: 0.0675 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1453 - output_original_loss: 0.0754 - output_noisy_loss: 0.0644\n",
            "Epoch 53: val_loss improved from 0.22424 to 0.22134, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.1524 - output_original_loss: 0.0819 - output_noisy_loss: 0.0650 - val_loss: 0.2213 - val_output_original_loss: 0.1506 - val_output_noisy_loss: 0.0652 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1692 - output_original_loss: 0.1031 - output_noisy_loss: 0.0606\n",
            "Epoch 54: val_loss improved from 0.22134 to 0.21979, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.1734 - output_original_loss: 0.1058 - output_noisy_loss: 0.0621 - val_loss: 0.2198 - val_output_original_loss: 0.1508 - val_output_noisy_loss: 0.0635 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1490 - output_original_loss: 0.0852 - output_noisy_loss: 0.0582\n",
            "Epoch 55: val_loss improved from 0.21979 to 0.20453, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 117ms/step - loss: 0.1614 - output_original_loss: 0.0966 - output_noisy_loss: 0.0592 - val_loss: 0.2045 - val_output_original_loss: 0.1373 - val_output_noisy_loss: 0.0617 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1712 - output_original_loss: 0.1064 - output_noisy_loss: 0.0592\n",
            "Epoch 56: val_loss improved from 0.20453 to 0.19435, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.1720 - output_original_loss: 0.1070 - output_noisy_loss: 0.0594 - val_loss: 0.1944 - val_output_original_loss: 0.1287 - val_output_noisy_loss: 0.0601 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1302 - output_original_loss: 0.0678 - output_noisy_loss: 0.0568\n",
            "Epoch 57: val_loss improved from 0.19435 to 0.18982, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.1302 - output_original_loss: 0.0678 - output_noisy_loss: 0.0568 - val_loss: 0.1898 - val_output_original_loss: 0.1256 - val_output_noisy_loss: 0.0586 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1414 - output_original_loss: 0.0808 - output_noisy_loss: 0.0550\n",
            "Epoch 58: val_loss improved from 0.18982 to 0.18687, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1384 - output_original_loss: 0.0775 - output_noisy_loss: 0.0553 - val_loss: 0.1869 - val_output_original_loss: 0.1243 - val_output_noisy_loss: 0.0570 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1259 - output_original_loss: 0.0677 - output_noisy_loss: 0.0526\n",
            "Epoch 59: val_loss improved from 0.18687 to 0.17725, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.1369 - output_original_loss: 0.0774 - output_noisy_loss: 0.0539 - val_loss: 0.1772 - val_output_original_loss: 0.1161 - val_output_noisy_loss: 0.0556 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1380 - output_original_loss: 0.0779 - output_noisy_loss: 0.0545\n",
            "Epoch 60: val_loss improved from 0.17725 to 0.17073, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.1317 - output_original_loss: 0.0736 - output_noisy_loss: 0.0526 - val_loss: 0.1707 - val_output_original_loss: 0.1110 - val_output_noisy_loss: 0.0541 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1459 - output_original_loss: 0.0883 - output_noisy_loss: 0.0520\n",
            "Epoch 61: val_loss improved from 0.17073 to 0.16559, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.1157 - output_original_loss: 0.0587 - output_noisy_loss: 0.0514 - val_loss: 0.1656 - val_output_original_loss: 0.1073 - val_output_noisy_loss: 0.0527 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1274 - output_original_loss: 0.0726 - output_noisy_loss: 0.0493\n",
            "Epoch 62: val_loss improved from 0.16559 to 0.15997, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.1221 - output_original_loss: 0.0665 - output_noisy_loss: 0.0500 - val_loss: 0.1600 - val_output_original_loss: 0.1030 - val_output_noisy_loss: 0.0513 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1354 - output_original_loss: 0.0810 - output_noisy_loss: 0.0488\n",
            "Epoch 63: val_loss improved from 0.15997 to 0.15518, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1222 - output_original_loss: 0.0679 - output_noisy_loss: 0.0486 - val_loss: 0.1552 - val_output_original_loss: 0.0996 - val_output_noisy_loss: 0.0500 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0893 - output_original_loss: 0.0369 - output_noisy_loss: 0.0469\n",
            "Epoch 64: val_loss improved from 0.15518 to 0.15038, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.1161 - output_original_loss: 0.0630 - output_noisy_loss: 0.0475 - val_loss: 0.1504 - val_output_original_loss: 0.0960 - val_output_noisy_loss: 0.0487 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0982 - output_original_loss: 0.0447 - output_noisy_loss: 0.0479\n",
            "Epoch 65: val_loss improved from 0.15038 to 0.14616, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.1135 - output_original_loss: 0.0617 - output_noisy_loss: 0.0462 - val_loss: 0.1462 - val_output_original_loss: 0.0930 - val_output_noisy_loss: 0.0475 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1039 - output_original_loss: 0.0516 - output_noisy_loss: 0.0467\n",
            "Epoch 66: val_loss improved from 0.14616 to 0.14001, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.1033 - output_original_loss: 0.0524 - output_noisy_loss: 0.0453 - val_loss: 0.1400 - val_output_original_loss: 0.0880 - val_output_noisy_loss: 0.0464 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0951 - output_original_loss: 0.0450 - output_noisy_loss: 0.0445\n",
            "Epoch 67: val_loss did not improve from 0.14001\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1062 - output_original_loss: 0.0565 - output_noisy_loss: 0.0440 - val_loss: 0.1401 - val_output_original_loss: 0.0891 - val_output_noisy_loss: 0.0453 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1028 - output_original_loss: 0.0519 - output_noisy_loss: 0.0452\n",
            "Epoch 68: val_loss improved from 0.14001 to 0.13378, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.1068 - output_original_loss: 0.0580 - output_noisy_loss: 0.0431 - val_loss: 0.1338 - val_output_original_loss: 0.0838 - val_output_noisy_loss: 0.0443 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0760 - output_original_loss: 0.0280 - output_noisy_loss: 0.0424\n",
            "Epoch 69: val_loss did not improve from 0.13378\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.0996 - output_original_loss: 0.0518 - output_noisy_loss: 0.0423 - val_loss: 0.1351 - val_output_original_loss: 0.0862 - val_output_noisy_loss: 0.0433 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1081 - output_original_loss: 0.0606 - output_noisy_loss: 0.0418\n",
            "Epoch 70: val_loss improved from 0.13378 to 0.12568, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0986 - output_original_loss: 0.0518 - output_noisy_loss: 0.0411 - val_loss: 0.1257 - val_output_original_loss: 0.0776 - val_output_noisy_loss: 0.0425 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0969 - output_original_loss: 0.0508 - output_noisy_loss: 0.0405\n",
            "Epoch 71: val_loss improved from 0.12568 to 0.11857, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0936 - output_original_loss: 0.0475 - output_noisy_loss: 0.0404 - val_loss: 0.1186 - val_output_original_loss: 0.0713 - val_output_noisy_loss: 0.0416 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0877 - output_original_loss: 0.0431 - output_noisy_loss: 0.0390\n",
            "Epoch 72: val_loss did not improve from 0.11857\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0889 - output_original_loss: 0.0438 - output_noisy_loss: 0.0394 - val_loss: 0.1207 - val_output_original_loss: 0.0743 - val_output_noisy_loss: 0.0408 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0971 - output_original_loss: 0.0521 - output_noisy_loss: 0.0394\n",
            "Epoch 73: val_loss did not improve from 0.11857\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0961 - output_original_loss: 0.0515 - output_noisy_loss: 0.0389 - val_loss: 0.1187 - val_output_original_loss: 0.0729 - val_output_noisy_loss: 0.0401 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1011 - output_original_loss: 0.0569 - output_noisy_loss: 0.0385\n",
            "Epoch 74: val_loss did not improve from 0.11857\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0958 - output_original_loss: 0.0515 - output_noisy_loss: 0.0387 - val_loss: 0.1288 - val_output_original_loss: 0.0837 - val_output_noisy_loss: 0.0394 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0867 - output_original_loss: 0.0448 - output_noisy_loss: 0.0363\n",
            "Epoch 75: val_loss improved from 0.11857 to 0.10907, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0843 - output_original_loss: 0.0416 - output_noisy_loss: 0.0371 - val_loss: 0.1091 - val_output_original_loss: 0.0645 - val_output_noisy_loss: 0.0389 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0920 - output_original_loss: 0.0486 - output_noisy_loss: 0.0377\n",
            "Epoch 76: val_loss did not improve from 0.10907\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0846 - output_original_loss: 0.0417 - output_noisy_loss: 0.0372 - val_loss: 0.1127 - val_output_original_loss: 0.0687 - val_output_noisy_loss: 0.0384 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0932 - output_original_loss: 0.0508 - output_noisy_loss: 0.0367\n",
            "Epoch 77: val_loss improved from 0.10907 to 0.10801, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0883 - output_original_loss: 0.0462 - output_noisy_loss: 0.0364 - val_loss: 0.1080 - val_output_original_loss: 0.0646 - val_output_noisy_loss: 0.0378 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0914 - output_original_loss: 0.0488 - output_noisy_loss: 0.0369\n",
            "Epoch 78: val_loss improved from 0.10801 to 0.10585, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0892 - output_original_loss: 0.0470 - output_noisy_loss: 0.0365 - val_loss: 0.1058 - val_output_original_loss: 0.0629 - val_output_noisy_loss: 0.0373 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0842 - output_original_loss: 0.0430 - output_noisy_loss: 0.0355\n",
            "Epoch 79: val_loss improved from 0.10585 to 0.09709, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0909 - output_original_loss: 0.0495 - output_noisy_loss: 0.0358 - val_loss: 0.0971 - val_output_original_loss: 0.0545 - val_output_noisy_loss: 0.0369 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0651 - output_original_loss: 0.0249 - output_noisy_loss: 0.0346\n",
            "Epoch 80: val_loss improved from 0.09709 to 0.09375, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0689 - output_original_loss: 0.0284 - output_noisy_loss: 0.0348 - val_loss: 0.0937 - val_output_original_loss: 0.0517 - val_output_noisy_loss: 0.0364 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0612 - output_original_loss: 0.0206 - output_noisy_loss: 0.0349\n",
            "Epoch 81: val_loss improved from 0.09375 to 0.09240, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0743 - output_original_loss: 0.0330 - output_noisy_loss: 0.0356 - val_loss: 0.0924 - val_output_original_loss: 0.0509 - val_output_noisy_loss: 0.0358 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0746 - output_original_loss: 0.0357 - output_noisy_loss: 0.0332\n",
            "Epoch 82: val_loss improved from 0.09240 to 0.08791, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0664 - output_original_loss: 0.0269 - output_noisy_loss: 0.0338 - val_loss: 0.0879 - val_output_original_loss: 0.0469 - val_output_noisy_loss: 0.0353 - lr: 0.0010\n",
            "Epoch 83/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0696 - output_original_loss: 0.0302 - output_noisy_loss: 0.0337\n",
            "Epoch 83: val_loss improved from 0.08791 to 0.08548, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0691 - output_original_loss: 0.0297 - output_noisy_loss: 0.0337 - val_loss: 0.0855 - val_output_original_loss: 0.0450 - val_output_noisy_loss: 0.0348 - lr: 0.0010\n",
            "Epoch 84/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0720 - output_original_loss: 0.0335 - output_noisy_loss: 0.0328\n",
            "Epoch 84: val_loss improved from 0.08548 to 0.08435, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0700 - output_original_loss: 0.0312 - output_noisy_loss: 0.0330 - val_loss: 0.0844 - val_output_original_loss: 0.0444 - val_output_noisy_loss: 0.0342 - lr: 0.0010\n",
            "Epoch 85/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0642 - output_original_loss: 0.0252 - output_noisy_loss: 0.0333\n",
            "Epoch 85: val_loss improved from 0.08435 to 0.08133, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0639 - output_original_loss: 0.0257 - output_noisy_loss: 0.0325 - val_loss: 0.0813 - val_output_original_loss: 0.0420 - val_output_noisy_loss: 0.0337 - lr: 0.0010\n",
            "Epoch 86/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0615 - output_original_loss: 0.0238 - output_noisy_loss: 0.0321\n",
            "Epoch 86: val_loss did not improve from 0.08133\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0628 - output_original_loss: 0.0250 - output_noisy_loss: 0.0321 - val_loss: 0.0825 - val_output_original_loss: 0.0437 - val_output_noisy_loss: 0.0331 - lr: 0.0010\n",
            "Epoch 87/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0534 - output_original_loss: 0.0162 - output_noisy_loss: 0.0314\n",
            "Epoch 87: val_loss improved from 0.08133 to 0.07890, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0652 - output_original_loss: 0.0273 - output_noisy_loss: 0.0322 - val_loss: 0.0789 - val_output_original_loss: 0.0406 - val_output_noisy_loss: 0.0326 - lr: 0.0010\n",
            "Epoch 88/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0600 - output_original_loss: 0.0246 - output_noisy_loss: 0.0297\n",
            "Epoch 88: val_loss improved from 0.07890 to 0.07700, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0606 - output_original_loss: 0.0245 - output_noisy_loss: 0.0304 - val_loss: 0.0770 - val_output_original_loss: 0.0391 - val_output_noisy_loss: 0.0322 - lr: 0.0010\n",
            "Epoch 89/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0615 - output_original_loss: 0.0248 - output_noisy_loss: 0.0310\n",
            "Epoch 89: val_loss did not improve from 0.07700\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0639 - output_original_loss: 0.0269 - output_noisy_loss: 0.0313 - val_loss: 0.0771 - val_output_original_loss: 0.0397 - val_output_noisy_loss: 0.0317 - lr: 0.0010\n",
            "Epoch 90/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0626 - output_original_loss: 0.0265 - output_noisy_loss: 0.0304\n",
            "Epoch 90: val_loss improved from 0.07700 to 0.07287, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0615 - output_original_loss: 0.0258 - output_noisy_loss: 0.0300 - val_loss: 0.0729 - val_output_original_loss: 0.0359 - val_output_noisy_loss: 0.0312 - lr: 0.0010\n",
            "Epoch 91/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0535 - output_original_loss: 0.0174 - output_noisy_loss: 0.0303\n",
            "Epoch 91: val_loss improved from 0.07287 to 0.07135, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0498 - output_original_loss: 0.0145 - output_noisy_loss: 0.0296 - val_loss: 0.0713 - val_output_original_loss: 0.0349 - val_output_noisy_loss: 0.0308 - lr: 0.0010\n",
            "Epoch 92/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0580 - output_original_loss: 0.0227 - output_noisy_loss: 0.0296\n",
            "Epoch 92: val_loss did not improve from 0.07135\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0616 - output_original_loss: 0.0265 - output_noisy_loss: 0.0294 - val_loss: 0.0724 - val_output_original_loss: 0.0364 - val_output_noisy_loss: 0.0303 - lr: 0.0010\n",
            "Epoch 93/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0537 - output_original_loss: 0.0179 - output_noisy_loss: 0.0301\n",
            "Epoch 93: val_loss did not improve from 0.07135\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0538 - output_original_loss: 0.0190 - output_noisy_loss: 0.0291 - val_loss: 0.0722 - val_output_original_loss: 0.0366 - val_output_noisy_loss: 0.0299 - lr: 0.0010\n",
            "Epoch 94/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0498 - output_original_loss: 0.0158 - output_noisy_loss: 0.0283\n",
            "Epoch 94: val_loss did not improve from 0.07135\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0574 - output_original_loss: 0.0231 - output_noisy_loss: 0.0286 - val_loss: 0.0739 - val_output_original_loss: 0.0387 - val_output_noisy_loss: 0.0295 - lr: 0.0010\n",
            "Epoch 95/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0605 - output_original_loss: 0.0267 - output_noisy_loss: 0.0281\n",
            "Epoch 95: val_loss did not improve from 0.07135\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0630 - output_original_loss: 0.0290 - output_noisy_loss: 0.0283 - val_loss: 0.0867 - val_output_original_loss: 0.0518 - val_output_noisy_loss: 0.0292 - lr: 0.0010\n",
            "Epoch 96/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0819 - output_original_loss: 0.0475 - output_noisy_loss: 0.0287\n",
            "Epoch 96: val_loss improved from 0.07135 to 0.06990, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0678 - output_original_loss: 0.0339 - output_noisy_loss: 0.0282 - val_loss: 0.0699 - val_output_original_loss: 0.0354 - val_output_noisy_loss: 0.0288 - lr: 0.0010\n",
            "Epoch 97/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0555 - output_original_loss: 0.0217 - output_noisy_loss: 0.0281\n",
            "Epoch 97: val_loss did not improve from 0.06990\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0609 - output_original_loss: 0.0280 - output_noisy_loss: 0.0272 - val_loss: 0.0782 - val_output_original_loss: 0.0439 - val_output_noisy_loss: 0.0285 - lr: 0.0010\n",
            "Epoch 98/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0647 - output_original_loss: 0.0310 - output_noisy_loss: 0.0280\n",
            "Epoch 98: val_loss did not improve from 0.06990\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0577 - output_original_loss: 0.0246 - output_noisy_loss: 0.0274 - val_loss: 0.0745 - val_output_original_loss: 0.0404 - val_output_noisy_loss: 0.0283 - lr: 0.0010\n",
            "Epoch 99/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0718 - output_original_loss: 0.0385 - output_noisy_loss: 0.0276\n",
            "Epoch 99: val_loss improved from 0.06990 to 0.06369, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0555 - output_original_loss: 0.0225 - output_noisy_loss: 0.0272 - val_loss: 0.0637 - val_output_original_loss: 0.0299 - val_output_noisy_loss: 0.0281 - lr: 0.0010\n",
            "Epoch 100/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0528 - output_original_loss: 0.0198 - output_noisy_loss: 0.0273\n",
            "Epoch 100: val_loss did not improve from 0.06369\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0513 - output_original_loss: 0.0187 - output_noisy_loss: 0.0268 - val_loss: 0.0644 - val_output_original_loss: 0.0308 - val_output_noisy_loss: 0.0278 - lr: 0.0010\n",
            "Epoch 101/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0529 - output_original_loss: 0.0198 - output_noisy_loss: 0.0274\n",
            "Epoch 101: val_loss improved from 0.06369 to 0.06240, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0506 - output_original_loss: 0.0182 - output_noisy_loss: 0.0267 - val_loss: 0.0624 - val_output_original_loss: 0.0292 - val_output_noisy_loss: 0.0275 - lr: 0.0010\n",
            "Epoch 102/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0501 - output_original_loss: 0.0180 - output_noisy_loss: 0.0264\n",
            "Epoch 102: val_loss did not improve from 0.06240\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0485 - output_original_loss: 0.0166 - output_noisy_loss: 0.0261 - val_loss: 0.0629 - val_output_original_loss: 0.0300 - val_output_noisy_loss: 0.0272 - lr: 0.0010\n",
            "Epoch 103/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0564 - output_original_loss: 0.0238 - output_noisy_loss: 0.0269\n",
            "Epoch 103: val_loss improved from 0.06240 to 0.05986, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0479 - output_original_loss: 0.0162 - output_noisy_loss: 0.0260 - val_loss: 0.0599 - val_output_original_loss: 0.0273 - val_output_noisy_loss: 0.0268 - lr: 0.0010\n",
            "Epoch 104/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0481 - output_original_loss: 0.0163 - output_noisy_loss: 0.0261\n",
            "Epoch 104: val_loss improved from 0.05986 to 0.05760, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0471 - output_original_loss: 0.0158 - output_noisy_loss: 0.0256 - val_loss: 0.0576 - val_output_original_loss: 0.0253 - val_output_noisy_loss: 0.0265 - lr: 0.0010\n",
            "Epoch 105/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0428 - output_original_loss: 0.0107 - output_noisy_loss: 0.0263\n",
            "Epoch 105: val_loss improved from 0.05760 to 0.05512, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0442 - output_original_loss: 0.0131 - output_noisy_loss: 0.0254 - val_loss: 0.0551 - val_output_original_loss: 0.0231 - val_output_noisy_loss: 0.0262 - lr: 0.0010\n",
            "Epoch 106/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0463 - output_original_loss: 0.0142 - output_noisy_loss: 0.0264\n",
            "Epoch 106: val_loss improved from 0.05512 to 0.05466, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0450 - output_original_loss: 0.0141 - output_noisy_loss: 0.0252 - val_loss: 0.0547 - val_output_original_loss: 0.0230 - val_output_noisy_loss: 0.0259 - lr: 0.0010\n",
            "Epoch 107/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0418 - output_original_loss: 0.0108 - output_noisy_loss: 0.0252\n",
            "Epoch 107: val_loss improved from 0.05466 to 0.05443, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0427 - output_original_loss: 0.0121 - output_noisy_loss: 0.0249 - val_loss: 0.0544 - val_output_original_loss: 0.0231 - val_output_noisy_loss: 0.0256 - lr: 0.0010\n",
            "Epoch 108/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0426 - output_original_loss: 0.0122 - output_noisy_loss: 0.0247\n",
            "Epoch 108: val_loss improved from 0.05443 to 0.05244, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0417 - output_original_loss: 0.0114 - output_noisy_loss: 0.0245 - val_loss: 0.0524 - val_output_original_loss: 0.0214 - val_output_noisy_loss: 0.0253 - lr: 0.0010\n",
            "Epoch 109/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0449 - output_original_loss: 0.0148 - output_noisy_loss: 0.0243\n",
            "Epoch 109: val_loss improved from 0.05244 to 0.05187, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0431 - output_original_loss: 0.0132 - output_noisy_loss: 0.0241 - val_loss: 0.0519 - val_output_original_loss: 0.0211 - val_output_noisy_loss: 0.0250 - lr: 0.0010\n",
            "Epoch 110/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0406 - output_original_loss: 0.0107 - output_noisy_loss: 0.0242\n",
            "Epoch 110: val_loss did not improve from 0.05187\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0418 - output_original_loss: 0.0121 - output_noisy_loss: 0.0240 - val_loss: 0.0542 - val_output_original_loss: 0.0238 - val_output_noisy_loss: 0.0247 - lr: 0.0010\n",
            "Epoch 111/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0439 - output_original_loss: 0.0144 - output_noisy_loss: 0.0237\n",
            "Epoch 111: val_loss did not improve from 0.05187\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0413 - output_original_loss: 0.0122 - output_noisy_loss: 0.0234 - val_loss: 0.0523 - val_output_original_loss: 0.0221 - val_output_noisy_loss: 0.0244 - lr: 0.0010\n",
            "Epoch 112/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0459 - output_original_loss: 0.0160 - output_noisy_loss: 0.0241\n",
            "Epoch 112: val_loss improved from 0.05187 to 0.04955, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0431 - output_original_loss: 0.0136 - output_noisy_loss: 0.0238 - val_loss: 0.0495 - val_output_original_loss: 0.0196 - val_output_noisy_loss: 0.0242 - lr: 0.0010\n",
            "Epoch 113/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0365 - output_original_loss: 0.0078 - output_noisy_loss: 0.0230\n",
            "Epoch 113: val_loss improved from 0.04955 to 0.04954, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0382 - output_original_loss: 0.0097 - output_noisy_loss: 0.0228 - val_loss: 0.0495 - val_output_original_loss: 0.0199 - val_output_noisy_loss: 0.0239 - lr: 0.0010\n",
            "Epoch 114/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0397 - output_original_loss: 0.0105 - output_noisy_loss: 0.0234\n",
            "Epoch 114: val_loss did not improve from 0.04954\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0418 - output_original_loss: 0.0130 - output_noisy_loss: 0.0229 - val_loss: 0.0529 - val_output_original_loss: 0.0235 - val_output_noisy_loss: 0.0236 - lr: 0.0010\n",
            "Epoch 115/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0414 - output_original_loss: 0.0125 - output_noisy_loss: 0.0232\n",
            "Epoch 115: val_loss did not improve from 0.04954\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0406 - output_original_loss: 0.0122 - output_noisy_loss: 0.0226 - val_loss: 0.0508 - val_output_original_loss: 0.0217 - val_output_noisy_loss: 0.0234 - lr: 0.0010\n",
            "Epoch 116/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0403 - output_original_loss: 0.0120 - output_noisy_loss: 0.0225\n",
            "Epoch 116: val_loss did not improve from 0.04954\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0382 - output_original_loss: 0.0102 - output_noisy_loss: 0.0222 - val_loss: 0.0574 - val_output_original_loss: 0.0286 - val_output_noisy_loss: 0.0231 - lr: 0.0010\n",
            "Epoch 117/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0537 - output_original_loss: 0.0252 - output_noisy_loss: 0.0227\n",
            "Epoch 117: val_loss did not improve from 0.04954\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0455 - output_original_loss: 0.0173 - output_noisy_loss: 0.0224 - val_loss: 0.0559 - val_output_original_loss: 0.0272 - val_output_noisy_loss: 0.0229 - lr: 0.0010\n",
            "Epoch 118/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0426 - output_original_loss: 0.0152 - output_noisy_loss: 0.0216\n",
            "Epoch 118: val_loss did not improve from 0.04954\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0410 - output_original_loss: 0.0131 - output_noisy_loss: 0.0222 - val_loss: 0.0523 - val_output_original_loss: 0.0239 - val_output_noisy_loss: 0.0227 - lr: 0.0010\n",
            "Epoch 119/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0420 - output_original_loss: 0.0148 - output_noisy_loss: 0.0215\n",
            "Epoch 119: val_loss did not improve from 0.04954\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0415 - output_original_loss: 0.0140 - output_noisy_loss: 0.0217 - val_loss: 0.0513 - val_output_original_loss: 0.0231 - val_output_noisy_loss: 0.0224 - lr: 0.0010\n",
            "Epoch 120/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0418 - output_original_loss: 0.0148 - output_noisy_loss: 0.0212\n",
            "Epoch 120: val_loss improved from 0.04954 to 0.04639, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0443 - output_original_loss: 0.0170 - output_noisy_loss: 0.0215 - val_loss: 0.0464 - val_output_original_loss: 0.0183 - val_output_noisy_loss: 0.0223 - lr: 0.0010\n",
            "Epoch 121/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0377 - output_original_loss: 0.0104 - output_noisy_loss: 0.0215\n",
            "Epoch 121: val_loss improved from 0.04639 to 0.04461, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0421 - output_original_loss: 0.0149 - output_noisy_loss: 0.0214 - val_loss: 0.0446 - val_output_original_loss: 0.0167 - val_output_noisy_loss: 0.0221 - lr: 0.0010\n",
            "Epoch 122/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0378 - output_original_loss: 0.0102 - output_noisy_loss: 0.0218\n",
            "Epoch 122: val_loss improved from 0.04461 to 0.04439, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0406 - output_original_loss: 0.0138 - output_noisy_loss: 0.0210 - val_loss: 0.0444 - val_output_original_loss: 0.0167 - val_output_noisy_loss: 0.0219 - lr: 0.0010\n",
            "Epoch 123/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0339 - output_original_loss: 0.0070 - output_noisy_loss: 0.0211\n",
            "Epoch 123: val_loss did not improve from 0.04439\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0383 - output_original_loss: 0.0116 - output_noisy_loss: 0.0210 - val_loss: 0.0446 - val_output_original_loss: 0.0170 - val_output_noisy_loss: 0.0217 - lr: 0.0010\n",
            "Epoch 124/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0396 - output_original_loss: 0.0122 - output_noisy_loss: 0.0216\n",
            "Epoch 124: val_loss did not improve from 0.04439\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0419 - output_original_loss: 0.0151 - output_noisy_loss: 0.0211 - val_loss: 0.0458 - val_output_original_loss: 0.0184 - val_output_noisy_loss: 0.0216 - lr: 0.0010\n",
            "Epoch 125/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0373 - output_original_loss: 0.0108 - output_noisy_loss: 0.0208\n",
            "Epoch 125: val_loss improved from 0.04439 to 0.04334, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0369 - output_original_loss: 0.0107 - output_noisy_loss: 0.0204 - val_loss: 0.0433 - val_output_original_loss: 0.0162 - val_output_noisy_loss: 0.0214 - lr: 0.0010\n",
            "Epoch 126/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0386 - output_original_loss: 0.0117 - output_noisy_loss: 0.0212\n",
            "Epoch 126: val_loss improved from 0.04334 to 0.04251, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0369 - output_original_loss: 0.0103 - output_noisy_loss: 0.0208 - val_loss: 0.0425 - val_output_original_loss: 0.0155 - val_output_noisy_loss: 0.0212 - lr: 0.0010\n",
            "Epoch 127/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0320 - output_original_loss: 0.0063 - output_noisy_loss: 0.0199\n",
            "Epoch 127: val_loss improved from 0.04251 to 0.04234, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0352 - output_original_loss: 0.0089 - output_noisy_loss: 0.0204 - val_loss: 0.0423 - val_output_original_loss: 0.0155 - val_output_noisy_loss: 0.0210 - lr: 0.0010\n",
            "Epoch 128/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0308 - output_original_loss: 0.0055 - output_noisy_loss: 0.0196\n",
            "Epoch 128: val_loss improved from 0.04234 to 0.04140, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0369 - output_original_loss: 0.0111 - output_noisy_loss: 0.0201 - val_loss: 0.0414 - val_output_original_loss: 0.0148 - val_output_noisy_loss: 0.0208 - lr: 0.0010\n",
            "Epoch 129/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0314 - output_original_loss: 0.0058 - output_noisy_loss: 0.0197\n",
            "Epoch 129: val_loss improved from 0.04140 to 0.04103, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0362 - output_original_loss: 0.0106 - output_noisy_loss: 0.0198 - val_loss: 0.0410 - val_output_original_loss: 0.0146 - val_output_noisy_loss: 0.0207 - lr: 0.0010\n",
            "Epoch 130/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0309 - output_original_loss: 0.0050 - output_noisy_loss: 0.0200\n",
            "Epoch 130: val_loss did not improve from 0.04103\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0378 - output_original_loss: 0.0120 - output_noisy_loss: 0.0200 - val_loss: 0.0434 - val_output_original_loss: 0.0171 - val_output_noisy_loss: 0.0205 - lr: 0.0010\n",
            "Epoch 131/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0357 - output_original_loss: 0.0099 - output_noisy_loss: 0.0200\n",
            "Epoch 131: val_loss did not improve from 0.04103\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0352 - output_original_loss: 0.0099 - output_noisy_loss: 0.0195 - val_loss: 0.0576 - val_output_original_loss: 0.0315 - val_output_noisy_loss: 0.0203 - lr: 0.0010\n",
            "Epoch 132/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0523 - output_original_loss: 0.0267 - output_noisy_loss: 0.0198\n",
            "Epoch 132: val_loss did not improve from 0.04103\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0420 - output_original_loss: 0.0168 - output_noisy_loss: 0.0195 - val_loss: 0.0734 - val_output_original_loss: 0.0474 - val_output_noisy_loss: 0.0202 - lr: 0.0010\n",
            "Epoch 133/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0608 - output_original_loss: 0.0354 - output_noisy_loss: 0.0196\n",
            "Epoch 133: val_loss did not improve from 0.04103\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0471 - output_original_loss: 0.0219 - output_noisy_loss: 0.0194 - val_loss: 0.0566 - val_output_original_loss: 0.0307 - val_output_noisy_loss: 0.0201 - lr: 0.0010\n",
            "Epoch 134/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0485 - output_original_loss: 0.0238 - output_noisy_loss: 0.0189\n",
            "Epoch 134: val_loss did not improve from 0.04103\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0417 - output_original_loss: 0.0167 - output_noisy_loss: 0.0192 - val_loss: 0.0474 - val_output_original_loss: 0.0216 - val_output_noisy_loss: 0.0200 - lr: 0.0010\n",
            "Epoch 135/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0404 - output_original_loss: 0.0150 - output_noisy_loss: 0.0196\n",
            "Epoch 135: val_loss did not improve from 0.04103\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0349 - output_original_loss: 0.0097 - output_noisy_loss: 0.0194 - val_loss: 0.0436 - val_output_original_loss: 0.0179 - val_output_noisy_loss: 0.0199 - lr: 0.0010\n",
            "Epoch 136/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0363 - output_original_loss: 0.0115 - output_noisy_loss: 0.0190\n",
            "Epoch 136: val_loss did not improve from 0.04103\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0347 - output_original_loss: 0.0099 - output_noisy_loss: 0.0190 - val_loss: 0.0426 - val_output_original_loss: 0.0170 - val_output_noisy_loss: 0.0198 - lr: 0.0010\n",
            "Epoch 137/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0337 - output_original_loss: 0.0089 - output_noisy_loss: 0.0190\n",
            "Epoch 137: val_loss improved from 0.04103 to 0.04016, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0334 - output_original_loss: 0.0087 - output_noisy_loss: 0.0190 - val_loss: 0.0402 - val_output_original_loss: 0.0148 - val_output_noisy_loss: 0.0196 - lr: 0.0010\n",
            "Epoch 138/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0322 - output_original_loss: 0.0078 - output_noisy_loss: 0.0186\n",
            "Epoch 138: val_loss improved from 0.04016 to 0.03973, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0316 - output_original_loss: 0.0071 - output_noisy_loss: 0.0187 - val_loss: 0.0397 - val_output_original_loss: 0.0145 - val_output_noisy_loss: 0.0195 - lr: 0.0010\n",
            "Epoch 139/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0309 - output_original_loss: 0.0064 - output_noisy_loss: 0.0187\n",
            "Epoch 139: val_loss improved from 0.03973 to 0.03861, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0314 - output_original_loss: 0.0069 - output_noisy_loss: 0.0187 - val_loss: 0.0386 - val_output_original_loss: 0.0135 - val_output_noisy_loss: 0.0193 - lr: 0.0010\n",
            "Epoch 140/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0309 - output_original_loss: 0.0066 - output_noisy_loss: 0.0186\n",
            "Epoch 140: val_loss did not improve from 0.03861\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0312 - output_original_loss: 0.0070 - output_noisy_loss: 0.0184 - val_loss: 0.0392 - val_output_original_loss: 0.0143 - val_output_noisy_loss: 0.0191 - lr: 0.0010\n",
            "Epoch 141/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0315 - output_original_loss: 0.0068 - output_noisy_loss: 0.0189\n",
            "Epoch 141: val_loss did not improve from 0.03861\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.0333 - output_original_loss: 0.0092 - output_noisy_loss: 0.0183 - val_loss: 0.0415 - val_output_original_loss: 0.0168 - val_output_noisy_loss: 0.0190 - lr: 0.0010\n",
            "Epoch 142/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0349 - output_original_loss: 0.0105 - output_noisy_loss: 0.0186\n",
            "Epoch 142: val_loss did not improve from 0.03861\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.0328 - output_original_loss: 0.0088 - output_noisy_loss: 0.0183 - val_loss: 0.0394 - val_output_original_loss: 0.0148 - val_output_noisy_loss: 0.0188 - lr: 0.0010\n",
            "Epoch 143/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0318 - output_original_loss: 0.0077 - output_noisy_loss: 0.0183\n",
            "Epoch 143: val_loss improved from 0.03861 to 0.03842, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0316 - output_original_loss: 0.0078 - output_noisy_loss: 0.0180 - val_loss: 0.0384 - val_output_original_loss: 0.0139 - val_output_noisy_loss: 0.0187 - lr: 0.0010\n",
            "Epoch 144/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0342 - output_original_loss: 0.0100 - output_noisy_loss: 0.0185\n",
            "Epoch 144: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0388 - output_original_loss: 0.0151 - output_noisy_loss: 0.0179 - val_loss: 0.0409 - val_output_original_loss: 0.0165 - val_output_noisy_loss: 0.0186 - lr: 0.0010\n",
            "Epoch 145/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0337 - output_original_loss: 0.0098 - output_noisy_loss: 0.0181\n",
            "Epoch 145: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.0377 - output_original_loss: 0.0141 - output_noisy_loss: 0.0179 - val_loss: 0.0574 - val_output_original_loss: 0.0331 - val_output_noisy_loss: 0.0184 - lr: 0.0010\n",
            "Epoch 146/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0515 - output_original_loss: 0.0276 - output_noisy_loss: 0.0181\n",
            "Epoch 146: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0394 - output_original_loss: 0.0158 - output_noisy_loss: 0.0178 - val_loss: 0.0445 - val_output_original_loss: 0.0204 - val_output_noisy_loss: 0.0183 - lr: 0.0010\n",
            "Epoch 147/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0364 - output_original_loss: 0.0127 - output_noisy_loss: 0.0179\n",
            "Epoch 147: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.0327 - output_original_loss: 0.0094 - output_noisy_loss: 0.0176 - val_loss: 0.0445 - val_output_original_loss: 0.0204 - val_output_noisy_loss: 0.0182 - lr: 0.0010\n",
            "Epoch 148/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0394 - output_original_loss: 0.0158 - output_noisy_loss: 0.0178\n",
            "Epoch 148: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0353 - output_original_loss: 0.0121 - output_noisy_loss: 0.0174 - val_loss: 0.0474 - val_output_original_loss: 0.0235 - val_output_noisy_loss: 0.0181 - lr: 0.0010\n",
            "Epoch 149/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0302 - output_original_loss: 0.0065 - output_noisy_loss: 0.0180\n",
            "Epoch 149: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0390 - output_original_loss: 0.0158 - output_noisy_loss: 0.0174 - val_loss: 0.0549 - val_output_original_loss: 0.0311 - val_output_noisy_loss: 0.0180 - lr: 0.0010\n",
            "Epoch 150/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0485 - output_original_loss: 0.0253 - output_noisy_loss: 0.0174\n",
            "Epoch 150: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0428 - output_original_loss: 0.0199 - output_noisy_loss: 0.0171 - val_loss: 0.0538 - val_output_original_loss: 0.0301 - val_output_noisy_loss: 0.0179 - lr: 0.0010\n",
            "Epoch 151/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0518 - output_original_loss: 0.0283 - output_noisy_loss: 0.0176\n",
            "Epoch 151: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0442 - output_original_loss: 0.0210 - output_noisy_loss: 0.0174 - val_loss: 0.0486 - val_output_original_loss: 0.0250 - val_output_noisy_loss: 0.0178 - lr: 0.0010\n",
            "Epoch 152/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0383 - output_original_loss: 0.0152 - output_noisy_loss: 0.0173\n",
            "Epoch 152: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0433 - output_original_loss: 0.0203 - output_noisy_loss: 0.0172 - val_loss: 0.0399 - val_output_original_loss: 0.0163 - val_output_noisy_loss: 0.0177 - lr: 0.0010\n",
            "Epoch 153/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0315 - output_original_loss: 0.0090 - output_noisy_loss: 0.0168\n",
            "Epoch 153: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0331 - output_original_loss: 0.0103 - output_noisy_loss: 0.0169 - val_loss: 0.0387 - val_output_original_loss: 0.0152 - val_output_noisy_loss: 0.0177 - lr: 0.0010\n",
            "Epoch 154/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0322 - output_original_loss: 0.0097 - output_noisy_loss: 0.0167\n",
            "Epoch 154: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0336 - output_original_loss: 0.0108 - output_noisy_loss: 0.0169 - val_loss: 0.0412 - val_output_original_loss: 0.0179 - val_output_noisy_loss: 0.0175 - lr: 0.0010\n",
            "Epoch 155/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0358 - output_original_loss: 0.0130 - output_noisy_loss: 0.0170\n",
            "Epoch 155: val_loss did not improve from 0.03842\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0309 - output_original_loss: 0.0084 - output_noisy_loss: 0.0168 - val_loss: 0.0425 - val_output_original_loss: 0.0192 - val_output_noisy_loss: 0.0174 - lr: 0.0010\n",
            "Epoch 156/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0322 - output_original_loss: 0.0096 - output_noisy_loss: 0.0167\n",
            "Epoch 156: val_loss improved from 0.03842 to 0.03734, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0302 - output_original_loss: 0.0076 - output_noisy_loss: 0.0168 - val_loss: 0.0373 - val_output_original_loss: 0.0142 - val_output_noisy_loss: 0.0173 - lr: 0.0010\n",
            "Epoch 157/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0291 - output_original_loss: 0.0066 - output_noisy_loss: 0.0168\n",
            "Epoch 157: val_loss did not improve from 0.03734\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0299 - output_original_loss: 0.0074 - output_noisy_loss: 0.0168 - val_loss: 0.0379 - val_output_original_loss: 0.0148 - val_output_noisy_loss: 0.0172 - lr: 0.0010\n",
            "Epoch 158/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0313 - output_original_loss: 0.0088 - output_noisy_loss: 0.0167\n",
            "Epoch 158: val_loss improved from 0.03734 to 0.03614, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0299 - output_original_loss: 0.0077 - output_noisy_loss: 0.0165 - val_loss: 0.0361 - val_output_original_loss: 0.0132 - val_output_noisy_loss: 0.0171 - lr: 0.0010\n",
            "Epoch 159/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0292 - output_original_loss: 0.0065 - output_noisy_loss: 0.0169\n",
            "Epoch 159: val_loss did not improve from 0.03614\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0309 - output_original_loss: 0.0087 - output_noisy_loss: 0.0164 - val_loss: 0.0452 - val_output_original_loss: 0.0224 - val_output_noisy_loss: 0.0170 - lr: 0.0010\n",
            "Epoch 160/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0355 - output_original_loss: 0.0134 - output_noisy_loss: 0.0163\n",
            "Epoch 160: val_loss did not improve from 0.03614\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0319 - output_original_loss: 0.0096 - output_noisy_loss: 0.0166 - val_loss: 0.0442 - val_output_original_loss: 0.0215 - val_output_noisy_loss: 0.0169 - lr: 0.0010\n",
            "Epoch 161/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0369 - output_original_loss: 0.0151 - output_noisy_loss: 0.0159\n",
            "Epoch 161: val_loss did not improve from 0.03614\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0313 - output_original_loss: 0.0094 - output_noisy_loss: 0.0161 - val_loss: 0.0379 - val_output_original_loss: 0.0154 - val_output_noisy_loss: 0.0168 - lr: 0.0010\n",
            "Epoch 162/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0306 - output_original_loss: 0.0088 - output_noisy_loss: 0.0160\n",
            "Epoch 162: val_loss improved from 0.03614 to 0.03561, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0299 - output_original_loss: 0.0081 - output_noisy_loss: 0.0160 - val_loss: 0.0356 - val_output_original_loss: 0.0131 - val_output_noisy_loss: 0.0167 - lr: 0.0010\n",
            "Epoch 163/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0313 - output_original_loss: 0.0091 - output_noisy_loss: 0.0164\n",
            "Epoch 163: val_loss improved from 0.03561 to 0.03242, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0313 - output_original_loss: 0.0094 - output_noisy_loss: 0.0160 - val_loss: 0.0324 - val_output_original_loss: 0.0101 - val_output_noisy_loss: 0.0166 - lr: 0.0010\n",
            "Epoch 164/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0280 - output_original_loss: 0.0061 - output_noisy_loss: 0.0161\n",
            "Epoch 164: val_loss did not improve from 0.03242\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.0279 - output_original_loss: 0.0063 - output_noisy_loss: 0.0158 - val_loss: 0.0336 - val_output_original_loss: 0.0114 - val_output_noisy_loss: 0.0165 - lr: 0.0010\n",
            "Epoch 165/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0261 - output_original_loss: 0.0045 - output_noisy_loss: 0.0158\n",
            "Epoch 165: val_loss did not improve from 0.03242\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0279 - output_original_loss: 0.0063 - output_noisy_loss: 0.0158 - val_loss: 0.0360 - val_output_original_loss: 0.0138 - val_output_noisy_loss: 0.0163 - lr: 0.0010\n",
            "Epoch 166/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0318 - output_original_loss: 0.0099 - output_noisy_loss: 0.0161\n",
            "Epoch 166: val_loss did not improve from 0.03242\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.0286 - output_original_loss: 0.0072 - output_noisy_loss: 0.0156 - val_loss: 0.0380 - val_output_original_loss: 0.0159 - val_output_noisy_loss: 0.0162 - lr: 0.0010\n",
            "Epoch 167/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0312 - output_original_loss: 0.0099 - output_noisy_loss: 0.0154\n",
            "Epoch 167: val_loss improved from 0.03242 to 0.03168, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0290 - output_original_loss: 0.0076 - output_noisy_loss: 0.0156 - val_loss: 0.0317 - val_output_original_loss: 0.0097 - val_output_noisy_loss: 0.0162 - lr: 0.0010\n",
            "Epoch 168/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0284 - output_original_loss: 0.0064 - output_noisy_loss: 0.0162\n",
            "Epoch 168: val_loss did not improve from 0.03168\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0268 - output_original_loss: 0.0055 - output_noisy_loss: 0.0155 - val_loss: 0.0344 - val_output_original_loss: 0.0126 - val_output_noisy_loss: 0.0160 - lr: 0.0010\n",
            "Epoch 169/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0303 - output_original_loss: 0.0088 - output_noisy_loss: 0.0157\n",
            "Epoch 169: val_loss did not improve from 0.03168\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0372 - output_original_loss: 0.0159 - output_noisy_loss: 0.0155 - val_loss: 0.0355 - val_output_original_loss: 0.0138 - val_output_noisy_loss: 0.0160 - lr: 0.0010\n",
            "Epoch 170/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0299 - output_original_loss: 0.0087 - output_noisy_loss: 0.0154\n",
            "Epoch 170: val_loss did not improve from 0.03168\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.0295 - output_original_loss: 0.0084 - output_noisy_loss: 0.0153 - val_loss: 0.0364 - val_output_original_loss: 0.0147 - val_output_noisy_loss: 0.0159 - lr: 0.0010\n",
            "Epoch 171/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0326 - output_original_loss: 0.0118 - output_noisy_loss: 0.0149\n",
            "Epoch 171: val_loss did not improve from 0.03168\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0286 - output_original_loss: 0.0077 - output_noisy_loss: 0.0152 - val_loss: 0.0328 - val_output_original_loss: 0.0112 - val_output_noisy_loss: 0.0158 - lr: 0.0010\n",
            "Epoch 172/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0261 - output_original_loss: 0.0051 - output_noisy_loss: 0.0152\n",
            "Epoch 172: val_loss improved from 0.03168 to 0.03033, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0264 - output_original_loss: 0.0054 - output_noisy_loss: 0.0152 - val_loss: 0.0303 - val_output_original_loss: 0.0088 - val_output_noisy_loss: 0.0157 - lr: 0.0010\n",
            "Epoch 173/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0261 - output_original_loss: 0.0049 - output_noisy_loss: 0.0154\n",
            "Epoch 173: val_loss did not improve from 0.03033\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0247 - output_original_loss: 0.0039 - output_noisy_loss: 0.0150 - val_loss: 0.0324 - val_output_original_loss: 0.0109 - val_output_noisy_loss: 0.0156 - lr: 0.0010\n",
            "Epoch 174/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0278 - output_original_loss: 0.0063 - output_noisy_loss: 0.0157\n",
            "Epoch 174: val_loss improved from 0.03033 to 0.02972, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0260 - output_original_loss: 0.0050 - output_noisy_loss: 0.0152 - val_loss: 0.0297 - val_output_original_loss: 0.0084 - val_output_noisy_loss: 0.0155 - lr: 0.0010\n",
            "Epoch 175/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0263 - output_original_loss: 0.0056 - output_noisy_loss: 0.0149\n",
            "Epoch 175: val_loss did not improve from 0.02972\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0257 - output_original_loss: 0.0050 - output_noisy_loss: 0.0148 - val_loss: 0.0301 - val_output_original_loss: 0.0089 - val_output_noisy_loss: 0.0154 - lr: 0.0010\n",
            "Epoch 176/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0239 - output_original_loss: 0.0031 - output_noisy_loss: 0.0150\n",
            "Epoch 176: val_loss did not improve from 0.02972\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0242 - output_original_loss: 0.0037 - output_noisy_loss: 0.0148 - val_loss: 0.0302 - val_output_original_loss: 0.0091 - val_output_noisy_loss: 0.0153 - lr: 0.0010\n",
            "Epoch 177/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0249 - output_original_loss: 0.0041 - output_noisy_loss: 0.0150\n",
            "Epoch 177: val_loss did not improve from 0.02972\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0249 - output_original_loss: 0.0042 - output_noisy_loss: 0.0149 - val_loss: 0.0297 - val_output_original_loss: 0.0087 - val_output_noisy_loss: 0.0152 - lr: 0.0010\n",
            "Epoch 178/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0230 - output_original_loss: 0.0028 - output_noisy_loss: 0.0145\n",
            "Epoch 178: val_loss did not improve from 0.02972\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0233 - output_original_loss: 0.0029 - output_noisy_loss: 0.0146 - val_loss: 0.0306 - val_output_original_loss: 0.0097 - val_output_noisy_loss: 0.0151 - lr: 0.0010\n",
            "Epoch 179/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0262 - output_original_loss: 0.0060 - output_noisy_loss: 0.0144\n",
            "Epoch 179: val_loss improved from 0.02972 to 0.02931, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0263 - output_original_loss: 0.0060 - output_noisy_loss: 0.0145 - val_loss: 0.0293 - val_output_original_loss: 0.0085 - val_output_noisy_loss: 0.0150 - lr: 0.0010\n",
            "Epoch 180/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0232 - output_original_loss: 0.0032 - output_noisy_loss: 0.0142\n",
            "Epoch 180: val_loss improved from 0.02931 to 0.02862, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0233 - output_original_loss: 0.0031 - output_noisy_loss: 0.0143 - val_loss: 0.0286 - val_output_original_loss: 0.0079 - val_output_noisy_loss: 0.0149 - lr: 0.0010\n",
            "Epoch 181/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0238 - output_original_loss: 0.0036 - output_noisy_loss: 0.0144\n",
            "Epoch 181: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0267 - output_original_loss: 0.0065 - output_noisy_loss: 0.0144 - val_loss: 0.0332 - val_output_original_loss: 0.0125 - val_output_noisy_loss: 0.0148 - lr: 0.0010\n",
            "Epoch 182/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0268 - output_original_loss: 0.0069 - output_noisy_loss: 0.0141\n",
            "Epoch 182: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0254 - output_original_loss: 0.0053 - output_noisy_loss: 0.0143 - val_loss: 0.0321 - val_output_original_loss: 0.0115 - val_output_noisy_loss: 0.0148 - lr: 0.0010\n",
            "Epoch 183/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0247 - output_original_loss: 0.0050 - output_noisy_loss: 0.0139\n",
            "Epoch 183: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0275 - output_original_loss: 0.0076 - output_noisy_loss: 0.0141 - val_loss: 0.0294 - val_output_original_loss: 0.0089 - val_output_noisy_loss: 0.0147 - lr: 0.0010\n",
            "Epoch 184/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0231 - output_original_loss: 0.0029 - output_noisy_loss: 0.0144\n",
            "Epoch 184: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0250 - output_original_loss: 0.0049 - output_noisy_loss: 0.0143 - val_loss: 0.0313 - val_output_original_loss: 0.0109 - val_output_noisy_loss: 0.0146 - lr: 0.0010\n",
            "Epoch 185/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0249 - output_original_loss: 0.0057 - output_noisy_loss: 0.0134\n",
            "Epoch 185: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0256 - output_original_loss: 0.0059 - output_noisy_loss: 0.0139 - val_loss: 0.0330 - val_output_original_loss: 0.0126 - val_output_noisy_loss: 0.0145 - lr: 0.0010\n",
            "Epoch 186/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0274 - output_original_loss: 0.0078 - output_noisy_loss: 0.0138\n",
            "Epoch 186: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0259 - output_original_loss: 0.0063 - output_noisy_loss: 0.0138 - val_loss: 0.0291 - val_output_original_loss: 0.0089 - val_output_noisy_loss: 0.0144 - lr: 0.0010\n",
            "Epoch 187/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0244 - output_original_loss: 0.0045 - output_noisy_loss: 0.0141\n",
            "Epoch 187: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0250 - output_original_loss: 0.0052 - output_noisy_loss: 0.0141 - val_loss: 0.0327 - val_output_original_loss: 0.0125 - val_output_noisy_loss: 0.0143 - lr: 0.0010\n",
            "Epoch 188/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0269 - output_original_loss: 0.0073 - output_noisy_loss: 0.0137\n",
            "Epoch 188: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0248 - output_original_loss: 0.0052 - output_noisy_loss: 0.0137 - val_loss: 0.0292 - val_output_original_loss: 0.0091 - val_output_noisy_loss: 0.0143 - lr: 0.0010\n",
            "Epoch 189/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0233 - output_original_loss: 0.0040 - output_noisy_loss: 0.0135\n",
            "Epoch 189: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0247 - output_original_loss: 0.0053 - output_noisy_loss: 0.0136 - val_loss: 0.0364 - val_output_original_loss: 0.0164 - val_output_noisy_loss: 0.0142 - lr: 0.0010\n",
            "Epoch 190/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0319 - output_original_loss: 0.0124 - output_noisy_loss: 0.0137\n",
            "Epoch 190: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0302 - output_original_loss: 0.0107 - output_noisy_loss: 0.0137 - val_loss: 0.0295 - val_output_original_loss: 0.0095 - val_output_noisy_loss: 0.0141 - lr: 0.0010\n",
            "Epoch 191/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0242 - output_original_loss: 0.0048 - output_noisy_loss: 0.0135\n",
            "Epoch 191: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0240 - output_original_loss: 0.0045 - output_noisy_loss: 0.0137 - val_loss: 0.0287 - val_output_original_loss: 0.0088 - val_output_noisy_loss: 0.0141 - lr: 0.0010\n",
            "Epoch 192/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0233 - output_original_loss: 0.0042 - output_noisy_loss: 0.0133\n",
            "Epoch 192: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0239 - output_original_loss: 0.0048 - output_noisy_loss: 0.0134 - val_loss: 0.0313 - val_output_original_loss: 0.0115 - val_output_noisy_loss: 0.0140 - lr: 0.0010\n",
            "Epoch 193/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0274 - output_original_loss: 0.0084 - output_noisy_loss: 0.0132\n",
            "Epoch 193: val_loss did not improve from 0.02862\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0271 - output_original_loss: 0.0077 - output_noisy_loss: 0.0136 - val_loss: 0.0290 - val_output_original_loss: 0.0093 - val_output_noisy_loss: 0.0139 - lr: 0.0010\n",
            "Epoch 194/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0226 - output_original_loss: 0.0037 - output_noisy_loss: 0.0131\n",
            "Epoch 194: val_loss improved from 0.02862 to 0.02832, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0235 - output_original_loss: 0.0043 - output_noisy_loss: 0.0133 - val_loss: 0.0283 - val_output_original_loss: 0.0087 - val_output_noisy_loss: 0.0138 - lr: 0.0010\n",
            "Epoch 195/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0238 - output_original_loss: 0.0048 - output_noisy_loss: 0.0133\n",
            "Epoch 195: val_loss improved from 0.02832 to 0.02776, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0222 - output_original_loss: 0.0032 - output_noisy_loss: 0.0133 - val_loss: 0.0278 - val_output_original_loss: 0.0082 - val_output_noisy_loss: 0.0138 - lr: 0.0010\n",
            "Epoch 196/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0239 - output_original_loss: 0.0046 - output_noisy_loss: 0.0135\n",
            "Epoch 196: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.0263 - output_original_loss: 0.0073 - output_noisy_loss: 0.0133 - val_loss: 0.0342 - val_output_original_loss: 0.0147 - val_output_noisy_loss: 0.0137 - lr: 0.0010\n",
            "Epoch 197/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0295 - output_original_loss: 0.0103 - output_noisy_loss: 0.0134\n",
            "Epoch 197: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0280 - output_original_loss: 0.0090 - output_noisy_loss: 0.0132 - val_loss: 0.0314 - val_output_original_loss: 0.0119 - val_output_noisy_loss: 0.0136 - lr: 0.0010\n",
            "Epoch 198/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0275 - output_original_loss: 0.0086 - output_noisy_loss: 0.0130\n",
            "Epoch 198: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0257 - output_original_loss: 0.0068 - output_noisy_loss: 0.0131 - val_loss: 0.0313 - val_output_original_loss: 0.0119 - val_output_noisy_loss: 0.0136 - lr: 0.0010\n",
            "Epoch 199/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0287 - output_original_loss: 0.0095 - output_noisy_loss: 0.0135\n",
            "Epoch 199: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0298 - output_original_loss: 0.0108 - output_noisy_loss: 0.0131 - val_loss: 0.0387 - val_output_original_loss: 0.0193 - val_output_noisy_loss: 0.0136 - lr: 0.0010\n",
            "Epoch 200/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0316 - output_original_loss: 0.0131 - output_noisy_loss: 0.0127\n",
            "Epoch 200: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0326 - output_original_loss: 0.0138 - output_noisy_loss: 0.0130 - val_loss: 0.0351 - val_output_original_loss: 0.0158 - val_output_noisy_loss: 0.0135 - lr: 0.0010\n",
            "Epoch 201/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0319 - output_original_loss: 0.0129 - output_noisy_loss: 0.0132\n",
            "Epoch 201: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0386 - output_original_loss: 0.0198 - output_noisy_loss: 0.0130 - val_loss: 0.0439 - val_output_original_loss: 0.0246 - val_output_noisy_loss: 0.0134 - lr: 0.0010\n",
            "Epoch 202/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0396 - output_original_loss: 0.0207 - output_noisy_loss: 0.0131\n",
            "Epoch 202: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0308 - output_original_loss: 0.0120 - output_noisy_loss: 0.0130 - val_loss: 0.0348 - val_output_original_loss: 0.0155 - val_output_noisy_loss: 0.0134 - lr: 0.0010\n",
            "Epoch 203/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0298 - output_original_loss: 0.0114 - output_noisy_loss: 0.0126\n",
            "Epoch 203: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0433 - output_original_loss: 0.0246 - output_noisy_loss: 0.0130 - val_loss: 0.0418 - val_output_original_loss: 0.0225 - val_output_noisy_loss: 0.0135 - lr: 0.0010\n",
            "Epoch 204/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0346 - output_original_loss: 0.0160 - output_noisy_loss: 0.0128\n",
            "Epoch 204: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0307 - output_original_loss: 0.0120 - output_noisy_loss: 0.0129 - val_loss: 0.0513 - val_output_original_loss: 0.0320 - val_output_noisy_loss: 0.0135 - lr: 0.0010\n",
            "Epoch 205/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0478 - output_original_loss: 0.0293 - output_noisy_loss: 0.0127\n",
            "Epoch 205: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0455 - output_original_loss: 0.0266 - output_noisy_loss: 0.0130 - val_loss: 0.0308 - val_output_original_loss: 0.0115 - val_output_noisy_loss: 0.0135 - lr: 0.0010\n",
            "Epoch 206/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0250 - output_original_loss: 0.0065 - output_noisy_loss: 0.0127\n",
            "Epoch 206: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0347 - output_original_loss: 0.0160 - output_noisy_loss: 0.0129 - val_loss: 0.0548 - val_output_original_loss: 0.0356 - val_output_noisy_loss: 0.0135 - lr: 0.0010\n",
            "Epoch 207/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0500 - output_original_loss: 0.0312 - output_noisy_loss: 0.0129\n",
            "Epoch 207: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0358 - output_original_loss: 0.0172 - output_noisy_loss: 0.0128 - val_loss: 0.0444 - val_output_original_loss: 0.0252 - val_output_noisy_loss: 0.0135 - lr: 0.0010\n",
            "Epoch 208/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0396 - output_original_loss: 0.0203 - output_noisy_loss: 0.0135\n",
            "Epoch 208: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0357 - output_original_loss: 0.0167 - output_noisy_loss: 0.0132 - val_loss: 0.0316 - val_output_original_loss: 0.0123 - val_output_noisy_loss: 0.0134 - lr: 0.0010\n",
            "Epoch 209/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0281 - output_original_loss: 0.0095 - output_noisy_loss: 0.0127\n",
            "Epoch 209: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0354 - output_original_loss: 0.0167 - output_noisy_loss: 0.0130 - val_loss: 0.0286 - val_output_original_loss: 0.0094 - val_output_noisy_loss: 0.0134 - lr: 0.0010\n",
            "Epoch 210/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0236 - output_original_loss: 0.0051 - output_noisy_loss: 0.0127\n",
            "Epoch 210: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0317 - output_original_loss: 0.0129 - output_noisy_loss: 0.0129 - val_loss: 0.0328 - val_output_original_loss: 0.0135 - val_output_noisy_loss: 0.0134 - lr: 0.0010\n",
            "Epoch 211/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0281 - output_original_loss: 0.0098 - output_noisy_loss: 0.0126\n",
            "Epoch 211: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0264 - output_original_loss: 0.0077 - output_noisy_loss: 0.0128 - val_loss: 0.0347 - val_output_original_loss: 0.0155 - val_output_noisy_loss: 0.0134 - lr: 0.0010\n",
            "Epoch 212/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0293 - output_original_loss: 0.0105 - output_noisy_loss: 0.0130\n",
            "Epoch 212: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0247 - output_original_loss: 0.0062 - output_noisy_loss: 0.0127 - val_loss: 0.0307 - val_output_original_loss: 0.0115 - val_output_noisy_loss: 0.0134 - lr: 0.0010\n",
            "Epoch 213/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0285 - output_original_loss: 0.0097 - output_noisy_loss: 0.0130\n",
            "Epoch 213: val_loss did not improve from 0.02776\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0271 - output_original_loss: 0.0082 - output_noisy_loss: 0.0131 - val_loss: 0.0294 - val_output_original_loss: 0.0103 - val_output_noisy_loss: 0.0134 - lr: 0.0010\n",
            "Epoch 214/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0251 - output_original_loss: 0.0068 - output_noisy_loss: 0.0126\n",
            "Epoch 214: val_loss improved from 0.02776 to 0.02671, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0265 - output_original_loss: 0.0080 - output_noisy_loss: 0.0127 - val_loss: 0.0267 - val_output_original_loss: 0.0076 - val_output_noisy_loss: 0.0133 - lr: 0.0010\n",
            "Epoch 215/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0218 - output_original_loss: 0.0031 - output_noisy_loss: 0.0129\n",
            "Epoch 215: val_loss improved from 0.02671 to 0.02563, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0239 - output_original_loss: 0.0052 - output_noisy_loss: 0.0129 - val_loss: 0.0256 - val_output_original_loss: 0.0066 - val_output_noisy_loss: 0.0133 - lr: 0.0010\n",
            "Epoch 216/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0206 - output_original_loss: 0.0021 - output_noisy_loss: 0.0128\n",
            "Epoch 216: val_loss did not improve from 0.02563\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0219 - output_original_loss: 0.0034 - output_noisy_loss: 0.0127 - val_loss: 0.0269 - val_output_original_loss: 0.0079 - val_output_noisy_loss: 0.0132 - lr: 0.0010\n",
            "Epoch 217/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0222 - output_original_loss: 0.0037 - output_noisy_loss: 0.0127\n",
            "Epoch 217: val_loss did not improve from 0.02563\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0215 - output_original_loss: 0.0031 - output_noisy_loss: 0.0126 - val_loss: 0.0308 - val_output_original_loss: 0.0118 - val_output_noisy_loss: 0.0131 - lr: 0.0010\n",
            "Epoch 218/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0271 - output_original_loss: 0.0084 - output_noisy_loss: 0.0129\n",
            "Epoch 218: val_loss improved from 0.02563 to 0.02515, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0255 - output_original_loss: 0.0071 - output_noisy_loss: 0.0126 - val_loss: 0.0251 - val_output_original_loss: 0.0063 - val_output_noisy_loss: 0.0131 - lr: 0.0010\n",
            "Epoch 219/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0212 - output_original_loss: 0.0025 - output_noisy_loss: 0.0129\n",
            "Epoch 219: val_loss did not improve from 0.02515\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0213 - output_original_loss: 0.0028 - output_noisy_loss: 0.0127 - val_loss: 0.0253 - val_output_original_loss: 0.0065 - val_output_noisy_loss: 0.0130 - lr: 0.0010\n",
            "Epoch 220/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0215 - output_original_loss: 0.0031 - output_noisy_loss: 0.0126\n",
            "Epoch 220: val_loss did not improve from 0.02515\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0212 - output_original_loss: 0.0029 - output_noisy_loss: 0.0125 - val_loss: 0.0255 - val_output_original_loss: 0.0067 - val_output_noisy_loss: 0.0130 - lr: 0.0010\n",
            "Epoch 221/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0208 - output_original_loss: 0.0024 - output_noisy_loss: 0.0126\n",
            "Epoch 221: val_loss did not improve from 0.02515\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0211 - output_original_loss: 0.0028 - output_noisy_loss: 0.0125 - val_loss: 0.0253 - val_output_original_loss: 0.0066 - val_output_noisy_loss: 0.0129 - lr: 0.0010\n",
            "Epoch 222/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0203 - output_original_loss: 0.0025 - output_noisy_loss: 0.0120\n",
            "Epoch 222: val_loss improved from 0.02515 to 0.02500, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0220 - output_original_loss: 0.0039 - output_noisy_loss: 0.0123 - val_loss: 0.0250 - val_output_original_loss: 0.0064 - val_output_noisy_loss: 0.0128 - lr: 0.0010\n",
            "Epoch 223/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0201 - output_original_loss: 0.0019 - output_noisy_loss: 0.0124\n",
            "Epoch 223: val_loss did not improve from 0.02500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0250 - output_original_loss: 0.0067 - output_noisy_loss: 0.0124 - val_loss: 0.0311 - val_output_original_loss: 0.0125 - val_output_noisy_loss: 0.0128 - lr: 0.0010\n",
            "Epoch 224/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0258 - output_original_loss: 0.0077 - output_noisy_loss: 0.0123\n",
            "Epoch 224: val_loss did not improve from 0.02500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0245 - output_original_loss: 0.0065 - output_noisy_loss: 0.0122 - val_loss: 0.0381 - val_output_original_loss: 0.0196 - val_output_noisy_loss: 0.0127 - lr: 0.0010\n",
            "Epoch 225/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0322 - output_original_loss: 0.0142 - output_noisy_loss: 0.0122\n",
            "Epoch 225: val_loss did not improve from 0.02500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0282 - output_original_loss: 0.0103 - output_noisy_loss: 0.0121 - val_loss: 0.0257 - val_output_original_loss: 0.0072 - val_output_noisy_loss: 0.0127 - lr: 0.0010\n",
            "Epoch 226/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0205 - output_original_loss: 0.0025 - output_noisy_loss: 0.0122\n",
            "Epoch 226: val_loss did not improve from 0.02500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0224 - output_original_loss: 0.0044 - output_noisy_loss: 0.0122 - val_loss: 0.0251 - val_output_original_loss: 0.0067 - val_output_noisy_loss: 0.0126 - lr: 0.0010\n",
            "Epoch 227/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0203 - output_original_loss: 0.0025 - output_noisy_loss: 0.0120\n",
            "Epoch 227: val_loss improved from 0.02500 to 0.02452, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0208 - output_original_loss: 0.0029 - output_noisy_loss: 0.0122 - val_loss: 0.0245 - val_output_original_loss: 0.0062 - val_output_noisy_loss: 0.0126 - lr: 0.0010\n",
            "Epoch 228/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0196 - output_original_loss: 0.0019 - output_noisy_loss: 0.0118\n",
            "Epoch 228: val_loss did not improve from 0.02452\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0227 - output_original_loss: 0.0049 - output_noisy_loss: 0.0120 - val_loss: 0.0280 - val_output_original_loss: 0.0097 - val_output_noisy_loss: 0.0125 - lr: 0.0010\n",
            "Epoch 229/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0213 - output_original_loss: 0.0036 - output_noisy_loss: 0.0119\n",
            "Epoch 229: val_loss did not improve from 0.02452\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0230 - output_original_loss: 0.0051 - output_noisy_loss: 0.0121 - val_loss: 0.0293 - val_output_original_loss: 0.0111 - val_output_noisy_loss: 0.0124 - lr: 0.0010\n",
            "Epoch 230/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0235 - output_original_loss: 0.0059 - output_noisy_loss: 0.0118\n",
            "Epoch 230: val_loss did not improve from 0.02452\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0221 - output_original_loss: 0.0044 - output_noisy_loss: 0.0119 - val_loss: 0.0324 - val_output_original_loss: 0.0142 - val_output_noisy_loss: 0.0124 - lr: 0.0010\n",
            "Epoch 231/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0288 - output_original_loss: 0.0113 - output_noisy_loss: 0.0118\n",
            "Epoch 231: val_loss did not improve from 0.02452\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0295 - output_original_loss: 0.0118 - output_noisy_loss: 0.0120 - val_loss: 0.0252 - val_output_original_loss: 0.0071 - val_output_noisy_loss: 0.0124 - lr: 0.0010\n",
            "Epoch 232/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0199 - output_original_loss: 0.0026 - output_noisy_loss: 0.0115\n",
            "Epoch 232: val_loss did not improve from 0.02452\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0232 - output_original_loss: 0.0055 - output_noisy_loss: 0.0119 - val_loss: 0.0266 - val_output_original_loss: 0.0085 - val_output_noisy_loss: 0.0123 - lr: 0.0010\n",
            "Epoch 233/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0222 - output_original_loss: 0.0048 - output_noisy_loss: 0.0117\n",
            "Epoch 233: val_loss did not improve from 0.02452\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0211 - output_original_loss: 0.0035 - output_noisy_loss: 0.0119 - val_loss: 0.0251 - val_output_original_loss: 0.0070 - val_output_noisy_loss: 0.0123 - lr: 0.0010\n",
            "Epoch 234/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0209 - output_original_loss: 0.0033 - output_noisy_loss: 0.0118\n",
            "Epoch 234: val_loss improved from 0.02452 to 0.02395, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0198 - output_original_loss: 0.0022 - output_noisy_loss: 0.0118 - val_loss: 0.0240 - val_output_original_loss: 0.0059 - val_output_noisy_loss: 0.0122 - lr: 0.0010\n",
            "Epoch 235/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0201 - output_original_loss: 0.0024 - output_noisy_loss: 0.0120\n",
            "Epoch 235: val_loss improved from 0.02395 to 0.02354, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0200 - output_original_loss: 0.0024 - output_noisy_loss: 0.0118 - val_loss: 0.0235 - val_output_original_loss: 0.0056 - val_output_noisy_loss: 0.0122 - lr: 0.0010\n",
            "Epoch 236/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0192 - output_original_loss: 0.0015 - output_noisy_loss: 0.0119\n",
            "Epoch 236: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0194 - output_original_loss: 0.0020 - output_noisy_loss: 0.0116 - val_loss: 0.0240 - val_output_original_loss: 0.0061 - val_output_noisy_loss: 0.0121 - lr: 0.0010\n",
            "Epoch 237/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0202 - output_original_loss: 0.0028 - output_noisy_loss: 0.0116\n",
            "Epoch 237: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0202 - output_original_loss: 0.0028 - output_noisy_loss: 0.0116 - val_loss: 0.0257 - val_output_original_loss: 0.0079 - val_output_noisy_loss: 0.0121 - lr: 0.0010\n",
            "Epoch 238/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0208 - output_original_loss: 0.0033 - output_noisy_loss: 0.0117\n",
            "Epoch 238: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0208 - output_original_loss: 0.0033 - output_noisy_loss: 0.0117 - val_loss: 0.0241 - val_output_original_loss: 0.0063 - val_output_noisy_loss: 0.0120 - lr: 0.0010\n",
            "Epoch 239/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0206 - output_original_loss: 0.0034 - output_noisy_loss: 0.0114\n",
            "Epoch 239: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0206 - output_original_loss: 0.0034 - output_noisy_loss: 0.0114 - val_loss: 0.0275 - val_output_original_loss: 0.0098 - val_output_noisy_loss: 0.0119 - lr: 0.0010\n",
            "Epoch 240/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0218 - output_original_loss: 0.0045 - output_noisy_loss: 0.0115\n",
            "Epoch 240: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0218 - output_original_loss: 0.0045 - output_noisy_loss: 0.0115 - val_loss: 0.0251 - val_output_original_loss: 0.0075 - val_output_noisy_loss: 0.0119 - lr: 0.0010\n",
            "Epoch 241/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0214 - output_original_loss: 0.0040 - output_noisy_loss: 0.0117\n",
            "Epoch 241: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0214 - output_original_loss: 0.0042 - output_noisy_loss: 0.0115 - val_loss: 0.0244 - val_output_original_loss: 0.0067 - val_output_noisy_loss: 0.0118 - lr: 0.0010\n",
            "Epoch 242/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0212 - output_original_loss: 0.0041 - output_noisy_loss: 0.0113\n",
            "Epoch 242: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0212 - output_original_loss: 0.0041 - output_noisy_loss: 0.0113 - val_loss: 0.0408 - val_output_original_loss: 0.0232 - val_output_noisy_loss: 0.0118 - lr: 0.0010\n",
            "Epoch 243/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0407 - output_original_loss: 0.0235 - output_noisy_loss: 0.0114\n",
            "Epoch 243: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0407 - output_original_loss: 0.0235 - output_noisy_loss: 0.0114 - val_loss: 0.0322 - val_output_original_loss: 0.0147 - val_output_noisy_loss: 0.0117 - lr: 0.0010\n",
            "Epoch 244/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0292 - output_original_loss: 0.0120 - output_noisy_loss: 0.0114\n",
            "Epoch 244: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0237 - output_original_loss: 0.0068 - output_noisy_loss: 0.0111 - val_loss: 0.0313 - val_output_original_loss: 0.0137 - val_output_noisy_loss: 0.0117 - lr: 0.0010\n",
            "Epoch 245/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0273 - output_original_loss: 0.0100 - output_noisy_loss: 0.0116\n",
            "Epoch 245: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0278 - output_original_loss: 0.0107 - output_noisy_loss: 0.0114 - val_loss: 0.0248 - val_output_original_loss: 0.0073 - val_output_noisy_loss: 0.0117 - lr: 0.0010\n",
            "Epoch 246/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0232 - output_original_loss: 0.0061 - output_noisy_loss: 0.0113\n",
            "Epoch 246: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0280 - output_original_loss: 0.0109 - output_noisy_loss: 0.0113 - val_loss: 0.0457 - val_output_original_loss: 0.0283 - val_output_noisy_loss: 0.0117 - lr: 0.0010\n",
            "Epoch 247/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0369 - output_original_loss: 0.0196 - output_noisy_loss: 0.0115\n",
            "Epoch 247: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0287 - output_original_loss: 0.0116 - output_noisy_loss: 0.0113 - val_loss: 0.0279 - val_output_original_loss: 0.0105 - val_output_noisy_loss: 0.0116 - lr: 0.0010\n",
            "Epoch 248/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0241 - output_original_loss: 0.0072 - output_noisy_loss: 0.0112\n",
            "Epoch 248: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0262 - output_original_loss: 0.0093 - output_noisy_loss: 0.0111 - val_loss: 0.0273 - val_output_original_loss: 0.0099 - val_output_noisy_loss: 0.0116 - lr: 0.0010\n",
            "Epoch 249/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0220 - output_original_loss: 0.0049 - output_noisy_loss: 0.0113\n",
            "Epoch 249: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0210 - output_original_loss: 0.0038 - output_noisy_loss: 0.0113 - val_loss: 0.0236 - val_output_original_loss: 0.0062 - val_output_noisy_loss: 0.0116 - lr: 0.0010\n",
            "Epoch 250/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0181 - output_original_loss: 0.0014 - output_noisy_loss: 0.0109\n",
            "Epoch 250: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0202 - output_original_loss: 0.0034 - output_noisy_loss: 0.0110 - val_loss: 0.0312 - val_output_original_loss: 0.0139 - val_output_noisy_loss: 0.0115 - lr: 0.0010\n",
            "Epoch 251/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0255 - output_original_loss: 0.0085 - output_noisy_loss: 0.0112\n",
            "Epoch 251: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0255 - output_original_loss: 0.0085 - output_noisy_loss: 0.0112 - val_loss: 0.0244 - val_output_original_loss: 0.0071 - val_output_noisy_loss: 0.0115 - lr: 0.0010\n",
            "Epoch 252/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0204 - output_original_loss: 0.0034 - output_noisy_loss: 0.0112\n",
            "Epoch 252: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0217 - output_original_loss: 0.0047 - output_noisy_loss: 0.0112 - val_loss: 0.0336 - val_output_original_loss: 0.0163 - val_output_noisy_loss: 0.0115 - lr: 0.0010\n",
            "Epoch 253/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0302 - output_original_loss: 0.0138 - output_noisy_loss: 0.0106\n",
            "Epoch 253: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0267 - output_original_loss: 0.0099 - output_noisy_loss: 0.0110 - val_loss: 0.0270 - val_output_original_loss: 0.0098 - val_output_noisy_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 254/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0204 - output_original_loss: 0.0039 - output_noisy_loss: 0.0106\n",
            "Epoch 254: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0256 - output_original_loss: 0.0087 - output_noisy_loss: 0.0111 - val_loss: 0.0262 - val_output_original_loss: 0.0090 - val_output_noisy_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 255/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0210 - output_original_loss: 0.0045 - output_noisy_loss: 0.0107\n",
            "Epoch 255: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0224 - output_original_loss: 0.0056 - output_noisy_loss: 0.0110 - val_loss: 0.0402 - val_output_original_loss: 0.0230 - val_output_noisy_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 256/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0342 - output_original_loss: 0.0178 - output_noisy_loss: 0.0106\n",
            "Epoch 256: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0304 - output_original_loss: 0.0137 - output_noisy_loss: 0.0109 - val_loss: 0.0263 - val_output_original_loss: 0.0092 - val_output_noisy_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 257/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0201 - output_original_loss: 0.0035 - output_noisy_loss: 0.0108\n",
            "Epoch 257: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0281 - output_original_loss: 0.0114 - output_noisy_loss: 0.0109 - val_loss: 0.0414 - val_output_original_loss: 0.0242 - val_output_noisy_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 258/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0374 - output_original_loss: 0.0204 - output_noisy_loss: 0.0113\n",
            "Epoch 258: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0290 - output_original_loss: 0.0122 - output_noisy_loss: 0.0110 - val_loss: 0.0416 - val_output_original_loss: 0.0245 - val_output_noisy_loss: 0.0113 - lr: 0.0010\n",
            "Epoch 259/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0372 - output_original_loss: 0.0205 - output_noisy_loss: 0.0109\n",
            "Epoch 259: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0455 - output_original_loss: 0.0289 - output_noisy_loss: 0.0108 - val_loss: 0.0259 - val_output_original_loss: 0.0088 - val_output_noisy_loss: 0.0113 - lr: 0.0010\n",
            "Epoch 260/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0214 - output_original_loss: 0.0046 - output_noisy_loss: 0.0110\n",
            "Epoch 260: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0409 - output_original_loss: 0.0242 - output_noisy_loss: 0.0110 - val_loss: 0.0482 - val_output_original_loss: 0.0311 - val_output_noisy_loss: 0.0113 - lr: 0.0010\n",
            "Epoch 261/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0408 - output_original_loss: 0.0241 - output_noisy_loss: 0.0109\n",
            "Epoch 261: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0410 - output_original_loss: 0.0244 - output_noisy_loss: 0.0108 - val_loss: 0.0579 - val_output_original_loss: 0.0408 - val_output_noisy_loss: 0.0113 - lr: 0.0010\n",
            "Epoch 262/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0464 - output_original_loss: 0.0300 - output_noisy_loss: 0.0106\n",
            "Epoch 262: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0373 - output_original_loss: 0.0207 - output_noisy_loss: 0.0109 - val_loss: 0.0422 - val_output_original_loss: 0.0251 - val_output_noisy_loss: 0.0113 - lr: 0.0010\n",
            "Epoch 263/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0392 - output_original_loss: 0.0225 - output_noisy_loss: 0.0109\n",
            "Epoch 263: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0323 - output_original_loss: 0.0156 - output_noisy_loss: 0.0109 - val_loss: 0.0344 - val_output_original_loss: 0.0172 - val_output_noisy_loss: 0.0113 - lr: 0.0010\n",
            "Epoch 264/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0288 - output_original_loss: 0.0120 - output_noisy_loss: 0.0110\n",
            "Epoch 264: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0347 - output_original_loss: 0.0180 - output_noisy_loss: 0.0109 - val_loss: 0.0305 - val_output_original_loss: 0.0134 - val_output_noisy_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 265/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0264 - output_original_loss: 0.0093 - output_noisy_loss: 0.0113\n",
            "Epoch 265: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0300 - output_original_loss: 0.0133 - output_noisy_loss: 0.0109 - val_loss: 0.0520 - val_output_original_loss: 0.0348 - val_output_noisy_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 266/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0450 - output_original_loss: 0.0282 - output_noisy_loss: 0.0110\n",
            "Epoch 266: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0317 - output_original_loss: 0.0149 - output_noisy_loss: 0.0110 - val_loss: 0.0384 - val_output_original_loss: 0.0212 - val_output_noisy_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 267/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0380 - output_original_loss: 0.0212 - output_noisy_loss: 0.0110\n",
            "Epoch 267: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0296 - output_original_loss: 0.0128 - output_noisy_loss: 0.0110 - val_loss: 0.0391 - val_output_original_loss: 0.0219 - val_output_noisy_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 268/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0332 - output_original_loss: 0.0167 - output_noisy_loss: 0.0108\n",
            "Epoch 268: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0284 - output_original_loss: 0.0117 - output_noisy_loss: 0.0109 - val_loss: 0.0283 - val_output_original_loss: 0.0112 - val_output_noisy_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 269/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0257 - output_original_loss: 0.0089 - output_noisy_loss: 0.0111\n",
            "Epoch 269: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0235 - output_original_loss: 0.0067 - output_noisy_loss: 0.0110 - val_loss: 0.0254 - val_output_original_loss: 0.0083 - val_output_noisy_loss: 0.0113 - lr: 0.0010\n",
            "Epoch 270/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0201 - output_original_loss: 0.0034 - output_noisy_loss: 0.0110\n",
            "Epoch 270: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0212 - output_original_loss: 0.0046 - output_noisy_loss: 0.0108 - val_loss: 0.0238 - val_output_original_loss: 0.0067 - val_output_noisy_loss: 0.0113 - lr: 0.0010\n",
            "Epoch 271/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0194 - output_original_loss: 0.0030 - output_noisy_loss: 0.0107\n",
            "Epoch 271: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0212 - output_original_loss: 0.0047 - output_noisy_loss: 0.0108 - val_loss: 0.0262 - val_output_original_loss: 0.0092 - val_output_noisy_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 272/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0221 - output_original_loss: 0.0055 - output_noisy_loss: 0.0108\n",
            "Epoch 272: val_loss did not improve from 0.02354\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0222 - output_original_loss: 0.0057 - output_noisy_loss: 0.0108 - val_loss: 0.0245 - val_output_original_loss: 0.0076 - val_output_noisy_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 273/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0206 - output_original_loss: 0.0038 - output_noisy_loss: 0.0111\n",
            "Epoch 273: val_loss improved from 0.02354 to 0.02313, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0204 - output_original_loss: 0.0037 - output_noisy_loss: 0.0109 - val_loss: 0.0231 - val_output_original_loss: 0.0062 - val_output_noisy_loss: 0.0111 - lr: 0.0010\n",
            "Epoch 274/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0183 - output_original_loss: 0.0019 - output_noisy_loss: 0.0106\n",
            "Epoch 274: val_loss did not improve from 0.02313\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0195 - output_original_loss: 0.0030 - output_noisy_loss: 0.0107 - val_loss: 0.0240 - val_output_original_loss: 0.0071 - val_output_noisy_loss: 0.0111 - lr: 0.0010\n",
            "Epoch 275/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0196 - output_original_loss: 0.0032 - output_noisy_loss: 0.0106\n",
            "Epoch 275: val_loss did not improve from 0.02313\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0211 - output_original_loss: 0.0046 - output_noisy_loss: 0.0107 - val_loss: 0.0265 - val_output_original_loss: 0.0097 - val_output_noisy_loss: 0.0111 - lr: 0.0010\n",
            "Epoch 276/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0208 - output_original_loss: 0.0045 - output_noisy_loss: 0.0106\n",
            "Epoch 276: val_loss did not improve from 0.02313\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0209 - output_original_loss: 0.0045 - output_noisy_loss: 0.0107 - val_loss: 0.0254 - val_output_original_loss: 0.0086 - val_output_noisy_loss: 0.0110 - lr: 0.0010\n",
            "Epoch 277/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0206 - output_original_loss: 0.0044 - output_noisy_loss: 0.0105\n",
            "Epoch 277: val_loss did not improve from 0.02313\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0207 - output_original_loss: 0.0045 - output_noisy_loss: 0.0105 - val_loss: 0.0269 - val_output_original_loss: 0.0101 - val_output_noisy_loss: 0.0110 - lr: 0.0010\n",
            "Epoch 278/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0228 - output_original_loss: 0.0062 - output_noisy_loss: 0.0108\n",
            "Epoch 278: val_loss did not improve from 0.02313\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0209 - output_original_loss: 0.0045 - output_noisy_loss: 0.0106 - val_loss: 0.0286 - val_output_original_loss: 0.0119 - val_output_noisy_loss: 0.0109 - lr: 0.0010\n",
            "Epoch 279/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0247 - output_original_loss: 0.0081 - output_noisy_loss: 0.0108\n",
            "Epoch 279: val_loss did not improve from 0.02313\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0214 - output_original_loss: 0.0050 - output_noisy_loss: 0.0106 - val_loss: 0.0258 - val_output_original_loss: 0.0092 - val_output_noisy_loss: 0.0109 - lr: 0.0010\n",
            "Epoch 280/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0219 - output_original_loss: 0.0056 - output_noisy_loss: 0.0106\n",
            "Epoch 280: val_loss improved from 0.02313 to 0.02208, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0217 - output_original_loss: 0.0055 - output_noisy_loss: 0.0104 - val_loss: 0.0221 - val_output_original_loss: 0.0055 - val_output_noisy_loss: 0.0109 - lr: 0.0010\n",
            "Epoch 281/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0184 - output_original_loss: 0.0019 - output_noisy_loss: 0.0108\n",
            "Epoch 281: val_loss did not improve from 0.02208\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0189 - output_original_loss: 0.0027 - output_noisy_loss: 0.0105 - val_loss: 0.0228 - val_output_original_loss: 0.0063 - val_output_noisy_loss: 0.0108 - lr: 0.0010\n",
            "Epoch 282/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0182 - output_original_loss: 0.0018 - output_noisy_loss: 0.0106\n",
            "Epoch 282: val_loss improved from 0.02208 to 0.02201, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0182 - output_original_loss: 0.0018 - output_noisy_loss: 0.0106 - val_loss: 0.0220 - val_output_original_loss: 0.0055 - val_output_noisy_loss: 0.0108 - lr: 0.0010\n",
            "Epoch 283/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0177 - output_original_loss: 0.0018 - output_noisy_loss: 0.0101\n",
            "Epoch 283: val_loss improved from 0.02201 to 0.02187, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0180 - output_original_loss: 0.0020 - output_noisy_loss: 0.0102 - val_loss: 0.0219 - val_output_original_loss: 0.0054 - val_output_noisy_loss: 0.0107 - lr: 0.0010\n",
            "Epoch 284/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0168 - output_original_loss: 0.0012 - output_noisy_loss: 0.0098\n",
            "Epoch 284: val_loss did not improve from 0.02187\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0179 - output_original_loss: 0.0019 - output_noisy_loss: 0.0103 - val_loss: 0.0221 - val_output_original_loss: 0.0056 - val_output_noisy_loss: 0.0107 - lr: 0.0010\n",
            "Epoch 285/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0167 - output_original_loss: 9.6406e-04 - output_noisy_loss: 0.0100\n",
            "Epoch 285: val_loss did not improve from 0.02187\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0178 - output_original_loss: 0.0019 - output_noisy_loss: 0.0102 - val_loss: 0.0231 - val_output_original_loss: 0.0067 - val_output_noisy_loss: 0.0106 - lr: 0.0010\n",
            "Epoch 286/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0193 - output_original_loss: 0.0030 - output_noisy_loss: 0.0105\n",
            "Epoch 286: val_loss did not improve from 0.02187\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0180 - output_original_loss: 0.0019 - output_noisy_loss: 0.0104 - val_loss: 0.0226 - val_output_original_loss: 0.0063 - val_output_noisy_loss: 0.0106 - lr: 0.0010\n",
            "Epoch 287/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0187 - output_original_loss: 0.0026 - output_noisy_loss: 0.0103\n",
            "Epoch 287: val_loss improved from 0.02187 to 0.02102, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0182 - output_original_loss: 0.0022 - output_noisy_loss: 0.0102 - val_loss: 0.0210 - val_output_original_loss: 0.0047 - val_output_noisy_loss: 0.0106 - lr: 0.0010\n",
            "Epoch 288/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0173 - output_original_loss: 0.0012 - output_noisy_loss: 0.0104\n",
            "Epoch 288: val_loss did not improve from 0.02102\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0177 - output_original_loss: 0.0018 - output_noisy_loss: 0.0101 - val_loss: 0.0215 - val_output_original_loss: 0.0052 - val_output_noisy_loss: 0.0105 - lr: 0.0010\n",
            "Epoch 289/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0175 - output_original_loss: 0.0013 - output_noisy_loss: 0.0104\n",
            "Epoch 289: val_loss did not improve from 0.02102\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0171 - output_original_loss: 0.0013 - output_noisy_loss: 0.0100 - val_loss: 0.0212 - val_output_original_loss: 0.0049 - val_output_noisy_loss: 0.0105 - lr: 0.0010\n",
            "Epoch 290/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0174 - output_original_loss: 0.0014 - output_noisy_loss: 0.0103\n",
            "Epoch 290: val_loss did not improve from 0.02102\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0172 - output_original_loss: 0.0014 - output_noisy_loss: 0.0100 - val_loss: 0.0230 - val_output_original_loss: 0.0068 - val_output_noisy_loss: 0.0104 - lr: 0.0010\n",
            "Epoch 291/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0187 - output_original_loss: 0.0027 - output_noisy_loss: 0.0103\n",
            "Epoch 291: val_loss did not improve from 0.02102\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0178 - output_original_loss: 0.0020 - output_noisy_loss: 0.0100 - val_loss: 0.0212 - val_output_original_loss: 0.0051 - val_output_noisy_loss: 0.0104 - lr: 0.0010\n",
            "Epoch 292/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0171 - output_original_loss: 0.0014 - output_noisy_loss: 0.0100\n",
            "Epoch 292: val_loss did not improve from 0.02102\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0191 - output_original_loss: 0.0033 - output_noisy_loss: 0.0101 - val_loss: 0.0237 - val_output_original_loss: 0.0076 - val_output_noisy_loss: 0.0103 - lr: 0.0010\n",
            "Epoch 293/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0206 - output_original_loss: 0.0046 - output_noisy_loss: 0.0101\n",
            "Epoch 293: val_loss did not improve from 0.02102\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0185 - output_original_loss: 0.0027 - output_noisy_loss: 0.0100 - val_loss: 0.0211 - val_output_original_loss: 0.0051 - val_output_noisy_loss: 0.0103 - lr: 0.0010\n",
            "Epoch 294/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0181 - output_original_loss: 0.0024 - output_noisy_loss: 0.0099\n",
            "Epoch 294: val_loss did not improve from 0.02102\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0181 - output_original_loss: 0.0024 - output_noisy_loss: 0.0099 - val_loss: 0.0237 - val_output_original_loss: 0.0077 - val_output_noisy_loss: 0.0103 - lr: 0.0010\n",
            "Epoch 295/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0193 - output_original_loss: 0.0038 - output_noisy_loss: 0.0098\n",
            "Epoch 295: val_loss did not improve from 0.02102\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0180 - output_original_loss: 0.0024 - output_noisy_loss: 0.0099 - val_loss: 0.0217 - val_output_original_loss: 0.0057 - val_output_noisy_loss: 0.0102 - lr: 0.0010\n",
            "Epoch 296/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0175 - output_original_loss: 0.0021 - output_noisy_loss: 0.0096\n",
            "Epoch 296: val_loss did not improve from 0.02102\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0189 - output_original_loss: 0.0032 - output_noisy_loss: 0.0099 - val_loss: 0.0211 - val_output_original_loss: 0.0052 - val_output_noisy_loss: 0.0102 - lr: 0.0010\n",
            "Epoch 297/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0171 - output_original_loss: 0.0018 - output_noisy_loss: 0.0096\n",
            "Epoch 297: val_loss did not improve from 0.02102\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0171 - output_original_loss: 0.0016 - output_noisy_loss: 0.0097 - val_loss: 0.0239 - val_output_original_loss: 0.0080 - val_output_noisy_loss: 0.0101 - lr: 0.0010\n",
            "Epoch 298/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0199 - output_original_loss: 0.0043 - output_noisy_loss: 0.0098\n",
            "Epoch 298: val_loss improved from 0.02102 to 0.02076, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0198 - output_original_loss: 0.0043 - output_noisy_loss: 0.0098 - val_loss: 0.0208 - val_output_original_loss: 0.0049 - val_output_noisy_loss: 0.0101 - lr: 0.0010\n",
            "Epoch 299/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0169 - output_original_loss: 0.0012 - output_noisy_loss: 0.0099\n",
            "Epoch 299: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0172 - output_original_loss: 0.0017 - output_noisy_loss: 0.0098 - val_loss: 0.0213 - val_output_original_loss: 0.0054 - val_output_noisy_loss: 0.0101 - lr: 0.0010\n",
            "Epoch 300/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0170 - output_original_loss: 0.0017 - output_noisy_loss: 0.0095\n",
            "Epoch 300: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0171 - output_original_loss: 0.0017 - output_noisy_loss: 0.0097 - val_loss: 0.0228 - val_output_original_loss: 0.0071 - val_output_noisy_loss: 0.0100 - lr: 0.0010\n",
            "Epoch 301/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0187 - output_original_loss: 0.0033 - output_noisy_loss: 0.0096\n",
            "Epoch 301: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0184 - output_original_loss: 0.0030 - output_noisy_loss: 0.0096 - val_loss: 0.0210 - val_output_original_loss: 0.0053 - val_output_noisy_loss: 0.0100 - lr: 0.0010\n",
            "Epoch 302/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0167 - output_original_loss: 0.0014 - output_noisy_loss: 0.0096\n",
            "Epoch 302: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0167 - output_original_loss: 0.0014 - output_noisy_loss: 0.0095 - val_loss: 0.0227 - val_output_original_loss: 0.0070 - val_output_noisy_loss: 0.0100 - lr: 0.0010\n",
            "Epoch 303/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0191 - output_original_loss: 0.0034 - output_noisy_loss: 0.0099\n",
            "Epoch 303: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0195 - output_original_loss: 0.0041 - output_noisy_loss: 0.0096 - val_loss: 0.0209 - val_output_original_loss: 0.0052 - val_output_noisy_loss: 0.0099 - lr: 0.0010\n",
            "Epoch 304/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0165 - output_original_loss: 0.0013 - output_noisy_loss: 0.0094\n",
            "Epoch 304: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0170 - output_original_loss: 0.0018 - output_noisy_loss: 0.0095 - val_loss: 0.0254 - val_output_original_loss: 0.0097 - val_output_noisy_loss: 0.0099 - lr: 0.0010\n",
            "Epoch 305/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0213 - output_original_loss: 0.0058 - output_noisy_loss: 0.0097\n",
            "Epoch 305: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0218 - output_original_loss: 0.0064 - output_noisy_loss: 0.0097 - val_loss: 0.0223 - val_output_original_loss: 0.0067 - val_output_noisy_loss: 0.0099 - lr: 0.0010\n",
            "Epoch 306/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0178 - output_original_loss: 0.0027 - output_noisy_loss: 0.0093\n",
            "Epoch 306: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0182 - output_original_loss: 0.0029 - output_noisy_loss: 0.0095 - val_loss: 0.0277 - val_output_original_loss: 0.0121 - val_output_noisy_loss: 0.0098 - lr: 0.0010\n",
            "Epoch 307/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0247 - output_original_loss: 0.0096 - output_noisy_loss: 0.0093\n",
            "Epoch 307: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0259 - output_original_loss: 0.0108 - output_noisy_loss: 0.0094 - val_loss: 0.0248 - val_output_original_loss: 0.0093 - val_output_noisy_loss: 0.0098 - lr: 0.0010\n",
            "Epoch 308/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0215 - output_original_loss: 0.0062 - output_noisy_loss: 0.0095\n",
            "Epoch 308: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0186 - output_original_loss: 0.0034 - output_noisy_loss: 0.0094 - val_loss: 0.0222 - val_output_original_loss: 0.0067 - val_output_noisy_loss: 0.0098 - lr: 0.0010\n",
            "Epoch 309/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0182 - output_original_loss: 0.0030 - output_noisy_loss: 0.0095\n",
            "Epoch 309: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0190 - output_original_loss: 0.0038 - output_noisy_loss: 0.0094 - val_loss: 0.0220 - val_output_original_loss: 0.0065 - val_output_noisy_loss: 0.0098 - lr: 0.0010\n",
            "Epoch 310/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0184 - output_original_loss: 0.0031 - output_noisy_loss: 0.0095\n",
            "Epoch 310: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0181 - output_original_loss: 0.0030 - output_noisy_loss: 0.0093 - val_loss: 0.0211 - val_output_original_loss: 0.0056 - val_output_noisy_loss: 0.0097 - lr: 0.0010\n",
            "Epoch 311/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0175 - output_original_loss: 0.0022 - output_noisy_loss: 0.0095\n",
            "Epoch 311: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0177 - output_original_loss: 0.0025 - output_noisy_loss: 0.0094 - val_loss: 0.0266 - val_output_original_loss: 0.0111 - val_output_noisy_loss: 0.0097 - lr: 0.0010\n",
            "Epoch 312/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0224 - output_original_loss: 0.0073 - output_noisy_loss: 0.0094\n",
            "Epoch 312: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0204 - output_original_loss: 0.0052 - output_noisy_loss: 0.0094 - val_loss: 0.0208 - val_output_original_loss: 0.0053 - val_output_noisy_loss: 0.0097 - lr: 0.0010\n",
            "Epoch 313/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0169 - output_original_loss: 0.0022 - output_noisy_loss: 0.0090\n",
            "Epoch 313: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0169 - output_original_loss: 0.0019 - output_noisy_loss: 0.0092 - val_loss: 0.0215 - val_output_original_loss: 0.0060 - val_output_noisy_loss: 0.0097 - lr: 0.0010\n",
            "Epoch 314/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0184 - output_original_loss: 0.0034 - output_noisy_loss: 0.0092\n",
            "Epoch 314: val_loss did not improve from 0.02076\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0216 - output_original_loss: 0.0065 - output_noisy_loss: 0.0094 - val_loss: 0.0260 - val_output_original_loss: 0.0106 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 315/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0219 - output_original_loss: 0.0069 - output_noisy_loss: 0.0092\n",
            "Epoch 315: val_loss improved from 0.02076 to 0.02054, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0195 - output_original_loss: 0.0046 - output_noisy_loss: 0.0092 - val_loss: 0.0205 - val_output_original_loss: 0.0052 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 316/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0169 - output_original_loss: 0.0018 - output_noisy_loss: 0.0094\n",
            "Epoch 316: val_loss improved from 0.02054 to 0.02030, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0166 - output_original_loss: 0.0015 - output_noisy_loss: 0.0093 - val_loss: 0.0203 - val_output_original_loss: 0.0050 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 317/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0169 - output_original_loss: 0.0021 - output_noisy_loss: 0.0091\n",
            "Epoch 317: val_loss did not improve from 0.02030\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0168 - output_original_loss: 0.0019 - output_noisy_loss: 0.0091 - val_loss: 0.0211 - val_output_original_loss: 0.0058 - val_output_noisy_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 318/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0174 - output_original_loss: 0.0022 - output_noisy_loss: 0.0094\n",
            "Epoch 318: val_loss did not improve from 0.02030\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0172 - output_original_loss: 0.0024 - output_noisy_loss: 0.0091 - val_loss: 0.0204 - val_output_original_loss: 0.0051 - val_output_noisy_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 319/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0161 - output_original_loss: 0.0012 - output_noisy_loss: 0.0091\n",
            "Epoch 319: val_loss did not improve from 0.02030\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0168 - output_original_loss: 0.0018 - output_noisy_loss: 0.0093 - val_loss: 0.0235 - val_output_original_loss: 0.0083 - val_output_noisy_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 320/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0195 - output_original_loss: 0.0049 - output_noisy_loss: 0.0089\n",
            "Epoch 320: val_loss improved from 0.02030 to 0.01959, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0189 - output_original_loss: 0.0039 - output_noisy_loss: 0.0092 - val_loss: 0.0196 - val_output_original_loss: 0.0044 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 321/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0162 - output_original_loss: 0.0014 - output_noisy_loss: 0.0091\n",
            "Epoch 321: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0163 - output_original_loss: 0.0015 - output_noisy_loss: 0.0090 - val_loss: 0.0231 - val_output_original_loss: 0.0079 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 322/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0195 - output_original_loss: 0.0048 - output_noisy_loss: 0.0089\n",
            "Epoch 322: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0238 - output_original_loss: 0.0091 - output_noisy_loss: 0.0090 - val_loss: 0.0373 - val_output_original_loss: 0.0221 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 323/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0331 - output_original_loss: 0.0182 - output_noisy_loss: 0.0091\n",
            "Epoch 323: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0274 - output_original_loss: 0.0126 - output_noisy_loss: 0.0090 - val_loss: 0.0206 - val_output_original_loss: 0.0055 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 324/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0169 - output_original_loss: 0.0024 - output_noisy_loss: 0.0088\n",
            "Epoch 324: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0247 - output_original_loss: 0.0098 - output_noisy_loss: 0.0091 - val_loss: 0.0452 - val_output_original_loss: 0.0302 - val_output_noisy_loss: 0.0093 - lr: 0.0010\n",
            "Epoch 325/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0425 - output_original_loss: 0.0280 - output_noisy_loss: 0.0087\n",
            "Epoch 325: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0464 - output_original_loss: 0.0317 - output_noisy_loss: 0.0089 - val_loss: 0.0378 - val_output_original_loss: 0.0227 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 326/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0304 - output_original_loss: 0.0154 - output_noisy_loss: 0.0092\n",
            "Epoch 326: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0243 - output_original_loss: 0.0094 - output_noisy_loss: 0.0091 - val_loss: 0.0460 - val_output_original_loss: 0.0309 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 327/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0438 - output_original_loss: 0.0291 - output_noisy_loss: 0.0090\n",
            "Epoch 327: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0419 - output_original_loss: 0.0271 - output_noisy_loss: 0.0090 - val_loss: 0.0230 - val_output_original_loss: 0.0079 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 328/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0187 - output_original_loss: 0.0039 - output_noisy_loss: 0.0090\n",
            "Epoch 328: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0388 - output_original_loss: 0.0240 - output_noisy_loss: 0.0091 - val_loss: 0.0677 - val_output_original_loss: 0.0525 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 329/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0589 - output_original_loss: 0.0443 - output_noisy_loss: 0.0088\n",
            "Epoch 329: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0355 - output_original_loss: 0.0207 - output_noisy_loss: 0.0091 - val_loss: 0.0449 - val_output_original_loss: 0.0298 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 330/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0391 - output_original_loss: 0.0244 - output_noisy_loss: 0.0089\n",
            "Epoch 330: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0321 - output_original_loss: 0.0172 - output_noisy_loss: 0.0091 - val_loss: 0.0282 - val_output_original_loss: 0.0130 - val_output_noisy_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 331/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0251 - output_original_loss: 0.0100 - output_noisy_loss: 0.0094\n",
            "Epoch 331: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0348 - output_original_loss: 0.0199 - output_noisy_loss: 0.0091 - val_loss: 0.0263 - val_output_original_loss: 0.0111 - val_output_noisy_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 332/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0215 - output_original_loss: 0.0065 - output_noisy_loss: 0.0093\n",
            "Epoch 332: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0276 - output_original_loss: 0.0127 - output_noisy_loss: 0.0091 - val_loss: 0.0354 - val_output_original_loss: 0.0201 - val_output_noisy_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 333/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0300 - output_original_loss: 0.0150 - output_noisy_loss: 0.0092\n",
            "Epoch 333: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0319 - output_original_loss: 0.0169 - output_noisy_loss: 0.0093 - val_loss: 0.0568 - val_output_original_loss: 0.0415 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 334/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0592 - output_original_loss: 0.0444 - output_noisy_loss: 0.0091\n",
            "Epoch 334: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0384 - output_original_loss: 0.0235 - output_noisy_loss: 0.0091 - val_loss: 0.0595 - val_output_original_loss: 0.0442 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 335/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0510 - output_original_loss: 0.0361 - output_noisy_loss: 0.0092\n",
            "Epoch 335: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0357 - output_original_loss: 0.0207 - output_noisy_loss: 0.0093 - val_loss: 0.0514 - val_output_original_loss: 0.0361 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 336/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0408 - output_original_loss: 0.0260 - output_noisy_loss: 0.0092\n",
            "Epoch 336: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0310 - output_original_loss: 0.0161 - output_noisy_loss: 0.0092 - val_loss: 0.0397 - val_output_original_loss: 0.0244 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 337/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0330 - output_original_loss: 0.0179 - output_noisy_loss: 0.0093\n",
            "Epoch 337: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0252 - output_original_loss: 0.0102 - output_noisy_loss: 0.0093 - val_loss: 0.0340 - val_output_original_loss: 0.0187 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 338/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0285 - output_original_loss: 0.0135 - output_noisy_loss: 0.0093\n",
            "Epoch 338: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0227 - output_original_loss: 0.0077 - output_noisy_loss: 0.0093 - val_loss: 0.0310 - val_output_original_loss: 0.0157 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 339/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0263 - output_original_loss: 0.0114 - output_noisy_loss: 0.0092\n",
            "Epoch 339: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0224 - output_original_loss: 0.0074 - output_noisy_loss: 0.0092 - val_loss: 0.0241 - val_output_original_loss: 0.0088 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 340/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0193 - output_original_loss: 0.0044 - output_noisy_loss: 0.0091\n",
            "Epoch 340: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0188 - output_original_loss: 0.0037 - output_noisy_loss: 0.0093 - val_loss: 0.0228 - val_output_original_loss: 0.0075 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 341/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0179 - output_original_loss: 0.0033 - output_noisy_loss: 0.0089\n",
            "Epoch 341: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0180 - output_original_loss: 0.0032 - output_noisy_loss: 0.0091 - val_loss: 0.0218 - val_output_original_loss: 0.0065 - val_output_noisy_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 342/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0181 - output_original_loss: 0.0032 - output_noisy_loss: 0.0092\n",
            "Epoch 342: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0177 - output_original_loss: 0.0029 - output_noisy_loss: 0.0091 - val_loss: 0.0216 - val_output_original_loss: 0.0063 - val_output_noisy_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 343/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0180 - output_original_loss: 0.0028 - output_noisy_loss: 0.0095\n",
            "Epoch 343: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0188 - output_original_loss: 0.0037 - output_noisy_loss: 0.0093 - val_loss: 0.0213 - val_output_original_loss: 0.0061 - val_output_noisy_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 344/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0169 - output_original_loss: 0.0021 - output_noisy_loss: 0.0090\n",
            "Epoch 344: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0192 - output_original_loss: 0.0043 - output_noisy_loss: 0.0092 - val_loss: 0.0204 - val_output_original_loss: 0.0052 - val_output_noisy_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 345/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0165 - output_original_loss: 0.0017 - output_noisy_loss: 0.0091\n",
            "Epoch 345: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0205 - output_original_loss: 0.0057 - output_noisy_loss: 0.0091 - val_loss: 0.0234 - val_output_original_loss: 0.0082 - val_output_noisy_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 346/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0189 - output_original_loss: 0.0041 - output_noisy_loss: 0.0091\n",
            "Epoch 346: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0198 - output_original_loss: 0.0051 - output_noisy_loss: 0.0090 - val_loss: 0.0270 - val_output_original_loss: 0.0119 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 347/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0218 - output_original_loss: 0.0068 - output_noisy_loss: 0.0093\n",
            "Epoch 347: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0202 - output_original_loss: 0.0053 - output_noisy_loss: 0.0092 - val_loss: 0.0263 - val_output_original_loss: 0.0112 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 348/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0213 - output_original_loss: 0.0064 - output_noisy_loss: 0.0092\n",
            "Epoch 348: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0191 - output_original_loss: 0.0041 - output_noisy_loss: 0.0092 - val_loss: 0.0268 - val_output_original_loss: 0.0116 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 349/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0219 - output_original_loss: 0.0072 - output_noisy_loss: 0.0090\n",
            "Epoch 349: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0189 - output_original_loss: 0.0044 - output_noisy_loss: 0.0089 - val_loss: 0.0243 - val_output_original_loss: 0.0092 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 350/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0206 - output_original_loss: 0.0055 - output_noisy_loss: 0.0094\n",
            "Epoch 350: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0194 - output_original_loss: 0.0047 - output_noisy_loss: 0.0090 - val_loss: 0.0201 - val_output_original_loss: 0.0050 - val_output_noisy_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 351/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0165 - output_original_loss: 0.0013 - output_noisy_loss: 0.0094\n",
            "Epoch 351: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0160 - output_original_loss: 0.0011 - output_noisy_loss: 0.0091 - val_loss: 0.0200 - val_output_original_loss: 0.0049 - val_output_noisy_loss: 0.0094 - lr: 1.0000e-04\n",
            "Epoch 352/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0159 - output_original_loss: 0.0013 - output_noisy_loss: 0.0089\n",
            "Epoch 352: val_loss did not improve from 0.01959\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0159 - output_original_loss: 0.0012 - output_noisy_loss: 0.0090 - val_loss: 0.0197 - val_output_original_loss: 0.0046 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 353/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0157 - output_original_loss: 9.7838e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 353: val_loss improved from 0.01959 to 0.01959, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0155 - output_original_loss: 8.9989e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0196 - val_output_original_loss: 0.0045 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 354/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0160 - output_original_loss: 9.2362e-04 - output_noisy_loss: 0.0094\n",
            "Epoch 354: val_loss improved from 0.01959 to 0.01952, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0157 - output_original_loss: 8.9593e-04 - output_noisy_loss: 0.0091 - val_loss: 0.0195 - val_output_original_loss: 0.0045 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 355/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.6447e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 355: val_loss improved from 0.01952 to 0.01949, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0156 - output_original_loss: 8.4164e-04 - output_noisy_loss: 0.0091 - val_loss: 0.0195 - val_output_original_loss: 0.0044 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 356/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - output_original_loss: 7.8330e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 356: val_loss improved from 0.01949 to 0.01940, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0155 - output_original_loss: 9.3151e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0194 - val_output_original_loss: 0.0044 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 357/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - output_original_loss: 8.3077e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 357: val_loss improved from 0.01940 to 0.01940, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0155 - output_original_loss: 7.5333e-04 - output_noisy_loss: 0.0091 - val_loss: 0.0194 - val_output_original_loss: 0.0043 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 358/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 7.4992e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 358: val_loss improved from 0.01940 to 0.01936, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0154 - output_original_loss: 6.9561e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0194 - val_output_original_loss: 0.0043 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 359/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0155 - output_original_loss: 7.5000e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 359: val_loss improved from 0.01936 to 0.01935, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0153 - output_original_loss: 7.3760e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0194 - val_output_original_loss: 0.0043 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 360/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0160 - output_original_loss: 8.8729e-04 - output_noisy_loss: 0.0094\n",
            "Epoch 360: val_loss improved from 0.01935 to 0.01934, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0156 - output_original_loss: 8.1359e-04 - output_noisy_loss: 0.0091 - val_loss: 0.0193 - val_output_original_loss: 0.0043 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 361/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - output_original_loss: 8.1278e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 361: val_loss improved from 0.01934 to 0.01929, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0154 - output_original_loss: 6.8435e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0193 - val_output_original_loss: 0.0043 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 362/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0158 - output_original_loss: 9.5361e-04 - output_noisy_loss: 0.0092\n",
            "Epoch 362: val_loss improved from 0.01929 to 0.01926, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0155 - output_original_loss: 7.3495e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0193 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 363/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.8470e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 363: val_loss improved from 0.01926 to 0.01926, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0154 - output_original_loss: 7.1567e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0193 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 364/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 6.9503e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 364: val_loss did not improve from 0.01926\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0154 - output_original_loss: 7.1167e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0193 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 365/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0155 - output_original_loss: 7.9461e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 365: val_loss improved from 0.01926 to 0.01925, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0154 - output_original_loss: 7.2031e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 366/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0158 - output_original_loss: 7.4654e-04 - output_noisy_loss: 0.0093\n",
            "Epoch 366: val_loss improved from 0.01925 to 0.01924, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0154 - output_original_loss: 7.2215e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 367/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0160 - output_original_loss: 0.0011 - output_noisy_loss: 0.0092\n",
            "Epoch 367: val_loss did not improve from 0.01924\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0154 - output_original_loss: 7.0619e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 368/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 6.0586e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 368: val_loss improved from 0.01924 to 0.01924, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0153 - output_original_loss: 6.8802e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 369/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 5.9185e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 369: val_loss improved from 0.01924 to 0.01923, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0153 - output_original_loss: 7.0535e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 370/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - output_original_loss: 5.9679e-04 - output_noisy_loss: 0.0093\n",
            "Epoch 370: val_loss improved from 0.01923 to 0.01922, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0155 - output_original_loss: 7.1478e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 371/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.6921e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 371: val_loss improved from 0.01922 to 0.01921, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0153 - output_original_loss: 6.5079e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 372/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0155 - output_original_loss: 8.0126e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 372: val_loss did not improve from 0.01921\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0154 - output_original_loss: 7.3501e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 373/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 5.2197e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 373: val_loss improved from 0.01921 to 0.01919, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0152 - output_original_loss: 6.2860e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 374/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - output_original_loss: 7.5182e-04 - output_noisy_loss: 0.0092\n",
            "Epoch 374: val_loss improved from 0.01919 to 0.01919, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0154 - output_original_loss: 6.9001e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 375/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.9363e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 375: val_loss did not improve from 0.01919\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0154 - output_original_loss: 7.2789e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 376/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.5534e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 376: val_loss improved from 0.01919 to 0.01918, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0153 - output_original_loss: 6.5534e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 377/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 6.0967e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 377: val_loss improved from 0.01918 to 0.01917, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0152 - output_original_loss: 6.5630e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 378/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - output_original_loss: 7.7455e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 378: val_loss improved from 0.01917 to 0.01916, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0153 - output_original_loss: 6.8715e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 379/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0154 - output_original_loss: 6.9932e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 379: val_loss improved from 0.01916 to 0.01916, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.0154 - output_original_loss: 6.9932e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0192 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0093 - lr: 1.0000e-04\n",
            "Epoch 380/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 6.7298e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 380: val_loss improved from 0.01916 to 0.01914, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0153 - output_original_loss: 6.5910e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0191 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 381/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0154 - output_original_loss: 6.8433e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 381: val_loss improved from 0.01914 to 0.01914, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0154 - output_original_loss: 6.8433e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0191 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 382/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.6043e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 382: val_loss improved from 0.01914 to 0.01913, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0154 - output_original_loss: 7.4696e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0191 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 383/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 4.9889e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 383: val_loss improved from 0.01913 to 0.01911, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0153 - output_original_loss: 6.6049e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0191 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 384/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.5778e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 384: val_loss improved from 0.01911 to 0.01911, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0152 - output_original_loss: 5.9936e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0191 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 385/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 6.9549e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 385: val_loss improved from 0.01911 to 0.01911, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0152 - output_original_loss: 6.3723e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0191 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 386/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - output_original_loss: 6.9637e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 386: val_loss improved from 0.01911 to 0.01909, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0152 - output_original_loss: 6.2709e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0191 - val_output_original_loss: 0.0042 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 387/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 7.2544e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 387: val_loss improved from 0.01909 to 0.01908, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0155 - output_original_loss: 7.6536e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0191 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 388/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 6.0738e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 388: val_loss improved from 0.01908 to 0.01908, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0153 - output_original_loss: 6.9417e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0191 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 389/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 5.9288e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 389: val_loss improved from 0.01908 to 0.01907, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0152 - output_original_loss: 5.9184e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0191 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 390/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 4.8721e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 390: val_loss improved from 0.01907 to 0.01906, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0152 - output_original_loss: 6.3417e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0191 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 391/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0155 - output_original_loss: 8.3515e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 391: val_loss improved from 0.01906 to 0.01905, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0153 - output_original_loss: 6.8793e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 392/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 5.9147e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 392: val_loss did not improve from 0.01905\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0152 - output_original_loss: 6.2219e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0191 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 393/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 5.8651e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 393: val_loss improved from 0.01905 to 0.01903, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0152 - output_original_loss: 6.4031e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 394/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 7.8633e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 394: val_loss improved from 0.01903 to 0.01903, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.0152 - output_original_loss: 6.7614e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 395/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 5.7797e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 395: val_loss did not improve from 0.01903\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0153 - output_original_loss: 6.5605e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 396/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 7.6864e-04 - output_noisy_loss: 0.0086\n",
            "Epoch 396: val_loss improved from 0.01903 to 0.01902, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0152 - output_original_loss: 6.4345e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 397/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 6.2974e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 397: val_loss improved from 0.01902 to 0.01901, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0152 - output_original_loss: 6.4834e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 398/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 5.1093e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 398: val_loss improved from 0.01901 to 0.01900, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0152 - output_original_loss: 6.4238e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 399/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0152 - output_original_loss: 6.5241e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 399: val_loss improved from 0.01900 to 0.01899, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0152 - output_original_loss: 6.5241e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 400/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 7.5224e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 400: val_loss improved from 0.01899 to 0.01899, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.0152 - output_original_loss: 6.5842e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-04\n",
            "Epoch 401/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 4.6509e-04 - output_noisy_loss: 0.0092\n",
            "Epoch 401: val_loss improved from 0.01899 to 0.01899, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0152 - output_original_loss: 5.7650e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 402/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 8.0157e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 402: val_loss improved from 0.01899 to 0.01899, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0151 - output_original_loss: 6.6211e-04 - output_noisy_loss: 0.0087 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 403/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - output_original_loss: 6.8038e-04 - output_noisy_loss: 0.0092\n",
            "Epoch 403: val_loss improved from 0.01899 to 0.01898, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0151 - output_original_loss: 6.0347e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 404/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0158 - output_original_loss: 7.8222e-04 - output_noisy_loss: 0.0093\n",
            "Epoch 404: val_loss improved from 0.01898 to 0.01898, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0153 - output_original_loss: 6.4684e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 405/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 6.9454e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 405: val_loss improved from 0.01898 to 0.01898, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0150 - output_original_loss: 6.0989e-04 - output_noisy_loss: 0.0087 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 406/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0157 - output_original_loss: 7.5912e-04 - output_noisy_loss: 0.0092\n",
            "Epoch 406: val_loss improved from 0.01898 to 0.01898, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0153 - output_original_loss: 6.3925e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 407/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 8.0108e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 407: val_loss improved from 0.01898 to 0.01898, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0153 - output_original_loss: 7.3747e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 408/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 5.2081e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 408: val_loss improved from 0.01898 to 0.01898, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0152 - output_original_loss: 6.0490e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 409/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 6.2509e-04 - output_noisy_loss: 0.0086\n",
            "Epoch 409: val_loss improved from 0.01898 to 0.01898, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0151 - output_original_loss: 6.3728e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 410/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 5.9555e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 410: val_loss did not improve from 0.01898\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0153 - output_original_loss: 7.0956e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 411/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 3.6311e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 411: val_loss improved from 0.01898 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0151 - output_original_loss: 5.4034e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 412/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - output_original_loss: 7.8713e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 412: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0152 - output_original_loss: 6.6466e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 413/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 4.7989e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 413: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0152 - output_original_loss: 5.8191e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 414/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 7.4710e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 414: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0152 - output_original_loss: 6.6266e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 415/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0155 - output_original_loss: 7.3958e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 415: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0154 - output_original_loss: 7.3746e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 416/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 5.6442e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 416: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0151 - output_original_loss: 5.6941e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 417/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 7.6413e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 417: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0151 - output_original_loss: 6.0724e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 418/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 7.2050e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 418: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0153 - output_original_loss: 6.5131e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 419/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 3.9957e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 419: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0151 - output_original_loss: 6.3114e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 420/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 6.0196e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 420: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0152 - output_original_loss: 5.9340e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 421/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.7153e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 421: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0153 - output_original_loss: 7.1515e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 422/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 6.1179e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 422: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0151 - output_original_loss: 5.6515e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 423/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 7.3069e-04 - output_noisy_loss: 0.0086\n",
            "Epoch 423: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0151 - output_original_loss: 6.6636e-04 - output_noisy_loss: 0.0087 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 424/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 4.8410e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 424: val_loss improved from 0.01897 to 0.01897, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0153 - output_original_loss: 6.4073e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 425/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 6.1151e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 425: val_loss improved from 0.01897 to 0.01896, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0151 - output_original_loss: 6.2592e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 426/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.3192e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 426: val_loss improved from 0.01896 to 0.01896, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0151 - output_original_loss: 6.2894e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 427/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 5.1556e-04 - output_noisy_loss: 0.0092\n",
            "Epoch 427: val_loss improved from 0.01896 to 0.01896, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0152 - output_original_loss: 6.2340e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 428/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0149 - output_original_loss: 5.8614e-04 - output_noisy_loss: 0.0086\n",
            "Epoch 428: val_loss improved from 0.01896 to 0.01896, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0152 - output_original_loss: 6.2992e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 429/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 5.4020e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 429: val_loss improved from 0.01896 to 0.01896, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0151 - output_original_loss: 6.4742e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 430/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0149 - output_original_loss: 6.3047e-04 - output_noisy_loss: 0.0086\n",
            "Epoch 430: val_loss improved from 0.01896 to 0.01896, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0151 - output_original_loss: 5.8934e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 431/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 7.4807e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 431: val_loss improved from 0.01896 to 0.01896, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0152 - output_original_loss: 6.7134e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 432/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0149 - output_original_loss: 4.8774e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 432: val_loss improved from 0.01896 to 0.01896, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0152 - output_original_loss: 6.5271e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 433/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 5.0455e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 433: val_loss did not improve from 0.01896\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0152 - output_original_loss: 6.3561e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 434/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 7.0902e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 434: val_loss did not improve from 0.01896\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0152 - output_original_loss: 6.3260e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 435/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0147 - output_original_loss: 5.0423e-04 - output_noisy_loss: 0.0085\n",
            "Epoch 435: val_loss improved from 0.01896 to 0.01896, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0151 - output_original_loss: 6.0729e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 436/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 5.8026e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 436: val_loss did not improve from 0.01896\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0152 - output_original_loss: 5.7596e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 437/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 8.6208e-04 - output_noisy_loss: 0.0086\n",
            "Epoch 437: val_loss improved from 0.01896 to 0.01896, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0152 - output_original_loss: 7.2518e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 438/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 4.9434e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 438: val_loss improved from 0.01896 to 0.01895, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0152 - output_original_loss: 6.4645e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 439/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 4.7221e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 439: val_loss improved from 0.01895 to 0.01895, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0152 - output_original_loss: 6.7979e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 440/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 3.6174e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 440: val_loss improved from 0.01895 to 0.01895, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0152 - output_original_loss: 6.1504e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 441/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0149 - output_original_loss: 4.2661e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 441: val_loss improved from 0.01895 to 0.01895, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0152 - output_original_loss: 6.1856e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 442/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0148 - output_original_loss: 5.1964e-04 - output_noisy_loss: 0.0086\n",
            "Epoch 442: val_loss improved from 0.01895 to 0.01895, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0149 - output_original_loss: 5.9934e-04 - output_noisy_loss: 0.0086 - val_loss: 0.0190 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 443/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0157 - output_original_loss: 6.2414e-04 - output_noisy_loss: 0.0093\n",
            "Epoch 443: val_loss improved from 0.01895 to 0.01895, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0152 - output_original_loss: 5.5002e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 444/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0152 - output_original_loss: 6.8956e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 444: val_loss improved from 0.01895 to 0.01895, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0152 - output_original_loss: 6.8956e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 445/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.9079e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 445: val_loss improved from 0.01895 to 0.01895, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0152 - output_original_loss: 5.4642e-04 - output_noisy_loss: 0.0090 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 446/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 8.0476e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 446: val_loss improved from 0.01895 to 0.01895, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0152 - output_original_loss: 6.9535e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 447/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 5.9121e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 447: val_loss improved from 0.01895 to 0.01895, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0153 - output_original_loss: 6.7739e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 448/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 6.4173e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 448: val_loss improved from 0.01895 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0150 - output_original_loss: 5.8382e-04 - output_noisy_loss: 0.0087 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 449/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 5.7981e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 449: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0151 - output_original_loss: 6.2252e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 450/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 5.9019e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 450: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0152 - output_original_loss: 6.9744e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-05\n",
            "Epoch 451/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 6.2690e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 451: val_loss did not improve from 0.01894\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0152 - output_original_loss: 5.6523e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 452/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 7.2809e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 452: val_loss did not improve from 0.01894\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0151 - output_original_loss: 6.1171e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 453/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 8.6341e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 453: val_loss did not improve from 0.01894\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0151 - output_original_loss: 6.6205e-04 - output_noisy_loss: 0.0087 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 454/500\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.4158e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 454: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0153 - output_original_loss: 6.4158e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 455/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 7.9898e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 455: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0151 - output_original_loss: 6.5777e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 456/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 5.5637e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 456: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0152 - output_original_loss: 6.3791e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 457/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0149 - output_original_loss: 6.2207e-04 - output_noisy_loss: 0.0085\n",
            "Epoch 457: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0151 - output_original_loss: 5.8237e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 458/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 7.7543e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 458: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0153 - output_original_loss: 6.5719e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 459/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 5.8969e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 459: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0152 - output_original_loss: 6.7315e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 460/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 5.9254e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 460: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0151 - output_original_loss: 5.6931e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 461/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 8.1364e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 461: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0152 - output_original_loss: 6.4596e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 462/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 7.3688e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 462: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0151 - output_original_loss: 6.2704e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 463/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0149 - output_original_loss: 4.6422e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 463: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0152 - output_original_loss: 6.2873e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 464/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0149 - output_original_loss: 5.3848e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 464: val_loss did not improve from 0.01894\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0152 - output_original_loss: 6.4507e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 465/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 7.3168e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 465: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0151 - output_original_loss: 6.0860e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 466/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 5.6446e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 466: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0152 - output_original_loss: 6.5902e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 467/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 5.3042e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 467: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0152 - output_original_loss: 6.2691e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 468/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0149 - output_original_loss: 4.5499e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 468: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0151 - output_original_loss: 6.1069e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 469/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 5.4681e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 469: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0151 - output_original_loss: 5.5893e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 470/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0155 - output_original_loss: 8.1135e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 470: val_loss did not improve from 0.01894\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0152 - output_original_loss: 6.6408e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 471/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0155 - output_original_loss: 8.9078e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 471: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0152 - output_original_loss: 6.0480e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 472/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 7.7377e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 472: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0152 - output_original_loss: 6.8893e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 473/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 6.4277e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 473: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0151 - output_original_loss: 5.6105e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 474/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0155 - output_original_loss: 8.7096e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 474: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0151 - output_original_loss: 6.8783e-04 - output_noisy_loss: 0.0087 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 475/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - output_original_loss: 6.7743e-04 - output_noisy_loss: 0.0092\n",
            "Epoch 475: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0152 - output_original_loss: 6.3841e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 476/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 7.0870e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 476: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0151 - output_original_loss: 5.6312e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 477/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0155 - output_original_loss: 7.8100e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 477: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0153 - output_original_loss: 6.9946e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 478/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 5.8569e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 478: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0152 - output_original_loss: 6.8397e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 479/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 5.6021e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 479: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0151 - output_original_loss: 5.8585e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 480/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 6.2401e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 480: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0151 - output_original_loss: 6.4984e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 481/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 5.8900e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 481: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0151 - output_original_loss: 5.4440e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 482/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 7.2670e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 482: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0153 - output_original_loss: 6.9643e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 483/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 7.4922e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 483: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0152 - output_original_loss: 5.6468e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 484/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 9.0198e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 484: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0151 - output_original_loss: 7.1745e-04 - output_noisy_loss: 0.0087 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 485/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 4.9367e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 485: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0152 - output_original_loss: 6.4988e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 486/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0154 - output_original_loss: 7.1667e-04 - output_noisy_loss: 0.0090\n",
            "Epoch 486: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0152 - output_original_loss: 6.3381e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 487/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 6.4318e-04 - output_noisy_loss: 0.0087\n",
            "Epoch 487: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0152 - output_original_loss: 5.6034e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 488/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0149 - output_original_loss: 6.0286e-04 - output_noisy_loss: 0.0085\n",
            "Epoch 488: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0151 - output_original_loss: 6.1421e-04 - output_noisy_loss: 0.0087 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 489/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 6.7157e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 489: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0152 - output_original_loss: 6.3162e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 490/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 7.4281e-04 - output_noisy_loss: 0.0085\n",
            "Epoch 490: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0151 - output_original_loss: 6.3733e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 491/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.7531e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 491: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0153 - output_original_loss: 6.7373e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 492/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 6.3533e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 492: val_loss did not improve from 0.01894\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0151 - output_original_loss: 6.2293e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 493/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0152 - output_original_loss: 5.8661e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 493: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0152 - output_original_loss: 6.0797e-04 - output_noisy_loss: 0.0089 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 494/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0150 - output_original_loss: 6.3456e-04 - output_noisy_loss: 0.0086\n",
            "Epoch 494: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0151 - output_original_loss: 6.3403e-04 - output_noisy_loss: 0.0087 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 495/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0153 - output_original_loss: 7.3363e-04 - output_noisy_loss: 0.0088\n",
            "Epoch 495: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0152 - output_original_loss: 6.3757e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 496/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 4.2441e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 496: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0151 - output_original_loss: 6.2789e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 497/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0157 - output_original_loss: 8.9593e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 497: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0152 - output_original_loss: 6.1305e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 498/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - output_original_loss: 7.0680e-04 - output_noisy_loss: 0.0092\n",
            "Epoch 498: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0152 - output_original_loss: 6.5663e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 499/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0155 - output_original_loss: 7.0829e-04 - output_noisy_loss: 0.0091\n",
            "Epoch 499: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0152 - output_original_loss: 6.5125e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n",
            "Epoch 500/500\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - output_original_loss: 4.9437e-04 - output_noisy_loss: 0.0089\n",
            "Epoch 500: val_loss improved from 0.01894 to 0.01894, saving model to best_model_Siamese.h5\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0151 - output_original_loss: 5.3843e-04 - output_noisy_loss: 0.0088 - val_loss: 0.0189 - val_output_original_loss: 0.0041 - val_output_noisy_loss: 0.0092 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate\n",
        "\n",
        "# Define the shared layers for the Siamese network\n",
        "def shared_layers(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    shared_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(input_layer)\n",
        "    shared_layer_2 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_1)\n",
        "    shared_layer_3 = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_2)\n",
        "    shared_layer_4 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_3)\n",
        "    shared_layer_5 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_4)\n",
        "    shared_layer_6 = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.00001))(shared_layer_5)\n",
        "    return Model(inputs=input_layer, outputs=shared_layer_6)\n",
        "\n",
        "# Define the input layers for original and noisy data\n",
        "input_original = Input(shape=(4,), name='original')\n",
        "input_noisy = Input(shape=(4,), name='noisy')\n",
        "\n",
        "# Create shared layers for both original and noisy data\n",
        "shared_model = shared_layers((4,))\n",
        "\n",
        "# Apply shared layers to both inputs\n",
        "shared_original = shared_model(input_original)\n",
        "shared_noisy = shared_model(input_noisy)\n",
        "\n",
        "# Define the output layers for original and noisy data branches\n",
        "output_original = Dense(1, name='output_original')(shared_original)\n",
        "output_noisy = Dense(1, name='output_noisy')(shared_noisy)\n",
        "\n",
        "def learning_rate_schedule(epoch, initial_lr=0.001):\n",
        "    \"\"\"\n",
        "    Custom learning rate schedule. Adjust the learning rate based on the current epoch.\n",
        "    You can customize this function to fit your needs.\n",
        "    \"\"\"\n",
        "    if epoch < 300 or epoch == 300:\n",
        "        return initial_lr  # Keep the initial learning rate for the first 50 epochs\n",
        "    elif epoch > 300 and epoch%50 == 0:\n",
        "        initial_lr = initial_lr * 0.1  # Reduce the learning rate by a factor of 10 after epoch 50\n",
        "        return initial_lr\n",
        "    else:\n",
        "        return initial_lr\n",
        "\n",
        "# Define custom loss function\n",
        "def custom_loss_contrastive(y_true, y_pred):\n",
        "    epsilon = 10e-6\n",
        "    loss_2 = 1/(tf.losses.mean_squared_error(y_true, y_pred)+epsilon)\n",
        "\n",
        "    #print('MSE_original', loss_1)\n",
        "    # Combine the losses as per your requirements\n",
        "    # total_loss = loss_1 + loss_2\n",
        "\n",
        "    return loss_2\n",
        "\n",
        "def custom_loss_contrastive_abs(y_true, y_pred):\n",
        "    epsilon = 10e-6\n",
        "    loss_2 = 1 / (tf.losses.mean_absolute_error(y_true, y_pred) + epsilon)\n",
        "\n",
        "    return loss_2\n",
        "\n",
        "# Create the Adam optimizer with the initial learning rate\n",
        "initial_learning_rate = 0.001\n",
        "adam_optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
        "\n",
        "# Compile the model with custom losses\n",
        "model = Model(inputs=[input_original, input_noisy], outputs=[output_original, output_noisy])\n",
        "model.compile(optimizer=adam_optimizer, loss={'output_original': 'mse', 'output_noisy': custom_loss_contrastive_abs})\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Define a ModelCheckpoint callback to save the best model\n",
        "checkpoint = ModelCheckpoint('best_model_Siamese.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "# Calculate the number of steps\n",
        "num_steps = len(X_train) // batch_size\n",
        "val_steps = len(X_test) // batch_size\n",
        "\n",
        "print(val_steps)\n",
        "print(batch_size)\n",
        "\n",
        "# Train the model using your custom data loading pipeline\n",
        "# You need to replace X_train and y_train with your custom data loading logic\n",
        "# You can use tf.data.Dataset for custom data loading pipelines\n",
        "history = model.fit(\n",
        "    custom_data_loader(X_train, y_train, batch_size),  # Replace with your custom data loader\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    steps_per_epoch=num_steps,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=[checkpoint, lr_scheduler]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN6R4XHxE2Zo"
      },
      "outputs": [],
      "source": [
        "# len(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwei1mOb_kT5",
        "outputId": "f1bfabf3-1c8f-412d-cb61-2c1d33ce5084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0197 - output_original_loss: 0.0050 - output_noisy_loss: 0.0087\n",
            "Test Loss: [0.019691752269864082, 0.00501715112477541, 0.008660530671477318]\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model('/content/best_model_Siamese.h5', custom_objects={'custom_loss_contrastive_abs': custom_loss_contrastive_abs})\n",
        "\n",
        "losses = model.evaluate(val_dataset, steps=val_steps)\n",
        "print(\"Test Loss:\", losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLGn0FYR6pfT",
        "outputId": "8a796e4b-08aa-4141-ab39-5def1b690de6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 297ms/step - loss: 0.0189 - output_original_loss: 0.0041 - output_noisy_loss: 0.0092\n",
            "Test Loss: [0.018939677625894547, 0.004071385599672794, 0.009154844097793102]\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model('/content/best_model_Siamese.h5', custom_objects={'custom_loss_contrastive_abs': custom_loss_contrastive_abs})\n",
        "\n",
        "losses = model.evaluate(val_dataset, batch_size = 128, steps=val_steps) # steps=val_steps\n",
        "print(\"Test Loss:\", losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju8UKTCWAe7Z",
        "outputId": "dfe7bd33-f7c0-45f4-a540-cf0efde9ed90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 213ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions_1 = model.predict(val_dataset, batch_size = 128, steps=val_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRt7_kpJAjjC",
        "outputId": "e393a224-8ae9-49c2-aab6-c791572e882e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions_1[0].flatten().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--YGiwSDOrVy"
      },
      "outputs": [],
      "source": [
        "file_path = 'predictions.csv' #withStandardizing\n",
        "\n",
        "# Save the NumPy array to a CSV file\n",
        "np.savetxt(file_path, predictions_1[0].flatten(), delimiter=',', fmt='%.15f')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZNcv50VTsp_",
        "outputId": "1ae4d35e-b638-4e60-9784-0b31a0b41895"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=({'original': TensorSpec(shape=(None, 4), dtype=tf.float64, name=None), 'noisy': TensorSpec(shape=(None, 4), dtype=tf.float64, name=None)}, TensorSpec(shape=(None,), dtype=tf.float64, name=None))>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcKschCWGJtv",
        "outputId": "311538dc-11a7-456f-d30e-769034ee7468"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1.65166239,  0.93924733,  1.82976615,  0.40493604, -1.01989408,\n",
              "       -0.30747902,  1.82976615,  1.47355862,  0.76114357,  0.22683227,\n",
              "        0.04872851,  0.5830398 ,  0.40493604,  0.22683227,  0.5830398 ,\n",
              "       -0.48558278, -0.66368655, -1.01989408, -0.48558278,  1.82976615])"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test[:,0][:20] #frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWufPJRgGZdm",
        "outputId": "008fdf00-e191-4a35-ec82-0d52447769b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([10.08333333, 10.51666667, 10.        , 10.84166667, 11.72533333,\n",
              "       11.2816    , 10.        , 10.19166667, 10.625     , 10.95      ,\n",
              "       11.05973333, 10.73333333, 10.84166667, 10.95      , 10.73333333,\n",
              "       11.39253333, 11.50346667, 11.72533333, 11.39253333, 10.        ])"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test[:20] #true values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "PZM_ZJYzGBzn",
        "outputId": "807caaaa-c158-45cc-9ddc-5243fb3c3556"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2400x1500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# sorted_indices = np.argsort(X_test[:, 0][:256])\n",
        "# sorted_X_test = X_test[:, 0][:256][sorted_indices]\n",
        "# sorted_y_test = y_test[:256][sorted_indices]\n",
        "# sorted_predictions = predictions_1[0].flatten()[sorted_indices]\n",
        "\n",
        "plt.figure(figsize=(8, 5), dpi=300)\n",
        "\n",
        "# Plot the data with customizations\n",
        "plt.scatter(X_test[:, 0][:256], y_test[:256], label='True values')\n",
        "# plt.scatter(X_test[:, 0][:256], predictions_1[0].flatten(), label='Predictions')\n",
        "plt.xlabel('Experimental Permittivity Values', fontsize=12)\n",
        "plt.ylabel('Error', fontsize=12)\n",
        "plt.title('', fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Customize the axis labels and ticks (adjust as needed)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "# plt.xlim(9, 12)\n",
        "# plt.ylim(-0.01, 0.125)\n",
        "\n",
        "# Save the plot as a high-resolution image (adjust the format as needed)\n",
        "# plt.savefig('/content/drive/MyDrive/IEEE EMBS SMP Project/results/simulation_real_error_plot.png', format='png', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ5_eHk-QOlS"
      },
      "source": [
        "# Y_test and predictions vs frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZGR9J3qQkYi"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a pandas DataFrame\n",
        "df_vsFreq = pd.read_csv('/content/drive/MyDrive/IEEE EMBS SMP Project/Data_Simulation/new_simulations/y_test.csv')\n",
        "\n",
        "# Convert the DataFrame to a NumPy array\n",
        "predictions_vsFreq = df_vsFreq.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wangQsVRCyJ",
        "outputId": "64a25214-19a8-46cb-9f01-ed6416c60dad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(127, 3)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions_vsFreq.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI1vMpHdQ7-E"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5), dpi=300)\n",
        "\n",
        "# Plot the data with customizations\n",
        "plt.scatter(predictions_vsFreq[:,0], predictions_vsFreq[:,1], label='True values')\n",
        "plt.scatter(predictions_vsFreq[:,0], predictions_vsFreq[:,2], label='Predictions')\n",
        "plt.xlabel('Frequency (GHz)', fontsize=12)\n",
        "plt.ylabel('Permittivity Values', fontsize=12)\n",
        "plt.title('', fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Customize the axis labels and ticks (adjust as needed)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "# plt.xlim(9, 12)\n",
        "# plt.ylim(-0.01, 0.125)\n",
        "\n",
        "# Save the plot as a high-resolution image (adjust the format as needed)\n",
        "plt.savefig('/content/drive/MyDrive/IEEE EMBS SMP Project/results/vsFreq.png', format='png', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXyt93uqVn86"
      },
      "source": [
        "For one set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWm3SYFgVLqO"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/IEEE EMBS SMP Project/Data_Simulation/new_simulations/oneFreqPoint.csv')\n",
        "\n",
        "# Convert the DataFrame to a NumPy array\n",
        "full_array = df.values\n",
        "\n",
        "# print(full_array)\n",
        "\n",
        "sum_array_scaled = scaler.fit_transform(full_array[:,1:5])\n",
        "\n",
        "val_dataset = custom_data_loader(sum_array_scaled, full_array[:,5], batch_size=21, noise_stddev=1, shuffle=False)\n",
        "\n",
        "model = tf.keras.models.load_model('/content/best_model_Siamese.h5', custom_objects={'custom_loss_contrastive_abs': custom_loss_contrastive_abs})\n",
        "\n",
        "predictions_1 = model.predict(val_dataset, batch_size = 21, steps=1)\n",
        "\n",
        "print(predictions_1[0])\n",
        "\n",
        "plt.figure(figsize=(8, 5), dpi=300)\n",
        "\n",
        "# Plot the data with customizations\n",
        "plt.scatter(full_array[:,0], full_array[:,5], label='True values')\n",
        "plt.scatter(full_array[:,0], predictions_1[0].flatten(), label='Predictions')\n",
        "plt.xlabel('Frequency (GHz)', fontsize=12)\n",
        "plt.ylabel('Permittivity Values', fontsize=12)\n",
        "plt.title('', fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Customize the axis labels and ticks (adjust as needed)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "# plt.xlim(9, 12)\n",
        "# plt.ylim(-0.01, 0.125)\n",
        "\n",
        "# Save the plot as a high-resolution image (adjust the format as needed)\n",
        "# plt.savefig('/content/drive/MyDrive/IEEE EMBS SMP Project/results/vsFreq.png', format='png', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZl5c05N4vCU"
      },
      "source": [
        "# SOTA comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRBW4Tzj6FAz"
      },
      "source": [
        "ISAP/Micro-optical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaaFvbeJ4x4p",
        "outputId": "32a23ac0-fe24-4fa5-d03c-86d51242852a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/4 [======>.......................] - ETA: 5s - loss: 25.2245\n",
            "Epoch 1: val_loss improved from inf to 19.05173, saving model to isap_model.h5\n",
            "4/4 [==============================] - 2s 124ms/step - loss: 24.0187 - val_loss: 19.0517 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 21.3917\n",
            "Epoch 2: val_loss improved from 19.05173 to 14.37118, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 18.9024 - val_loss: 14.3712 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 14.9475\n",
            "Epoch 3: val_loss improved from 14.37118 to 10.01736, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 68ms/step - loss: 14.3460 - val_loss: 10.0174 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 11.3532\n",
            "Epoch 4: val_loss improved from 10.01736 to 6.56438, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 9.8023 - val_loss: 6.5644 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 7.9377\n",
            "Epoch 5: val_loss improved from 6.56438 to 4.00153, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 6.0237 - val_loss: 4.0015 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 3.5009\n",
            "Epoch 6: val_loss improved from 4.00153 to 3.11128, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 3.5785 - val_loss: 3.1113 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 3.1929\n",
            "Epoch 7: val_loss did not improve from 3.11128\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 2.8418 - val_loss: 3.3271 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 2.4655\n",
            "Epoch 8: val_loss improved from 3.11128 to 3.04786, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 2.9172 - val_loss: 3.0479 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 2.2237\n",
            "Epoch 9: val_loss improved from 3.04786 to 2.29488, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 2.4326 - val_loss: 2.2949 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.4878\n",
            "Epoch 10: val_loss improved from 2.29488 to 1.80715, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1.8104 - val_loss: 1.8071 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 2.2285\n",
            "Epoch 11: val_loss improved from 1.80715 to 1.62005, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 73ms/step - loss: 1.5689 - val_loss: 1.6201 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.4286\n",
            "Epoch 12: val_loss improved from 1.62005 to 1.53044, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 1.4785 - val_loss: 1.5304 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.4728\n",
            "Epoch 13: val_loss improved from 1.53044 to 1.39784, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.3831 - val_loss: 1.3978 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.0200\n",
            "Epoch 14: val_loss improved from 1.39784 to 1.33207, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 1.2541 - val_loss: 1.3321 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.2773\n",
            "Epoch 15: val_loss improved from 1.33207 to 1.27629, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 80ms/step - loss: 1.1523 - val_loss: 1.2763 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.9523\n",
            "Epoch 16: val_loss did not improve from 1.27629\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 1.0824 - val_loss: 1.2918 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.0598\n",
            "Epoch 17: val_loss improved from 1.27629 to 1.15039, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 75ms/step - loss: 1.0391 - val_loss: 1.1504 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.9028\n",
            "Epoch 18: val_loss improved from 1.15039 to 1.12958, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 75ms/step - loss: 0.9803 - val_loss: 1.1296 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.1387\n",
            "Epoch 19: val_loss improved from 1.12958 to 1.01610, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.0096 - val_loss: 1.0161 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.9712\n",
            "Epoch 20: val_loss did not improve from 1.01610\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.8828 - val_loss: 1.0459 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.9651\n",
            "Epoch 21: val_loss improved from 1.01610 to 1.01090, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.8611 - val_loss: 1.0109 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.0747\n",
            "Epoch 22: val_loss improved from 1.01090 to 0.98376, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.8535 - val_loss: 0.9838 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6840\n",
            "Epoch 23: val_loss did not improve from 0.98376\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.7883 - val_loss: 0.9953 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.8742\n",
            "Epoch 24: val_loss improved from 0.98376 to 0.90572, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.7717 - val_loss: 0.9057 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.7206\n",
            "Epoch 25: val_loss improved from 0.90572 to 0.90259, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.7430 - val_loss: 0.9026 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5305\n",
            "Epoch 26: val_loss improved from 0.90259 to 0.89103, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.7230 - val_loss: 0.8910 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6204\n",
            "Epoch 27: val_loss improved from 0.89103 to 0.84596, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.7027 - val_loss: 0.8460 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5346\n",
            "Epoch 28: val_loss improved from 0.84596 to 0.83973, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6685 - val_loss: 0.8397 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6711\n",
            "Epoch 29: val_loss improved from 0.83973 to 0.80525, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.6942 - val_loss: 0.8053 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6907\n",
            "Epoch 30: val_loss did not improve from 0.80525\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.6876 - val_loss: 0.8849 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.7740\n",
            "Epoch 31: val_loss did not improve from 0.80525\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6708 - val_loss: 0.8120 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5961\n",
            "Epoch 32: val_loss improved from 0.80525 to 0.77530, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6349 - val_loss: 0.7753 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4665\n",
            "Epoch 33: val_loss improved from 0.77530 to 0.76769, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5936 - val_loss: 0.7677 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5489\n",
            "Epoch 34: val_loss improved from 0.76769 to 0.75720, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.5815 - val_loss: 0.7572 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5975\n",
            "Epoch 35: val_loss improved from 0.75720 to 0.74731, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5720 - val_loss: 0.7473 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4916\n",
            "Epoch 36: val_loss improved from 0.74731 to 0.74721, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5520 - val_loss: 0.7472 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4676\n",
            "Epoch 37: val_loss improved from 0.74721 to 0.72760, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.5926 - val_loss: 0.7276 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5301\n",
            "Epoch 38: val_loss did not improve from 0.72760\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5415 - val_loss: 0.7433 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5609\n",
            "Epoch 39: val_loss improved from 0.72760 to 0.68396, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.5635 - val_loss: 0.6840 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6210\n",
            "Epoch 40: val_loss did not improve from 0.68396\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.5116 - val_loss: 0.7203 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3143\n",
            "Epoch 41: val_loss improved from 0.68396 to 0.66381, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4996 - val_loss: 0.6638 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3700\n",
            "Epoch 42: val_loss did not improve from 0.66381\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4655 - val_loss: 0.6878 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5353\n",
            "Epoch 43: val_loss did not improve from 0.66381\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.5198 - val_loss: 0.6914 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4985\n",
            "Epoch 44: val_loss improved from 0.66381 to 0.64749, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5024 - val_loss: 0.6475 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4353\n",
            "Epoch 45: val_loss improved from 0.64749 to 0.61565, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4554 - val_loss: 0.6157 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4120\n",
            "Epoch 46: val_loss did not improve from 0.61565\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4474 - val_loss: 0.6340 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4249\n",
            "Epoch 47: val_loss did not improve from 0.61565\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4395 - val_loss: 0.6457 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4887\n",
            "Epoch 48: val_loss improved from 0.61565 to 0.58867, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4404 - val_loss: 0.5887 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3558\n",
            "Epoch 49: val_loss did not improve from 0.58867\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4233 - val_loss: 0.5932 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4595\n",
            "Epoch 50: val_loss did not improve from 0.58867\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.3966 - val_loss: 0.5903 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4073\n",
            "Epoch 51: val_loss did not improve from 0.58867\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4225 - val_loss: 0.5941 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2921\n",
            "Epoch 52: val_loss did not improve from 0.58867\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5279 - val_loss: 0.5895 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4218\n",
            "Epoch 53: val_loss did not improve from 0.58867\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.4124 - val_loss: 0.6480 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3372\n",
            "Epoch 54: val_loss improved from 0.58867 to 0.57912, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.4259 - val_loss: 0.5791 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3730\n",
            "Epoch 55: val_loss did not improve from 0.57912\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3816 - val_loss: 0.6273 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4237\n",
            "Epoch 56: val_loss improved from 0.57912 to 0.53400, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.4129 - val_loss: 0.5340 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4836\n",
            "Epoch 57: val_loss improved from 0.53400 to 0.52625, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3673 - val_loss: 0.5262 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3408\n",
            "Epoch 58: val_loss did not improve from 0.52625\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3480 - val_loss: 0.5495 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3565\n",
            "Epoch 59: val_loss improved from 0.52625 to 0.52603, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3862 - val_loss: 0.5260 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3024\n",
            "Epoch 60: val_loss did not improve from 0.52603\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3423 - val_loss: 0.5614 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2884\n",
            "Epoch 61: val_loss improved from 0.52603 to 0.48839, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.3462 - val_loss: 0.4884 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3386\n",
            "Epoch 62: val_loss improved from 0.48839 to 0.48805, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.3351 - val_loss: 0.4880 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3257\n",
            "Epoch 63: val_loss improved from 0.48805 to 0.48456, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3121 - val_loss: 0.4846 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2674\n",
            "Epoch 64: val_loss improved from 0.48456 to 0.47827, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3172 - val_loss: 0.4783 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2365\n",
            "Epoch 65: val_loss improved from 0.47827 to 0.45577, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.2976 - val_loss: 0.4558 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2397\n",
            "Epoch 66: val_loss did not improve from 0.45577\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.3128 - val_loss: 0.4735 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3619\n",
            "Epoch 67: val_loss improved from 0.45577 to 0.45045, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 0.3092 - val_loss: 0.4505 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3343\n",
            "Epoch 68: val_loss did not improve from 0.45045\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.2756 - val_loss: 0.4704 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2617\n",
            "Epoch 69: val_loss improved from 0.45045 to 0.44108, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.2749 - val_loss: 0.4411 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3472\n",
            "Epoch 70: val_loss improved from 0.44108 to 0.42417, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 0.2882 - val_loss: 0.4242 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2476\n",
            "Epoch 71: val_loss did not improve from 0.42417\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2693 - val_loss: 0.4324 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2791\n",
            "Epoch 72: val_loss did not improve from 0.42417\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.2532 - val_loss: 0.4408 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2778\n",
            "Epoch 73: val_loss improved from 0.42417 to 0.41797, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.2536 - val_loss: 0.4180 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2851\n",
            "Epoch 74: val_loss improved from 0.41797 to 0.40634, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 0.2384 - val_loss: 0.4063 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2660\n",
            "Epoch 75: val_loss improved from 0.40634 to 0.39036, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.2480 - val_loss: 0.3904 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2532\n",
            "Epoch 76: val_loss did not improve from 0.39036\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2366 - val_loss: 0.3964 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1863\n",
            "Epoch 77: val_loss did not improve from 0.39036\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.2324 - val_loss: 0.3909 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2842\n",
            "Epoch 78: val_loss improved from 0.39036 to 0.37610, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 0.2214 - val_loss: 0.3761 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2506\n",
            "Epoch 79: val_loss improved from 0.37610 to 0.37099, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.2152 - val_loss: 0.3710 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2026\n",
            "Epoch 80: val_loss did not improve from 0.37099\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.2350 - val_loss: 0.3990 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1865\n",
            "Epoch 81: val_loss did not improve from 0.37099\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.2227 - val_loss: 0.3754 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2136\n",
            "Epoch 82: val_loss did not improve from 0.37099\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.2114 - val_loss: 0.3796 - lr: 0.0010\n",
            "Epoch 83/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2372\n",
            "Epoch 83: val_loss improved from 0.37099 to 0.36280, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2163 - val_loss: 0.3628 - lr: 0.0010\n",
            "Epoch 84/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1710\n",
            "Epoch 84: val_loss improved from 0.36280 to 0.35611, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 0.2068 - val_loss: 0.3561 - lr: 0.0010\n",
            "Epoch 85/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2143\n",
            "Epoch 85: val_loss did not improve from 0.35611\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2145 - val_loss: 0.3597 - lr: 0.0010\n",
            "Epoch 86/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1882\n",
            "Epoch 86: val_loss did not improve from 0.35611\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2100 - val_loss: 0.3695 - lr: 0.0010\n",
            "Epoch 87/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2086\n",
            "Epoch 87: val_loss improved from 0.35611 to 0.34863, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 0.2336 - val_loss: 0.3486 - lr: 0.0010\n",
            "Epoch 88/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2014\n",
            "Epoch 88: val_loss did not improve from 0.34863\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.2518 - val_loss: 0.3534 - lr: 0.0010\n",
            "Epoch 89/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2013\n",
            "Epoch 89: val_loss did not improve from 0.34863\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.2059 - val_loss: 0.4084 - lr: 0.0010\n",
            "Epoch 90/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2973\n",
            "Epoch 90: val_loss did not improve from 0.34863\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.2658 - val_loss: 0.4323 - lr: 0.0010\n",
            "Epoch 91/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2166\n",
            "Epoch 91: val_loss improved from 0.34863 to 0.32917, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 0.2879 - val_loss: 0.3292 - lr: 0.0010\n",
            "Epoch 92/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1591\n",
            "Epoch 92: val_loss did not improve from 0.32917\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.2329 - val_loss: 0.3410 - lr: 0.0010\n",
            "Epoch 93/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1981\n",
            "Epoch 93: val_loss did not improve from 0.32917\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2203 - val_loss: 0.3755 - lr: 0.0010\n",
            "Epoch 94/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1764\n",
            "Epoch 94: val_loss did not improve from 0.32917\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1821 - val_loss: 0.3439 - lr: 0.0010\n",
            "Epoch 95/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2319\n",
            "Epoch 95: val_loss improved from 0.32917 to 0.31915, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.2229 - val_loss: 0.3191 - lr: 0.0010\n",
            "Epoch 96/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1573\n",
            "Epoch 96: val_loss did not improve from 0.31915\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.1944 - val_loss: 0.3303 - lr: 0.0010\n",
            "Epoch 97/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1965\n",
            "Epoch 97: val_loss did not improve from 0.31915\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.1999 - val_loss: 0.3277 - lr: 0.0010\n",
            "Epoch 98/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1484\n",
            "Epoch 98: val_loss did not improve from 0.31915\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2043 - val_loss: 0.4393 - lr: 0.0010\n",
            "Epoch 99/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2938\n",
            "Epoch 99: val_loss improved from 0.31915 to 0.31620, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 64ms/step - loss: 0.2365 - val_loss: 0.3162 - lr: 0.0010\n",
            "Epoch 100/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2082\n",
            "Epoch 100: val_loss improved from 0.31620 to 0.29934, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.1966 - val_loss: 0.2993 - lr: 0.0010\n",
            "Epoch 101/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2024\n",
            "Epoch 101: val_loss did not improve from 0.29934\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1762 - val_loss: 0.3154 - lr: 0.0010\n",
            "Epoch 102/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1899\n",
            "Epoch 102: val_loss improved from 0.29934 to 0.29210, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1650 - val_loss: 0.2921 - lr: 0.0010\n",
            "Epoch 103/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1865\n",
            "Epoch 103: val_loss did not improve from 0.29210\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1617 - val_loss: 0.3052 - lr: 0.0010\n",
            "Epoch 104/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1299\n",
            "Epoch 104: val_loss improved from 0.29210 to 0.28745, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.1621 - val_loss: 0.2874 - lr: 0.0010\n",
            "Epoch 105/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1559\n",
            "Epoch 105: val_loss improved from 0.28745 to 0.27936, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1628 - val_loss: 0.2794 - lr: 0.0010\n",
            "Epoch 106/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1466\n",
            "Epoch 106: val_loss did not improve from 0.27936\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1481 - val_loss: 0.3030 - lr: 0.0010\n",
            "Epoch 107/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1289\n",
            "Epoch 107: val_loss did not improve from 0.27936\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1734 - val_loss: 0.3146 - lr: 0.0010\n",
            "Epoch 108/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1748\n",
            "Epoch 108: val_loss did not improve from 0.27936\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1995 - val_loss: 0.3161 - lr: 0.0010\n",
            "Epoch 109/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1977\n",
            "Epoch 109: val_loss improved from 0.27936 to 0.27492, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1620 - val_loss: 0.2749 - lr: 0.0010\n",
            "Epoch 110/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1270\n",
            "Epoch 110: val_loss did not improve from 0.27492\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1454 - val_loss: 0.2949 - lr: 0.0010\n",
            "Epoch 111/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1146\n",
            "Epoch 111: val_loss did not improve from 0.27492\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1383 - val_loss: 0.2767 - lr: 0.0010\n",
            "Epoch 112/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1280\n",
            "Epoch 112: val_loss did not improve from 0.27492\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1583 - val_loss: 0.2756 - lr: 0.0010\n",
            "Epoch 113/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1112\n",
            "Epoch 113: val_loss improved from 0.27492 to 0.27104, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.1469 - val_loss: 0.2710 - lr: 0.0010\n",
            "Epoch 114/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1407\n",
            "Epoch 114: val_loss improved from 0.27104 to 0.26777, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1458 - val_loss: 0.2678 - lr: 0.0010\n",
            "Epoch 115/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1632\n",
            "Epoch 115: val_loss did not improve from 0.26777\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1430 - val_loss: 0.2965 - lr: 0.0010\n",
            "Epoch 116/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1592\n",
            "Epoch 116: val_loss improved from 0.26777 to 0.25231, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.1452 - val_loss: 0.2523 - lr: 0.0010\n",
            "Epoch 117/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1619\n",
            "Epoch 117: val_loss did not improve from 0.25231\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1417 - val_loss: 0.2577 - lr: 0.0010\n",
            "Epoch 118/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1413\n",
            "Epoch 118: val_loss did not improve from 0.25231\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1308 - val_loss: 0.2816 - lr: 0.0010\n",
            "Epoch 119/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1284\n",
            "Epoch 119: val_loss did not improve from 0.25231\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1346 - val_loss: 0.2549 - lr: 0.0010\n",
            "Epoch 120/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1207\n",
            "Epoch 120: val_loss improved from 0.25231 to 0.24630, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.1519 - val_loss: 0.2463 - lr: 0.0010\n",
            "Epoch 121/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1628\n",
            "Epoch 121: val_loss did not improve from 0.24630\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1226 - val_loss: 0.3356 - lr: 0.0010\n",
            "Epoch 122/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2345\n",
            "Epoch 122: val_loss did not improve from 0.24630\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1628 - val_loss: 0.2627 - lr: 0.0010\n",
            "Epoch 123/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1283\n",
            "Epoch 123: val_loss did not improve from 0.24630\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1514 - val_loss: 0.2545 - lr: 0.0010\n",
            "Epoch 124/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1083\n",
            "Epoch 124: val_loss improved from 0.24630 to 0.24452, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1403 - val_loss: 0.2445 - lr: 0.0010\n",
            "Epoch 125/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1026\n",
            "Epoch 125: val_loss did not improve from 0.24452\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1413 - val_loss: 0.3000 - lr: 0.0010\n",
            "Epoch 126/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1587\n",
            "Epoch 126: val_loss did not improve from 0.24452\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1687 - val_loss: 0.2955 - lr: 0.0010\n",
            "Epoch 127/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1446\n",
            "Epoch 127: val_loss did not improve from 0.24452\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1486 - val_loss: 0.2798 - lr: 0.0010\n",
            "Epoch 128/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2067\n",
            "Epoch 128: val_loss did not improve from 0.24452\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1363 - val_loss: 0.2749 - lr: 0.0010\n",
            "Epoch 129/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2182\n",
            "Epoch 129: val_loss improved from 0.24452 to 0.23004, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1342 - val_loss: 0.2300 - lr: 0.0010\n",
            "Epoch 130/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1141\n",
            "Epoch 130: val_loss improved from 0.23004 to 0.22871, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.1154 - val_loss: 0.2287 - lr: 0.0010\n",
            "Epoch 131/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0990\n",
            "Epoch 131: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1128 - val_loss: 0.2302 - lr: 0.0010\n",
            "Epoch 132/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1212\n",
            "Epoch 132: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1165 - val_loss: 0.2317 - lr: 0.0010\n",
            "Epoch 133/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1145\n",
            "Epoch 133: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1170 - val_loss: 0.2308 - lr: 0.0010\n",
            "Epoch 134/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1049\n",
            "Epoch 134: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1077 - val_loss: 0.2385 - lr: 0.0010\n",
            "Epoch 135/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0795\n",
            "Epoch 135: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1159 - val_loss: 0.2749 - lr: 0.0010\n",
            "Epoch 136/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1798\n",
            "Epoch 136: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1199 - val_loss: 0.2411 - lr: 0.0010\n",
            "Epoch 137/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0792\n",
            "Epoch 137: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1306 - val_loss: 0.2435 - lr: 0.0010\n",
            "Epoch 138/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0979\n",
            "Epoch 138: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 0.1157 - val_loss: 0.2500 - lr: 0.0010\n",
            "Epoch 139/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1044\n",
            "Epoch 139: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.1780 - val_loss: 0.2644 - lr: 0.0010\n",
            "Epoch 140/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1370\n",
            "Epoch 140: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1302 - val_loss: 0.2698 - lr: 0.0010\n",
            "Epoch 141/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1777\n",
            "Epoch 141: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.1262 - val_loss: 0.2405 - lr: 0.0010\n",
            "Epoch 142/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1112\n",
            "Epoch 142: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1248 - val_loss: 0.2595 - lr: 0.0010\n",
            "Epoch 143/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1495\n",
            "Epoch 143: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1461 - val_loss: 0.2705 - lr: 0.0010\n",
            "Epoch 144/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1681\n",
            "Epoch 144: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1363 - val_loss: 0.2629 - lr: 0.0010\n",
            "Epoch 145/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1801\n",
            "Epoch 145: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1291 - val_loss: 0.3442 - lr: 0.0010\n",
            "Epoch 146/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1755\n",
            "Epoch 146: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1813 - val_loss: 0.2917 - lr: 0.0010\n",
            "Epoch 147/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1874\n",
            "Epoch 147: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1681 - val_loss: 0.3060 - lr: 0.0010\n",
            "Epoch 148/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2033\n",
            "Epoch 148: val_loss did not improve from 0.22871\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1372 - val_loss: 0.2536 - lr: 0.0010\n",
            "Epoch 149/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1550\n",
            "Epoch 149: val_loss improved from 0.22871 to 0.20428, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 0.1244 - val_loss: 0.2043 - lr: 0.0010\n",
            "Epoch 150/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1404\n",
            "Epoch 150: val_loss improved from 0.20428 to 0.19602, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 0.1129 - val_loss: 0.1960 - lr: 0.0010\n",
            "Epoch 151/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0993\n",
            "Epoch 151: val_loss did not improve from 0.19602\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1156 - val_loss: 0.1977 - lr: 0.0010\n",
            "Epoch 152/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1230\n",
            "Epoch 152: val_loss did not improve from 0.19602\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.1007 - val_loss: 0.2104 - lr: 0.0010\n",
            "Epoch 153/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1033\n",
            "Epoch 153: val_loss did not improve from 0.19602\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1103 - val_loss: 0.2386 - lr: 0.0010\n",
            "Epoch 154/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1062\n",
            "Epoch 154: val_loss improved from 0.19602 to 0.19529, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.1353 - val_loss: 0.1953 - lr: 0.0010\n",
            "Epoch 155/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0647\n",
            "Epoch 155: val_loss did not improve from 0.19529\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.1067 - val_loss: 0.1955 - lr: 0.0010\n",
            "Epoch 156/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0964\n",
            "Epoch 156: val_loss did not improve from 0.19529\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1024 - val_loss: 0.2100 - lr: 0.0010\n",
            "Epoch 157/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1044\n",
            "Epoch 157: val_loss improved from 0.19529 to 0.19090, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 0.1011 - val_loss: 0.1909 - lr: 0.0010\n",
            "Epoch 158/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0901\n",
            "Epoch 158: val_loss did not improve from 0.19090\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0909 - val_loss: 0.2033 - lr: 0.0010\n",
            "Epoch 159/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0766\n",
            "Epoch 159: val_loss improved from 0.19090 to 0.18853, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 0.0892 - val_loss: 0.1885 - lr: 0.0010\n",
            "Epoch 160/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1040\n",
            "Epoch 160: val_loss did not improve from 0.18853\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0844 - val_loss: 0.1928 - lr: 0.0010\n",
            "Epoch 161/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0610\n",
            "Epoch 161: val_loss did not improve from 0.18853\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0838 - val_loss: 0.1898 - lr: 0.0010\n",
            "Epoch 162/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0541\n",
            "Epoch 162: val_loss did not improve from 0.18853\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0822 - val_loss: 0.2021 - lr: 0.0010\n",
            "Epoch 163/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0875\n",
            "Epoch 163: val_loss did not improve from 0.18853\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0936 - val_loss: 0.2133 - lr: 0.0010\n",
            "Epoch 164/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0794\n",
            "Epoch 164: val_loss did not improve from 0.18853\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0911 - val_loss: 0.1930 - lr: 0.0010\n",
            "Epoch 165/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0685\n",
            "Epoch 165: val_loss did not improve from 0.18853\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0869 - val_loss: 0.1991 - lr: 0.0010\n",
            "Epoch 166/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1153\n",
            "Epoch 166: val_loss did not improve from 0.18853\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.1269 - val_loss: 0.1979 - lr: 0.0010\n",
            "Epoch 167/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0831\n",
            "Epoch 167: val_loss did not improve from 0.18853\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1132 - val_loss: 0.1911 - lr: 0.0010\n",
            "Epoch 168/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1345\n",
            "Epoch 168: val_loss did not improve from 0.18853\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1245 - val_loss: 0.2016 - lr: 0.0010\n",
            "Epoch 169/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0811\n",
            "Epoch 169: val_loss did not improve from 0.18853\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0980 - val_loss: 0.2026 - lr: 0.0010\n",
            "Epoch 170/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1168\n",
            "Epoch 170: val_loss improved from 0.18853 to 0.18099, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.1024 - val_loss: 0.1810 - lr: 0.0010\n",
            "Epoch 171/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0486\n",
            "Epoch 171: val_loss did not improve from 0.18099\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0852 - val_loss: 0.1879 - lr: 0.0010\n",
            "Epoch 172/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0763\n",
            "Epoch 172: val_loss did not improve from 0.18099\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0842 - val_loss: 0.1840 - lr: 0.0010\n",
            "Epoch 173/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0600\n",
            "Epoch 173: val_loss did not improve from 0.18099\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0933 - val_loss: 0.1886 - lr: 0.0010\n",
            "Epoch 174/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0884\n",
            "Epoch 174: val_loss improved from 0.18099 to 0.17687, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0858 - val_loss: 0.1769 - lr: 0.0010\n",
            "Epoch 175/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1222\n",
            "Epoch 175: val_loss did not improve from 0.17687\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0834 - val_loss: 0.2006 - lr: 0.0010\n",
            "Epoch 176/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1033\n",
            "Epoch 176: val_loss did not improve from 0.17687\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1028 - val_loss: 0.1864 - lr: 0.0010\n",
            "Epoch 177/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0933\n",
            "Epoch 177: val_loss did not improve from 0.17687\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1122 - val_loss: 0.1807 - lr: 0.0010\n",
            "Epoch 178/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0670\n",
            "Epoch 178: val_loss did not improve from 0.17687\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1074 - val_loss: 0.1953 - lr: 0.0010\n",
            "Epoch 179/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0547\n",
            "Epoch 179: val_loss did not improve from 0.17687\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0979 - val_loss: 0.2126 - lr: 0.0010\n",
            "Epoch 180/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1024\n",
            "Epoch 180: val_loss did not improve from 0.17687\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0977 - val_loss: 0.1905 - lr: 0.0010\n",
            "Epoch 181/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0515\n",
            "Epoch 181: val_loss did not improve from 0.17687\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0823 - val_loss: 0.1772 - lr: 0.0010\n",
            "Epoch 182/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0793\n",
            "Epoch 182: val_loss did not improve from 0.17687\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0768 - val_loss: 0.1887 - lr: 0.0010\n",
            "Epoch 183/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0837\n",
            "Epoch 183: val_loss did not improve from 0.17687\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0946 - val_loss: 0.1967 - lr: 0.0010\n",
            "Epoch 184/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1029\n",
            "Epoch 184: val_loss improved from 0.17687 to 0.17335, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0906 - val_loss: 0.1734 - lr: 0.0010\n",
            "Epoch 185/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0765\n",
            "Epoch 185: val_loss did not improve from 0.17335\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0841 - val_loss: 0.1924 - lr: 0.0010\n",
            "Epoch 186/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1079\n",
            "Epoch 186: val_loss did not improve from 0.17335\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1388 - val_loss: 0.2153 - lr: 0.0010\n",
            "Epoch 187/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1558\n",
            "Epoch 187: val_loss did not improve from 0.17335\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1215 - val_loss: 0.1768 - lr: 0.0010\n",
            "Epoch 188/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1230\n",
            "Epoch 188: val_loss did not improve from 0.17335\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0958 - val_loss: 0.1760 - lr: 0.0010\n",
            "Epoch 189/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0978\n",
            "Epoch 189: val_loss improved from 0.17335 to 0.16476, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0953 - val_loss: 0.1648 - lr: 0.0010\n",
            "Epoch 190/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0437\n",
            "Epoch 190: val_loss did not improve from 0.16476\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0813 - val_loss: 0.1734 - lr: 0.0010\n",
            "Epoch 191/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1089\n",
            "Epoch 191: val_loss did not improve from 0.16476\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0900 - val_loss: 0.1703 - lr: 0.0010\n",
            "Epoch 192/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0877\n",
            "Epoch 192: val_loss did not improve from 0.16476\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0756 - val_loss: 0.1653 - lr: 0.0010\n",
            "Epoch 193/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0987\n",
            "Epoch 193: val_loss did not improve from 0.16476\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0766 - val_loss: 0.1750 - lr: 0.0010\n",
            "Epoch 194/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0562\n",
            "Epoch 194: val_loss did not improve from 0.16476\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0811 - val_loss: 0.1772 - lr: 0.0010\n",
            "Epoch 195/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0530\n",
            "Epoch 195: val_loss improved from 0.16476 to 0.16063, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0937 - val_loss: 0.1606 - lr: 0.0010\n",
            "Epoch 196/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0568\n",
            "Epoch 196: val_loss did not improve from 0.16063\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0750 - val_loss: 0.1644 - lr: 0.0010\n",
            "Epoch 197/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0600\n",
            "Epoch 197: val_loss improved from 0.16063 to 0.15353, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0718 - val_loss: 0.1535 - lr: 0.0010\n",
            "Epoch 198/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0775\n",
            "Epoch 198: val_loss did not improve from 0.15353\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0720 - val_loss: 0.1555 - lr: 0.0010\n",
            "Epoch 199/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0729\n",
            "Epoch 199: val_loss did not improve from 0.15353\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0730 - val_loss: 0.1586 - lr: 0.0010\n",
            "Epoch 200/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0797\n",
            "Epoch 200: val_loss improved from 0.15353 to 0.14967, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0739 - val_loss: 0.1497 - lr: 0.0010\n",
            "Epoch 201/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0484\n",
            "Epoch 201: val_loss did not improve from 0.14967\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0669 - val_loss: 0.1685 - lr: 0.0010\n",
            "Epoch 202/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0728\n",
            "Epoch 202: val_loss did not improve from 0.14967\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0673 - val_loss: 0.1522 - lr: 0.0010\n",
            "Epoch 203/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0994\n",
            "Epoch 203: val_loss did not improve from 0.14967\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0685 - val_loss: 0.1651 - lr: 0.0010\n",
            "Epoch 204/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0723\n",
            "Epoch 204: val_loss did not improve from 0.14967\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0649 - val_loss: 0.1514 - lr: 0.0010\n",
            "Epoch 205/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0837\n",
            "Epoch 205: val_loss did not improve from 0.14967\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0679 - val_loss: 0.1559 - lr: 0.0010\n",
            "Epoch 206/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0800\n",
            "Epoch 206: val_loss improved from 0.14967 to 0.14874, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0632 - val_loss: 0.1487 - lr: 0.0010\n",
            "Epoch 207/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0720\n",
            "Epoch 207: val_loss did not improve from 0.14874\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0608 - val_loss: 0.1595 - lr: 0.0010\n",
            "Epoch 208/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0554\n",
            "Epoch 208: val_loss did not improve from 0.14874\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0631 - val_loss: 0.1518 - lr: 0.0010\n",
            "Epoch 209/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0487\n",
            "Epoch 209: val_loss did not improve from 0.14874\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0601 - val_loss: 0.1488 - lr: 0.0010\n",
            "Epoch 210/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0496\n",
            "Epoch 210: val_loss did not improve from 0.14874\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0602 - val_loss: 0.1506 - lr: 0.0010\n",
            "Epoch 211/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0645\n",
            "Epoch 211: val_loss did not improve from 0.14874\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0635 - val_loss: 0.1580 - lr: 0.0010\n",
            "Epoch 212/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0618\n",
            "Epoch 212: val_loss did not improve from 0.14874\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0651 - val_loss: 0.1509 - lr: 0.0010\n",
            "Epoch 213/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0596\n",
            "Epoch 213: val_loss did not improve from 0.14874\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0753 - val_loss: 0.1705 - lr: 0.0010\n",
            "Epoch 214/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0642\n",
            "Epoch 214: val_loss did not improve from 0.14874\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0708 - val_loss: 0.1490 - lr: 0.0010\n",
            "Epoch 215/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0452\n",
            "Epoch 215: val_loss did not improve from 0.14874\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0713 - val_loss: 0.1838 - lr: 0.0010\n",
            "Epoch 216/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0672\n",
            "Epoch 216: val_loss improved from 0.14874 to 0.14550, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0966 - val_loss: 0.1455 - lr: 0.0010\n",
            "Epoch 217/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0437\n",
            "Epoch 217: val_loss did not improve from 0.14550\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0626 - val_loss: 0.1467 - lr: 0.0010\n",
            "Epoch 218/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0596\n",
            "Epoch 218: val_loss improved from 0.14550 to 0.14464, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0629 - val_loss: 0.1446 - lr: 0.0010\n",
            "Epoch 219/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0351\n",
            "Epoch 219: val_loss improved from 0.14464 to 0.14332, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0600 - val_loss: 0.1433 - lr: 0.0010\n",
            "Epoch 220/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0741\n",
            "Epoch 220: val_loss did not improve from 0.14332\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0560 - val_loss: 0.1604 - lr: 0.0010\n",
            "Epoch 221/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0698\n",
            "Epoch 221: val_loss did not improve from 0.14332\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0628 - val_loss: 0.1503 - lr: 0.0010\n",
            "Epoch 222/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0989\n",
            "Epoch 222: val_loss did not improve from 0.14332\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0628 - val_loss: 0.1452 - lr: 0.0010\n",
            "Epoch 223/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0323\n",
            "Epoch 223: val_loss did not improve from 0.14332\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0639 - val_loss: 0.1434 - lr: 0.0010\n",
            "Epoch 224/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0498\n",
            "Epoch 224: val_loss improved from 0.14332 to 0.13740, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0627 - val_loss: 0.1374 - lr: 0.0010\n",
            "Epoch 225/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0544\n",
            "Epoch 225: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0648 - val_loss: 0.1561 - lr: 0.0010\n",
            "Epoch 226/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0650\n",
            "Epoch 226: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0781 - val_loss: 0.1491 - lr: 0.0010\n",
            "Epoch 227/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0753\n",
            "Epoch 227: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0897 - val_loss: 0.1415 - lr: 0.0010\n",
            "Epoch 228/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0600\n",
            "Epoch 228: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0795 - val_loss: 0.1386 - lr: 0.0010\n",
            "Epoch 229/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0423\n",
            "Epoch 229: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0698 - val_loss: 0.1452 - lr: 0.0010\n",
            "Epoch 230/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0651\n",
            "Epoch 230: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0623 - val_loss: 0.1453 - lr: 0.0010\n",
            "Epoch 231/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0561\n",
            "Epoch 231: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0633 - val_loss: 0.2246 - lr: 0.0010\n",
            "Epoch 232/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1048\n",
            "Epoch 232: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1050 - val_loss: 0.1617 - lr: 0.0010\n",
            "Epoch 233/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0765\n",
            "Epoch 233: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0822 - val_loss: 0.1554 - lr: 0.0010\n",
            "Epoch 234/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0928\n",
            "Epoch 234: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0760 - val_loss: 0.1485 - lr: 0.0010\n",
            "Epoch 235/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0595\n",
            "Epoch 235: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0657 - val_loss: 0.1475 - lr: 0.0010\n",
            "Epoch 236/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0598\n",
            "Epoch 236: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0613 - val_loss: 0.1531 - lr: 0.0010\n",
            "Epoch 237/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0491\n",
            "Epoch 237: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0550 - val_loss: 0.1444 - lr: 0.0010\n",
            "Epoch 238/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0762\n",
            "Epoch 238: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0586 - val_loss: 0.1488 - lr: 0.0010\n",
            "Epoch 239/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0480\n",
            "Epoch 239: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0640 - val_loss: 0.1967 - lr: 0.0010\n",
            "Epoch 240/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1095\n",
            "Epoch 240: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0875 - val_loss: 0.1791 - lr: 0.0010\n",
            "Epoch 241/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0750\n",
            "Epoch 241: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0799 - val_loss: 0.1543 - lr: 0.0010\n",
            "Epoch 242/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0799\n",
            "Epoch 242: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0778 - val_loss: 0.1784 - lr: 0.0010\n",
            "Epoch 243/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0547\n",
            "Epoch 243: val_loss did not improve from 0.13740\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0798 - val_loss: 0.1421 - lr: 0.0010\n",
            "Epoch 244/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0663\n",
            "Epoch 244: val_loss improved from 0.13740 to 0.13317, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0621 - val_loss: 0.1332 - lr: 0.0010\n",
            "Epoch 245/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0557\n",
            "Epoch 245: val_loss did not improve from 0.13317\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0549 - val_loss: 0.1441 - lr: 0.0010\n",
            "Epoch 246/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0661\n",
            "Epoch 246: val_loss did not improve from 0.13317\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0641 - val_loss: 0.1361 - lr: 0.0010\n",
            "Epoch 247/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0445\n",
            "Epoch 247: val_loss did not improve from 0.13317\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0548 - val_loss: 0.1399 - lr: 0.0010\n",
            "Epoch 248/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0940\n",
            "Epoch 248: val_loss did not improve from 0.13317\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0608 - val_loss: 0.1369 - lr: 0.0010\n",
            "Epoch 249/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0651\n",
            "Epoch 249: val_loss did not improve from 0.13317\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0543 - val_loss: 0.1351 - lr: 0.0010\n",
            "Epoch 250/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0470\n",
            "Epoch 250: val_loss did not improve from 0.13317\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0512 - val_loss: 0.1358 - lr: 0.0010\n",
            "Epoch 251/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0706\n",
            "Epoch 251: val_loss did not improve from 0.13317\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0491 - val_loss: 0.1345 - lr: 0.0010\n",
            "Epoch 252/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0459\n",
            "Epoch 252: val_loss did not improve from 0.13317\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0469 - val_loss: 0.1381 - lr: 0.0010\n",
            "Epoch 253/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0618\n",
            "Epoch 253: val_loss improved from 0.13317 to 0.13253, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0495 - val_loss: 0.1325 - lr: 0.0010\n",
            "Epoch 254/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0339\n",
            "Epoch 254: val_loss improved from 0.13253 to 0.12548, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0465 - val_loss: 0.1255 - lr: 0.0010\n",
            "Epoch 255/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0314\n",
            "Epoch 255: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0430 - val_loss: 0.1293 - lr: 0.0010\n",
            "Epoch 256/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0614\n",
            "Epoch 256: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0507 - val_loss: 0.1587 - lr: 0.0010\n",
            "Epoch 257/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0648\n",
            "Epoch 257: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0701 - val_loss: 0.1487 - lr: 0.0010\n",
            "Epoch 258/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0790\n",
            "Epoch 258: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0753 - val_loss: 0.1710 - lr: 0.0010\n",
            "Epoch 259/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0756\n",
            "Epoch 259: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0725 - val_loss: 0.1630 - lr: 0.0010\n",
            "Epoch 260/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0843\n",
            "Epoch 260: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0743 - val_loss: 0.1503 - lr: 0.0010\n",
            "Epoch 261/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0571\n",
            "Epoch 261: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0651 - val_loss: 0.1346 - lr: 0.0010\n",
            "Epoch 262/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0561\n",
            "Epoch 262: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0537 - val_loss: 0.1417 - lr: 0.0010\n",
            "Epoch 263/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0715\n",
            "Epoch 263: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0540 - val_loss: 0.1379 - lr: 0.0010\n",
            "Epoch 264/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0635\n",
            "Epoch 264: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0640 - val_loss: 0.1807 - lr: 0.0010\n",
            "Epoch 265/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0612\n",
            "Epoch 265: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0780 - val_loss: 0.1815 - lr: 0.0010\n",
            "Epoch 266/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1213\n",
            "Epoch 266: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1010 - val_loss: 0.1525 - lr: 0.0010\n",
            "Epoch 267/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0752\n",
            "Epoch 267: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0645 - val_loss: 0.1360 - lr: 0.0010\n",
            "Epoch 268/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0596\n",
            "Epoch 268: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0617 - val_loss: 0.1437 - lr: 0.0010\n",
            "Epoch 269/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0512\n",
            "Epoch 269: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0512 - val_loss: 0.1422 - lr: 0.0010\n",
            "Epoch 270/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0656\n",
            "Epoch 270: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0631 - val_loss: 0.1605 - lr: 0.0010\n",
            "Epoch 271/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0546\n",
            "Epoch 271: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0605 - val_loss: 0.2052 - lr: 0.0010\n",
            "Epoch 272/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0971\n",
            "Epoch 272: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1667 - val_loss: 0.2347 - lr: 0.0010\n",
            "Epoch 273/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1567\n",
            "Epoch 273: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1493 - val_loss: 0.1795 - lr: 0.0010\n",
            "Epoch 274/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0996\n",
            "Epoch 274: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0857 - val_loss: 0.2400 - lr: 0.0010\n",
            "Epoch 275/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1460\n",
            "Epoch 275: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1367 - val_loss: 0.2393 - lr: 0.0010\n",
            "Epoch 276/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1712\n",
            "Epoch 276: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1787 - val_loss: 0.3134 - lr: 0.0010\n",
            "Epoch 277/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2170\n",
            "Epoch 277: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1668 - val_loss: 0.1890 - lr: 0.0010\n",
            "Epoch 278/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1532\n",
            "Epoch 278: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1466 - val_loss: 0.1567 - lr: 0.0010\n",
            "Epoch 279/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1026\n",
            "Epoch 279: val_loss did not improve from 0.12548\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1182 - val_loss: 0.1391 - lr: 0.0010\n",
            "Epoch 280/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0657\n",
            "Epoch 280: val_loss improved from 0.12548 to 0.11871, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.1009 - val_loss: 0.1187 - lr: 0.0010\n",
            "Epoch 281/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0439\n",
            "Epoch 281: val_loss improved from 0.11871 to 0.11668, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0664 - val_loss: 0.1167 - lr: 0.0010\n",
            "Epoch 282/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0477\n",
            "Epoch 282: val_loss did not improve from 0.11668\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0761 - val_loss: 0.1732 - lr: 0.0010\n",
            "Epoch 283/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1119\n",
            "Epoch 283: val_loss did not improve from 0.11668\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1151 - val_loss: 0.1757 - lr: 0.0010\n",
            "Epoch 284/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0896\n",
            "Epoch 284: val_loss did not improve from 0.11668\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0841 - val_loss: 0.2137 - lr: 0.0010\n",
            "Epoch 285/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1335\n",
            "Epoch 285: val_loss did not improve from 0.11668\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0858 - val_loss: 0.1644 - lr: 0.0010\n",
            "Epoch 286/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0600\n",
            "Epoch 286: val_loss did not improve from 0.11668\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0688 - val_loss: 0.1432 - lr: 0.0010\n",
            "Epoch 287/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0742\n",
            "Epoch 287: val_loss did not improve from 0.11668\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0739 - val_loss: 0.1552 - lr: 0.0010\n",
            "Epoch 288/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0573\n",
            "Epoch 288: val_loss did not improve from 0.11668\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0582 - val_loss: 0.1204 - lr: 0.0010\n",
            "Epoch 289/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0543\n",
            "Epoch 289: val_loss did not improve from 0.11668\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0490 - val_loss: 0.1378 - lr: 0.0010\n",
            "Epoch 290/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0545\n",
            "Epoch 290: val_loss improved from 0.11668 to 0.10801, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0520 - val_loss: 0.1080 - lr: 0.0010\n",
            "Epoch 291/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0299\n",
            "Epoch 291: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0481 - val_loss: 0.1225 - lr: 0.0010\n",
            "Epoch 292/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0510\n",
            "Epoch 292: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0533 - val_loss: 0.1532 - lr: 0.0010\n",
            "Epoch 293/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0996\n",
            "Epoch 293: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0572 - val_loss: 0.1250 - lr: 0.0010\n",
            "Epoch 294/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0540\n",
            "Epoch 294: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0514 - val_loss: 0.1313 - lr: 0.0010\n",
            "Epoch 295/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0586\n",
            "Epoch 295: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0488 - val_loss: 0.1288 - lr: 0.0010\n",
            "Epoch 296/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0587\n",
            "Epoch 296: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0541 - val_loss: 0.1228 - lr: 0.0010\n",
            "Epoch 297/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0374\n",
            "Epoch 297: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0552 - val_loss: 0.1298 - lr: 0.0010\n",
            "Epoch 298/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0482\n",
            "Epoch 298: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0451 - val_loss: 0.1117 - lr: 0.0010\n",
            "Epoch 299/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0320\n",
            "Epoch 299: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0449 - val_loss: 0.1099 - lr: 0.0010\n",
            "Epoch 300/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0472\n",
            "Epoch 300: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0436 - val_loss: 0.1394 - lr: 0.0010\n",
            "Epoch 301/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0420\n",
            "Epoch 301: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0463 - val_loss: 0.1220 - lr: 0.0010\n",
            "Epoch 302/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0464\n",
            "Epoch 302: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0547 - val_loss: 0.1295 - lr: 0.0010\n",
            "Epoch 303/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0511\n",
            "Epoch 303: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0535 - val_loss: 0.1338 - lr: 0.0010\n",
            "Epoch 304/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0583\n",
            "Epoch 304: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0544 - val_loss: 0.1315 - lr: 0.0010\n",
            "Epoch 305/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0332\n",
            "Epoch 305: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0448 - val_loss: 0.1199 - lr: 0.0010\n",
            "Epoch 306/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0370\n",
            "Epoch 306: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0419 - val_loss: 0.1268 - lr: 0.0010\n",
            "Epoch 307/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0300\n",
            "Epoch 307: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0426 - val_loss: 0.1091 - lr: 0.0010\n",
            "Epoch 308/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0423\n",
            "Epoch 308: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0399 - val_loss: 0.1124 - lr: 0.0010\n",
            "Epoch 309/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0318\n",
            "Epoch 309: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0396 - val_loss: 0.1373 - lr: 0.0010\n",
            "Epoch 310/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0387\n",
            "Epoch 310: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0473 - val_loss: 0.1092 - lr: 0.0010\n",
            "Epoch 311/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0314\n",
            "Epoch 311: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0446 - val_loss: 0.1176 - lr: 0.0010\n",
            "Epoch 312/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0360\n",
            "Epoch 312: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0447 - val_loss: 0.1202 - lr: 0.0010\n",
            "Epoch 313/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0536\n",
            "Epoch 313: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0382 - val_loss: 0.1161 - lr: 0.0010\n",
            "Epoch 314/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0265\n",
            "Epoch 314: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0412 - val_loss: 0.1162 - lr: 0.0010\n",
            "Epoch 315/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0353\n",
            "Epoch 315: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0459 - val_loss: 0.1491 - lr: 0.0010\n",
            "Epoch 316/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0528\n",
            "Epoch 316: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0598 - val_loss: 0.1275 - lr: 0.0010\n",
            "Epoch 317/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0602\n",
            "Epoch 317: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0558 - val_loss: 0.1669 - lr: 0.0010\n",
            "Epoch 318/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0902\n",
            "Epoch 318: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0613 - val_loss: 0.1233 - lr: 0.0010\n",
            "Epoch 319/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0623\n",
            "Epoch 319: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0522 - val_loss: 0.1330 - lr: 0.0010\n",
            "Epoch 320/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0490\n",
            "Epoch 320: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0518 - val_loss: 0.1347 - lr: 0.0010\n",
            "Epoch 321/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0988\n",
            "Epoch 321: val_loss did not improve from 0.10801\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0819 - val_loss: 0.1189 - lr: 0.0010\n",
            "Epoch 322/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0355\n",
            "Epoch 322: val_loss improved from 0.10801 to 0.09029, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0503 - val_loss: 0.0903 - lr: 0.0010\n",
            "Epoch 323/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0239\n",
            "Epoch 323: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0383 - val_loss: 0.0933 - lr: 0.0010\n",
            "Epoch 324/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0387\n",
            "Epoch 324: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0362 - val_loss: 0.1129 - lr: 0.0010\n",
            "Epoch 325/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0296\n",
            "Epoch 325: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0409 - val_loss: 0.1030 - lr: 0.0010\n",
            "Epoch 326/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0372\n",
            "Epoch 326: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0426 - val_loss: 0.1067 - lr: 0.0010\n",
            "Epoch 327/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0457\n",
            "Epoch 327: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0453 - val_loss: 0.1563 - lr: 0.0010\n",
            "Epoch 328/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0543\n",
            "Epoch 328: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0587 - val_loss: 0.1283 - lr: 0.0010\n",
            "Epoch 329/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0760\n",
            "Epoch 329: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0629 - val_loss: 0.1186 - lr: 0.0010\n",
            "Epoch 330/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0482\n",
            "Epoch 330: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0475 - val_loss: 0.1388 - lr: 0.0010\n",
            "Epoch 331/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0756\n",
            "Epoch 331: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0499 - val_loss: 0.1280 - lr: 0.0010\n",
            "Epoch 332/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0301\n",
            "Epoch 332: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0701 - val_loss: 0.1363 - lr: 0.0010\n",
            "Epoch 333/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0659\n",
            "Epoch 333: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0808 - val_loss: 0.1450 - lr: 0.0010\n",
            "Epoch 334/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0526\n",
            "Epoch 334: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0661 - val_loss: 0.1157 - lr: 0.0010\n",
            "Epoch 335/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0501\n",
            "Epoch 335: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0417 - val_loss: 0.1127 - lr: 0.0010\n",
            "Epoch 336/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0394\n",
            "Epoch 336: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0372 - val_loss: 0.1262 - lr: 0.0010\n",
            "Epoch 337/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0320\n",
            "Epoch 337: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0413 - val_loss: 0.1233 - lr: 0.0010\n",
            "Epoch 338/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0313\n",
            "Epoch 338: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0482 - val_loss: 0.1165 - lr: 0.0010\n",
            "Epoch 339/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0458\n",
            "Epoch 339: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0402 - val_loss: 0.1246 - lr: 0.0010\n",
            "Epoch 340/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0403\n",
            "Epoch 340: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0470 - val_loss: 0.1059 - lr: 0.0010\n",
            "Epoch 341/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0320\n",
            "Epoch 341: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0403 - val_loss: 0.0992 - lr: 0.0010\n",
            "Epoch 342/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0365\n",
            "Epoch 342: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0348 - val_loss: 0.0999 - lr: 0.0010\n",
            "Epoch 343/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0312\n",
            "Epoch 343: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0325 - val_loss: 0.1131 - lr: 0.0010\n",
            "Epoch 344/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0237\n",
            "Epoch 344: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0362 - val_loss: 0.1069 - lr: 0.0010\n",
            "Epoch 345/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0367\n",
            "Epoch 345: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0394 - val_loss: 0.0977 - lr: 0.0010\n",
            "Epoch 346/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0435\n",
            "Epoch 346: val_loss did not improve from 0.09029\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0428 - val_loss: 0.1443 - lr: 0.0010\n",
            "Epoch 347/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0792\n",
            "Epoch 347: val_loss improved from 0.09029 to 0.08978, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0632 - val_loss: 0.0898 - lr: 0.0010\n",
            "Epoch 348/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0300\n",
            "Epoch 348: val_loss did not improve from 0.08978\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0593 - val_loss: 0.1407 - lr: 0.0010\n",
            "Epoch 349/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0973\n",
            "Epoch 349: val_loss did not improve from 0.08978\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0774 - val_loss: 0.1835 - lr: 0.0010\n",
            "Epoch 350/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1049\n",
            "Epoch 350: val_loss did not improve from 0.08978\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0772 - val_loss: 0.1115 - lr: 0.0010\n",
            "Epoch 351/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0576\n",
            "Epoch 351: val_loss did not improve from 0.08978\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0524 - val_loss: 0.0911 - lr: 1.0000e-04\n",
            "Epoch 352/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0419\n",
            "Epoch 352: val_loss did not improve from 0.08978\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0350 - val_loss: 0.0986 - lr: 1.0000e-04\n",
            "Epoch 353/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0323\n",
            "Epoch 353: val_loss did not improve from 0.08978\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0367 - val_loss: 0.0922 - lr: 1.0000e-04\n",
            "Epoch 354/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0342\n",
            "Epoch 354: val_loss improved from 0.08978 to 0.08876, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0317 - val_loss: 0.0888 - lr: 1.0000e-04\n",
            "Epoch 355/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0378\n",
            "Epoch 355: val_loss did not improve from 0.08876\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0323 - val_loss: 0.0888 - lr: 1.0000e-04\n",
            "Epoch 356/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0208\n",
            "Epoch 356: val_loss did not improve from 0.08876\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0301 - val_loss: 0.0906 - lr: 1.0000e-04\n",
            "Epoch 357/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0394\n",
            "Epoch 357: val_loss did not improve from 0.08876\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0291 - val_loss: 0.0930 - lr: 1.0000e-04\n",
            "Epoch 358/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0440\n",
            "Epoch 358: val_loss did not improve from 0.08876\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0291 - val_loss: 0.0911 - lr: 1.0000e-04\n",
            "Epoch 359/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0149\n",
            "Epoch 359: val_loss did not improve from 0.08876\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0282 - val_loss: 0.0894 - lr: 1.0000e-04\n",
            "Epoch 360/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0258\n",
            "Epoch 360: val_loss improved from 0.08876 to 0.08856, saving model to isap_model.h5\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0280 - val_loss: 0.0886 - lr: 1.0000e-04\n",
            "Epoch 361/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0252\n",
            "Epoch 361: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0293 - val_loss: 0.0891 - lr: 1.0000e-04\n",
            "Epoch 362/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0373\n",
            "Epoch 362: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0291 - val_loss: 0.0896 - lr: 1.0000e-04\n",
            "Epoch 363/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0370\n",
            "Epoch 363: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0279 - val_loss: 0.0914 - lr: 1.0000e-04\n",
            "Epoch 364/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0153\n",
            "Epoch 364: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0280 - val_loss: 0.0918 - lr: 1.0000e-04\n",
            "Epoch 365/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0204\n",
            "Epoch 365: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0278 - val_loss: 0.0915 - lr: 1.0000e-04\n",
            "Epoch 366/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0172\n",
            "Epoch 366: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.0930 - lr: 1.0000e-04\n",
            "Epoch 367/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0252\n",
            "Epoch 367: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0282 - val_loss: 0.0940 - lr: 1.0000e-04\n",
            "Epoch 368/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0219\n",
            "Epoch 368: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0278 - val_loss: 0.0917 - lr: 1.0000e-04\n",
            "Epoch 369/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0217\n",
            "Epoch 369: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0274 - val_loss: 0.0911 - lr: 1.0000e-04\n",
            "Epoch 370/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0312\n",
            "Epoch 370: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0274 - val_loss: 0.0910 - lr: 1.0000e-04\n",
            "Epoch 371/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0176\n",
            "Epoch 371: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0277 - val_loss: 0.0907 - lr: 1.0000e-04\n",
            "Epoch 372/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0171\n",
            "Epoch 372: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0275 - val_loss: 0.0913 - lr: 1.0000e-04\n",
            "Epoch 373/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0301\n",
            "Epoch 373: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0274 - val_loss: 0.0911 - lr: 1.0000e-04\n",
            "Epoch 374/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 374: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0278 - val_loss: 0.0924 - lr: 1.0000e-04\n",
            "Epoch 375/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0351\n",
            "Epoch 375: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.0918 - lr: 1.0000e-04\n",
            "Epoch 376/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0348\n",
            "Epoch 376: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0272 - val_loss: 0.0922 - lr: 1.0000e-04\n",
            "Epoch 377/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0252\n",
            "Epoch 377: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0271 - val_loss: 0.0916 - lr: 1.0000e-04\n",
            "Epoch 378/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0175\n",
            "Epoch 378: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0272 - val_loss: 0.0909 - lr: 1.0000e-04\n",
            "Epoch 379/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0376\n",
            "Epoch 379: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0275 - val_loss: 0.0909 - lr: 1.0000e-04\n",
            "Epoch 380/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0333\n",
            "Epoch 380: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0276 - val_loss: 0.0918 - lr: 1.0000e-04\n",
            "Epoch 381/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0184\n",
            "Epoch 381: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0273 - val_loss: 0.0925 - lr: 1.0000e-04\n",
            "Epoch 382/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0258\n",
            "Epoch 382: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0269 - val_loss: 0.0925 - lr: 1.0000e-04\n",
            "Epoch 383/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0273\n",
            "Epoch 383: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0271 - val_loss: 0.0922 - lr: 1.0000e-04\n",
            "Epoch 384/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0149\n",
            "Epoch 384: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0271 - val_loss: 0.0925 - lr: 1.0000e-04\n",
            "Epoch 385/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0320\n",
            "Epoch 385: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0269 - val_loss: 0.0940 - lr: 1.0000e-04\n",
            "Epoch 386/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0181\n",
            "Epoch 386: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0271 - val_loss: 0.0946 - lr: 1.0000e-04\n",
            "Epoch 387/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0289\n",
            "Epoch 387: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0271 - val_loss: 0.0960 - lr: 1.0000e-04\n",
            "Epoch 388/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0355\n",
            "Epoch 388: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.1016 - lr: 1.0000e-04\n",
            "Epoch 389/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0222\n",
            "Epoch 389: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0291 - val_loss: 0.0993 - lr: 1.0000e-04\n",
            "Epoch 390/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0249\n",
            "Epoch 390: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0280 - val_loss: 0.0950 - lr: 1.0000e-04\n",
            "Epoch 391/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0161\n",
            "Epoch 391: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0274 - val_loss: 0.0937 - lr: 1.0000e-04\n",
            "Epoch 392/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0307\n",
            "Epoch 392: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0278 - val_loss: 0.0927 - lr: 1.0000e-04\n",
            "Epoch 393/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0275\n",
            "Epoch 393: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0280 - val_loss: 0.0932 - lr: 1.0000e-04\n",
            "Epoch 394/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0336\n",
            "Epoch 394: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0266 - val_loss: 0.0954 - lr: 1.0000e-04\n",
            "Epoch 395/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0208\n",
            "Epoch 395: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0271 - val_loss: 0.0974 - lr: 1.0000e-04\n",
            "Epoch 396/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0264\n",
            "Epoch 396: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0290 - val_loss: 0.0993 - lr: 1.0000e-04\n",
            "Epoch 397/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0227\n",
            "Epoch 397: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0289 - val_loss: 0.0970 - lr: 1.0000e-04\n",
            "Epoch 398/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0282\n",
            "Epoch 398: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.0936 - lr: 1.0000e-04\n",
            "Epoch 399/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0195\n",
            "Epoch 399: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0270 - val_loss: 0.0921 - lr: 1.0000e-04\n",
            "Epoch 400/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0273\n",
            "Epoch 400: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0271 - val_loss: 0.0919 - lr: 1.0000e-04\n",
            "Epoch 401/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0232\n",
            "Epoch 401: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0267 - val_loss: 0.0918 - lr: 1.0000e-05\n",
            "Epoch 402/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 402: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0267 - val_loss: 0.0917 - lr: 1.0000e-05\n",
            "Epoch 403/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0338\n",
            "Epoch 403: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0267 - val_loss: 0.0917 - lr: 1.0000e-05\n",
            "Epoch 404/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0304\n",
            "Epoch 404: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0267 - val_loss: 0.0915 - lr: 1.0000e-05\n",
            "Epoch 405/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0171\n",
            "Epoch 405: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0266 - val_loss: 0.0916 - lr: 1.0000e-05\n",
            "Epoch 406/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0367\n",
            "Epoch 406: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0266 - val_loss: 0.0917 - lr: 1.0000e-05\n",
            "Epoch 407/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0222\n",
            "Epoch 407: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0265 - val_loss: 0.0919 - lr: 1.0000e-05\n",
            "Epoch 408/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0343\n",
            "Epoch 408: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0264 - val_loss: 0.0923 - lr: 1.0000e-05\n",
            "Epoch 409/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0164\n",
            "Epoch 409: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0264 - val_loss: 0.0925 - lr: 1.0000e-05\n",
            "Epoch 410/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0261\n",
            "Epoch 410: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0264 - val_loss: 0.0926 - lr: 1.0000e-05\n",
            "Epoch 411/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0439\n",
            "Epoch 411: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0265 - val_loss: 0.0928 - lr: 1.0000e-05\n",
            "Epoch 412/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0324\n",
            "Epoch 412: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0264 - val_loss: 0.0929 - lr: 1.0000e-05\n",
            "Epoch 413/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0254\n",
            "Epoch 413: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0265 - val_loss: 0.0932 - lr: 1.0000e-05\n",
            "Epoch 414/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0208\n",
            "Epoch 414: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0265 - val_loss: 0.0934 - lr: 1.0000e-05\n",
            "Epoch 415/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0202\n",
            "Epoch 415: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0265 - val_loss: 0.0933 - lr: 1.0000e-05\n",
            "Epoch 416/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0237\n",
            "Epoch 416: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0265 - val_loss: 0.0931 - lr: 1.0000e-05\n",
            "Epoch 417/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0311\n",
            "Epoch 417: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0265 - val_loss: 0.0928 - lr: 1.0000e-05\n",
            "Epoch 418/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0335\n",
            "Epoch 418: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0264 - val_loss: 0.0925 - lr: 1.0000e-05\n",
            "Epoch 419/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0380\n",
            "Epoch 419: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0264 - val_loss: 0.0923 - lr: 1.0000e-05\n",
            "Epoch 420/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0235\n",
            "Epoch 420: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0264 - val_loss: 0.0921 - lr: 1.0000e-05\n",
            "Epoch 421/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0230\n",
            "Epoch 421: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0264 - val_loss: 0.0924 - lr: 1.0000e-05\n",
            "Epoch 422/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0295\n",
            "Epoch 422: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0265 - val_loss: 0.0925 - lr: 1.0000e-05\n",
            "Epoch 423/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0368\n",
            "Epoch 423: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0264 - val_loss: 0.0924 - lr: 1.0000e-05\n",
            "Epoch 424/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0189\n",
            "Epoch 424: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0264 - val_loss: 0.0924 - lr: 1.0000e-05\n",
            "Epoch 425/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0202\n",
            "Epoch 425: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0264 - val_loss: 0.0924 - lr: 1.0000e-05\n",
            "Epoch 426/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0185\n",
            "Epoch 426: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0264 - val_loss: 0.0925 - lr: 1.0000e-05\n",
            "Epoch 427/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0224\n",
            "Epoch 427: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0264 - val_loss: 0.0926 - lr: 1.0000e-05\n",
            "Epoch 428/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0227\n",
            "Epoch 428: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0264 - val_loss: 0.0927 - lr: 1.0000e-05\n",
            "Epoch 429/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0258\n",
            "Epoch 429: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0264 - val_loss: 0.0926 - lr: 1.0000e-05\n",
            "Epoch 430/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0364\n",
            "Epoch 430: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0925 - lr: 1.0000e-05\n",
            "Epoch 431/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0256\n",
            "Epoch 431: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0264 - val_loss: 0.0922 - lr: 1.0000e-05\n",
            "Epoch 432/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0377\n",
            "Epoch 432: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0921 - lr: 1.0000e-05\n",
            "Epoch 433/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0277\n",
            "Epoch 433: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0264 - val_loss: 0.0921 - lr: 1.0000e-05\n",
            "Epoch 434/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0279\n",
            "Epoch 434: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0264 - val_loss: 0.0921 - lr: 1.0000e-05\n",
            "Epoch 435/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0137\n",
            "Epoch 435: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0264 - val_loss: 0.0920 - lr: 1.0000e-05\n",
            "Epoch 436/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0219\n",
            "Epoch 436: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0916 - lr: 1.0000e-05\n",
            "Epoch 437/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0235\n",
            "Epoch 437: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0264 - val_loss: 0.0915 - lr: 1.0000e-05\n",
            "Epoch 438/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0311\n",
            "Epoch 438: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0264 - val_loss: 0.0917 - lr: 1.0000e-05\n",
            "Epoch 439/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0320\n",
            "Epoch 439: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0264 - val_loss: 0.0918 - lr: 1.0000e-05\n",
            "Epoch 440/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0330\n",
            "Epoch 440: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0264 - val_loss: 0.0918 - lr: 1.0000e-05\n",
            "Epoch 441/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0104\n",
            "Epoch 441: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0264 - val_loss: 0.0915 - lr: 1.0000e-05\n",
            "Epoch 442/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0356\n",
            "Epoch 442: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0264 - val_loss: 0.0913 - lr: 1.0000e-05\n",
            "Epoch 443/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0221\n",
            "Epoch 443: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0264 - val_loss: 0.0913 - lr: 1.0000e-05\n",
            "Epoch 444/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0304\n",
            "Epoch 444: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0264 - val_loss: 0.0913 - lr: 1.0000e-05\n",
            "Epoch 445/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0276\n",
            "Epoch 445: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0264 - val_loss: 0.0915 - lr: 1.0000e-05\n",
            "Epoch 446/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0249\n",
            "Epoch 446: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0916 - lr: 1.0000e-05\n",
            "Epoch 447/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0211\n",
            "Epoch 447: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0917 - lr: 1.0000e-05\n",
            "Epoch 448/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0165\n",
            "Epoch 448: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0918 - lr: 1.0000e-05\n",
            "Epoch 449/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0292\n",
            "Epoch 449: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0922 - lr: 1.0000e-05\n",
            "Epoch 450/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0399\n",
            "Epoch 450: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0263 - val_loss: 0.0925 - lr: 1.0000e-05\n",
            "Epoch 451/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0195\n",
            "Epoch 451: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0925 - lr: 1.0000e-06\n",
            "Epoch 452/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0299\n",
            "Epoch 452: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0925 - lr: 1.0000e-06\n",
            "Epoch 453/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0361\n",
            "Epoch 453: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 454/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0184\n",
            "Epoch 454: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 455/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0375\n",
            "Epoch 455: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0263 - val_loss: 0.0923 - lr: 1.0000e-06\n",
            "Epoch 456/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0257\n",
            "Epoch 456: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0263 - val_loss: 0.0922 - lr: 1.0000e-06\n",
            "Epoch 457/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0204\n",
            "Epoch 457: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0922 - lr: 1.0000e-06\n",
            "Epoch 458/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0301\n",
            "Epoch 458: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0263 - val_loss: 0.0921 - lr: 1.0000e-06\n",
            "Epoch 459/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0210\n",
            "Epoch 459: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0263 - val_loss: 0.0921 - lr: 1.0000e-06\n",
            "Epoch 460/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0274\n",
            "Epoch 460: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0263 - val_loss: 0.0921 - lr: 1.0000e-06\n",
            "Epoch 461/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0196\n",
            "Epoch 461: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0922 - lr: 1.0000e-06\n",
            "Epoch 462/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0232\n",
            "Epoch 462: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0263 - val_loss: 0.0922 - lr: 1.0000e-06\n",
            "Epoch 463/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0177\n",
            "Epoch 463: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0263 - val_loss: 0.0922 - lr: 1.0000e-06\n",
            "Epoch 464/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0297\n",
            "Epoch 464: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0923 - lr: 1.0000e-06\n",
            "Epoch 465/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0268\n",
            "Epoch 465: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0923 - lr: 1.0000e-06\n",
            "Epoch 466/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0274\n",
            "Epoch 466: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 467/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0293\n",
            "Epoch 467: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 468/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0377\n",
            "Epoch 468: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 469/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0356\n",
            "Epoch 469: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 470/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0386\n",
            "Epoch 470: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 471/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0254\n",
            "Epoch 471: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 472/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0131\n",
            "Epoch 472: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 473/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0176\n",
            "Epoch 473: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0923 - lr: 1.0000e-06\n",
            "Epoch 474/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0281\n",
            "Epoch 474: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0923 - lr: 1.0000e-06\n",
            "Epoch 475/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0318\n",
            "Epoch 475: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0263 - val_loss: 0.0923 - lr: 1.0000e-06\n",
            "Epoch 476/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0320\n",
            "Epoch 476: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0263 - val_loss: 0.0923 - lr: 1.0000e-06\n",
            "Epoch 477/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0182\n",
            "Epoch 477: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 478/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0342\n",
            "Epoch 478: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 479/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0200\n",
            "Epoch 479: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 480/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0201\n",
            "Epoch 480: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 481/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0338\n",
            "Epoch 481: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 482/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 482: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 483/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0294\n",
            "Epoch 483: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 484/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0161\n",
            "Epoch 484: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 485/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0412\n",
            "Epoch 485: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 486/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 486: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 487/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 487: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 488/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0222\n",
            "Epoch 488: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0263 - val_loss: 0.0924 - lr: 1.0000e-06\n",
            "Epoch 489/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0308\n",
            "Epoch 489: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0263 - val_loss: 0.0923 - lr: 1.0000e-06\n",
            "Epoch 490/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 490: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0263 - val_loss: 0.0922 - lr: 1.0000e-06\n",
            "Epoch 491/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0241\n",
            "Epoch 491: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0263 - val_loss: 0.0922 - lr: 1.0000e-06\n",
            "Epoch 492/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0302\n",
            "Epoch 492: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0263 - val_loss: 0.0921 - lr: 1.0000e-06\n",
            "Epoch 493/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0232\n",
            "Epoch 493: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0263 - val_loss: 0.0921 - lr: 1.0000e-06\n",
            "Epoch 494/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0399\n",
            "Epoch 494: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0263 - val_loss: 0.0921 - lr: 1.0000e-06\n",
            "Epoch 495/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0177\n",
            "Epoch 495: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0263 - val_loss: 0.0921 - lr: 1.0000e-06\n",
            "Epoch 496/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0221\n",
            "Epoch 496: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0263 - val_loss: 0.0921 - lr: 1.0000e-06\n",
            "Epoch 497/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0292\n",
            "Epoch 497: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0263 - val_loss: 0.0921 - lr: 1.0000e-06\n",
            "Epoch 498/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0317\n",
            "Epoch 498: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0263 - val_loss: 0.0920 - lr: 1.0000e-06\n",
            "Epoch 499/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0222\n",
            "Epoch 499: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0263 - val_loss: 0.0920 - lr: 1.0000e-06\n",
            "Epoch 500/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0201\n",
            "Epoch 500: val_loss did not improve from 0.08856\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0263 - val_loss: 0.0919 - lr: 1.0000e-06\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0919\n",
            "Test Loss: 0.09189844131469727\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0886\n",
            "Test Loss (MSE) with Best Model: 0.08856429904699326\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "model_1 = Sequential([\n",
        "    Dense(4, activation='selu', kernel_regularizer=regularizers.l2(0.00001), input_shape=(4,)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(8, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(16, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(32, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(64, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(128, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(1)  # Single output neuron for regression\n",
        "])\n",
        "\n",
        "def learning_rate_schedule(epoch, initial_lr=0.001):\n",
        "    \"\"\"\n",
        "    Custom learning rate schedule. Adjust the learning rate based on the current epoch.\n",
        "    You can customize this function to fit your needs.\n",
        "    \"\"\"\n",
        "    if epoch < 300 or epoch == 300:\n",
        "        return initial_lr  # Keep the initial learning rate for the first 50 epochs\n",
        "    elif epoch > 300 and epoch%50 == 0:\n",
        "        initial_lr = initial_lr * 0.1  # Reduce the learning rate by a factor of 10 after epoch 50\n",
        "        return initial_lr\n",
        "    else:\n",
        "        return initial_lr\n",
        "\n",
        "# Create the Adam optimizer with the initial learning rate\n",
        "initial_learning_rate = 0.001\n",
        "adam_optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(optimizer=adam_optimizer, loss='mean_squared_error')\n",
        "\n",
        "checkpoint_1 = ModelCheckpoint('isap_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "model_1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[checkpoint_1, lr_scheduler])\n",
        "\n",
        "loss_1 = model_1.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss_1)\n",
        "\n",
        "predictions_1 = model_1.predict(X_test)\n",
        "\n",
        "errors_1 = np.abs(predictions_1 - y_test)\n",
        "\n",
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model('isap_model.h5')\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "test_loss = best_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss (MSE) with Best Model: {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCHDaD3r9GjK"
      },
      "source": [
        "Access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i6Bae3m9MJq",
        "outputId": "ae9bf4e8-50ec-4606-b393-29df5d92e2b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/4 [======>.......................] - ETA: 3s - loss: 26.9493\n",
            "Epoch 1: val_loss improved from inf to 14.22326, saving model to access_model.h5\n",
            "4/4 [==============================] - 2s 95ms/step - loss: 24.2804 - val_loss: 14.2233 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 15.5423\n",
            "Epoch 2: val_loss improved from 14.22326 to 5.73274, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 12.5050 - val_loss: 5.7327 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 5.8948\n",
            "Epoch 3: val_loss improved from 5.73274 to 2.82369, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 4.2721 - val_loss: 2.8237 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 2.2030\n",
            "Epoch 4: val_loss did not improve from 2.82369\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 3.2690 - val_loss: 4.6453 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 4.1958\n",
            "Epoch 5: val_loss did not improve from 2.82369\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 3.5227 - val_loss: 2.9869 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 2.1978\n",
            "Epoch 6: val_loss improved from 2.82369 to 2.00052, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 1.9743 - val_loss: 2.0005 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 2.2165\n",
            "Epoch 7: val_loss did not improve from 2.00052\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 1.8514 - val_loss: 2.3534 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.8189\n",
            "Epoch 8: val_loss improved from 2.00052 to 1.75049, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 1.8597 - val_loss: 1.7505 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.7789\n",
            "Epoch 9: val_loss did not improve from 1.75049\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 1.4117 - val_loss: 1.8980 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.6025\n",
            "Epoch 10: val_loss did not improve from 1.75049\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 1.3484 - val_loss: 1.8882 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.9874\n",
            "Epoch 11: val_loss did not improve from 1.75049\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 1.6185 - val_loss: 1.8101 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.3430\n",
            "Epoch 12: val_loss improved from 1.75049 to 1.39640, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 1.2544 - val_loss: 1.3964 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.3942\n",
            "Epoch 13: val_loss improved from 1.39640 to 1.29950, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.1284 - val_loss: 1.2995 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.0504\n",
            "Epoch 14: val_loss improved from 1.29950 to 0.99908, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.9759 - val_loss: 0.9991 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.8884\n",
            "Epoch 15: val_loss improved from 0.99908 to 0.94685, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.8274 - val_loss: 0.9468 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6482\n",
            "Epoch 16: val_loss improved from 0.94685 to 0.91142, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.7209 - val_loss: 0.9114 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6889\n",
            "Epoch 17: val_loss improved from 0.91142 to 0.90791, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.7016 - val_loss: 0.9079 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6412\n",
            "Epoch 18: val_loss did not improve from 0.90791\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6593 - val_loss: 0.9301 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5912\n",
            "Epoch 19: val_loss improved from 0.90791 to 0.88382, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6297 - val_loss: 0.8838 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.8094\n",
            "Epoch 20: val_loss did not improve from 0.88382\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6647 - val_loss: 0.9396 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6659\n",
            "Epoch 21: val_loss improved from 0.88382 to 0.76346, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.6034 - val_loss: 0.7635 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5226\n",
            "Epoch 22: val_loss did not improve from 0.76346\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5895 - val_loss: 0.8804 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6807\n",
            "Epoch 23: val_loss improved from 0.76346 to 0.72509, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5440 - val_loss: 0.7251 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6144\n",
            "Epoch 24: val_loss improved from 0.72509 to 0.71447, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4995 - val_loss: 0.7145 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5134\n",
            "Epoch 25: val_loss improved from 0.71447 to 0.66955, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4509 - val_loss: 0.6696 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4681\n",
            "Epoch 26: val_loss improved from 0.66955 to 0.62828, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4536 - val_loss: 0.6283 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3797\n",
            "Epoch 27: val_loss did not improve from 0.62828\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.4185 - val_loss: 0.6409 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4466\n",
            "Epoch 28: val_loss improved from 0.62828 to 0.59538, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4248 - val_loss: 0.5954 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3443\n",
            "Epoch 29: val_loss did not improve from 0.59538\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3976 - val_loss: 0.6502 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4393\n",
            "Epoch 30: val_loss improved from 0.59538 to 0.56225, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3754 - val_loss: 0.5623 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3228\n",
            "Epoch 31: val_loss did not improve from 0.56225\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3762 - val_loss: 0.6456 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4056\n",
            "Epoch 32: val_loss improved from 0.56225 to 0.51740, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3642 - val_loss: 0.5174 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3547\n",
            "Epoch 33: val_loss improved from 0.51740 to 0.49118, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3288 - val_loss: 0.4912 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2663\n",
            "Epoch 34: val_loss did not improve from 0.49118\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3036 - val_loss: 0.5168 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3029\n",
            "Epoch 35: val_loss improved from 0.49118 to 0.46775, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2903 - val_loss: 0.4678 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2890\n",
            "Epoch 36: val_loss improved from 0.46775 to 0.45979, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.2814 - val_loss: 0.4598 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2832\n",
            "Epoch 37: val_loss did not improve from 0.45979\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2736 - val_loss: 0.7560 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4494\n",
            "Epoch 38: val_loss did not improve from 0.45979\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3838 - val_loss: 0.4641 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2951\n",
            "Epoch 39: val_loss improved from 0.45979 to 0.44033, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3156 - val_loss: 0.4403 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2909\n",
            "Epoch 40: val_loss did not improve from 0.44033\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2704 - val_loss: 0.5335 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3060\n",
            "Epoch 41: val_loss improved from 0.44033 to 0.41361, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2927 - val_loss: 0.4136 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2318\n",
            "Epoch 42: val_loss improved from 0.41361 to 0.40000, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2492 - val_loss: 0.4000 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2495\n",
            "Epoch 43: val_loss did not improve from 0.40000\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2390 - val_loss: 0.4013 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2167\n",
            "Epoch 44: val_loss improved from 0.40000 to 0.38131, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2216 - val_loss: 0.3813 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2555\n",
            "Epoch 45: val_loss improved from 0.38131 to 0.34941, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2106 - val_loss: 0.3494 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1449\n",
            "Epoch 46: val_loss did not improve from 0.34941\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2021 - val_loss: 0.3542 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2340\n",
            "Epoch 47: val_loss improved from 0.34941 to 0.33391, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2004 - val_loss: 0.3339 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1389\n",
            "Epoch 48: val_loss did not improve from 0.33391\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1972 - val_loss: 0.3410 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1715\n",
            "Epoch 49: val_loss did not improve from 0.33391\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1804 - val_loss: 0.3390 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1813\n",
            "Epoch 50: val_loss did not improve from 0.33391\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1798 - val_loss: 0.3713 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2101\n",
            "Epoch 51: val_loss did not improve from 0.33391\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2251 - val_loss: 0.4287 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2883\n",
            "Epoch 52: val_loss did not improve from 0.33391\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2691 - val_loss: 0.4122 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2685\n",
            "Epoch 53: val_loss did not improve from 0.33391\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2432 - val_loss: 0.6071 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4418\n",
            "Epoch 54: val_loss improved from 0.33391 to 0.29549, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.3332 - val_loss: 0.2955 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1696\n",
            "Epoch 55: val_loss did not improve from 0.29549\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2952 - val_loss: 0.4131 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3471\n",
            "Epoch 56: val_loss did not improve from 0.29549\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2648 - val_loss: 0.3247 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1706\n",
            "Epoch 57: val_loss did not improve from 0.29549\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2216 - val_loss: 0.4092 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2415\n",
            "Epoch 58: val_loss did not improve from 0.29549\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2168 - val_loss: 0.4375 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2731\n",
            "Epoch 59: val_loss improved from 0.29549 to 0.29490, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2630 - val_loss: 0.2949 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1923\n",
            "Epoch 60: val_loss did not improve from 0.29490\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2151 - val_loss: 0.3472 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2369\n",
            "Epoch 61: val_loss improved from 0.29490 to 0.28924, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2039 - val_loss: 0.2892 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1279\n",
            "Epoch 62: val_loss did not improve from 0.28924\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1867 - val_loss: 0.4940 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3352\n",
            "Epoch 63: val_loss did not improve from 0.28924\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2685 - val_loss: 0.3879 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2950\n",
            "Epoch 64: val_loss did not improve from 0.28924\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2583 - val_loss: 0.3591 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2382\n",
            "Epoch 65: val_loss did not improve from 0.28924\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2454 - val_loss: 0.5355 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3402\n",
            "Epoch 66: val_loss improved from 0.28924 to 0.27217, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2778 - val_loss: 0.2722 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1245\n",
            "Epoch 67: val_loss did not improve from 0.27217\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1915 - val_loss: 0.3285 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1996\n",
            "Epoch 68: val_loss did not improve from 0.27217\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1887 - val_loss: 0.3028 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1639\n",
            "Epoch 69: val_loss did not improve from 0.27217\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1705 - val_loss: 0.3584 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1895\n",
            "Epoch 70: val_loss did not improve from 0.27217\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1844 - val_loss: 0.3483 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2477\n",
            "Epoch 71: val_loss improved from 0.27217 to 0.24696, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2211 - val_loss: 0.2470 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1056\n",
            "Epoch 72: val_loss did not improve from 0.24696\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1528 - val_loss: 0.3775 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2262\n",
            "Epoch 73: val_loss did not improve from 0.24696\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1946 - val_loss: 0.2555 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1860\n",
            "Epoch 74: val_loss improved from 0.24696 to 0.24040, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1632 - val_loss: 0.2404 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1191\n",
            "Epoch 75: val_loss improved from 0.24040 to 0.23174, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1323 - val_loss: 0.2317 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0889\n",
            "Epoch 76: val_loss did not improve from 0.23174\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1193 - val_loss: 0.2814 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1718\n",
            "Epoch 77: val_loss improved from 0.23174 to 0.22797, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1515 - val_loss: 0.2280 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1258\n",
            "Epoch 78: val_loss did not improve from 0.22797\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1196 - val_loss: 0.2315 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1252\n",
            "Epoch 79: val_loss improved from 0.22797 to 0.22562, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1180 - val_loss: 0.2256 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1100\n",
            "Epoch 80: val_loss did not improve from 0.22562\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1202 - val_loss: 0.2447 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1321\n",
            "Epoch 81: val_loss did not improve from 0.22562\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1309 - val_loss: 0.2542 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1506\n",
            "Epoch 82: val_loss did not improve from 0.22562\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1390 - val_loss: 0.2718 - lr: 0.0010\n",
            "Epoch 83/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1343\n",
            "Epoch 83: val_loss improved from 0.22562 to 0.21019, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1318 - val_loss: 0.2102 - lr: 0.0010\n",
            "Epoch 84/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0953\n",
            "Epoch 84: val_loss did not improve from 0.21019\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1327 - val_loss: 0.2449 - lr: 0.0010\n",
            "Epoch 85/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1458\n",
            "Epoch 85: val_loss did not improve from 0.21019\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1327 - val_loss: 0.2155 - lr: 0.0010\n",
            "Epoch 86/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1063\n",
            "Epoch 86: val_loss did not improve from 0.21019\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1324 - val_loss: 0.2384 - lr: 0.0010\n",
            "Epoch 87/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1253\n",
            "Epoch 87: val_loss did not improve from 0.21019\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1311 - val_loss: 0.2989 - lr: 0.0010\n",
            "Epoch 88/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1424\n",
            "Epoch 88: val_loss improved from 0.21019 to 0.20242, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1615 - val_loss: 0.2024 - lr: 0.0010\n",
            "Epoch 89/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1158\n",
            "Epoch 89: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1531 - val_loss: 0.3527 - lr: 0.0010\n",
            "Epoch 90/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2435\n",
            "Epoch 90: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1918 - val_loss: 0.2144 - lr: 0.0010\n",
            "Epoch 91/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0907\n",
            "Epoch 91: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1936 - val_loss: 0.2596 - lr: 0.0010\n",
            "Epoch 92/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1719\n",
            "Epoch 92: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1650 - val_loss: 0.4797 - lr: 0.0010\n",
            "Epoch 93/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3959\n",
            "Epoch 93: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2848 - val_loss: 0.2578 - lr: 0.0010\n",
            "Epoch 94/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1097\n",
            "Epoch 94: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2776 - val_loss: 0.4476 - lr: 0.0010\n",
            "Epoch 95/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4086\n",
            "Epoch 95: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3576 - val_loss: 0.3033 - lr: 0.0010\n",
            "Epoch 96/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2177\n",
            "Epoch 96: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3099 - val_loss: 0.6534 - lr: 0.0010\n",
            "Epoch 97/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4529\n",
            "Epoch 97: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4207 - val_loss: 0.3188 - lr: 0.0010\n",
            "Epoch 98/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2187\n",
            "Epoch 98: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2516 - val_loss: 0.3963 - lr: 0.0010\n",
            "Epoch 99/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3431\n",
            "Epoch 99: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2308 - val_loss: 0.2804 - lr: 0.0010\n",
            "Epoch 100/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1551\n",
            "Epoch 100: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2058 - val_loss: 0.2965 - lr: 0.0010\n",
            "Epoch 101/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1991\n",
            "Epoch 101: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1490 - val_loss: 0.2266 - lr: 0.0010\n",
            "Epoch 102/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0978\n",
            "Epoch 102: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1292 - val_loss: 0.2519 - lr: 0.0010\n",
            "Epoch 103/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1661\n",
            "Epoch 103: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1424 - val_loss: 0.2270 - lr: 0.0010\n",
            "Epoch 104/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0958\n",
            "Epoch 104: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1564 - val_loss: 0.4224 - lr: 0.0010\n",
            "Epoch 105/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2531\n",
            "Epoch 105: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2290 - val_loss: 0.2716 - lr: 0.0010\n",
            "Epoch 106/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2052\n",
            "Epoch 106: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1970 - val_loss: 0.4732 - lr: 0.0010\n",
            "Epoch 107/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3936\n",
            "Epoch 107: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2585 - val_loss: 0.3411 - lr: 0.0010\n",
            "Epoch 108/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1765\n",
            "Epoch 108: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1991 - val_loss: 0.2442 - lr: 0.0010\n",
            "Epoch 109/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1250\n",
            "Epoch 109: val_loss did not improve from 0.20242\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1820 - val_loss: 0.2615 - lr: 0.0010\n",
            "Epoch 110/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1535\n",
            "Epoch 110: val_loss improved from 0.20242 to 0.19740, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1447 - val_loss: 0.1974 - lr: 0.0010\n",
            "Epoch 111/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1196\n",
            "Epoch 111: val_loss did not improve from 0.19740\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1412 - val_loss: 0.3438 - lr: 0.0010\n",
            "Epoch 112/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2131\n",
            "Epoch 112: val_loss did not improve from 0.19740\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1371 - val_loss: 0.2097 - lr: 0.0010\n",
            "Epoch 113/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1240\n",
            "Epoch 113: val_loss did not improve from 0.19740\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1099 - val_loss: 0.2053 - lr: 0.0010\n",
            "Epoch 114/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1220\n",
            "Epoch 114: val_loss did not improve from 0.19740\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1214 - val_loss: 0.2327 - lr: 0.0010\n",
            "Epoch 115/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1298\n",
            "Epoch 115: val_loss improved from 0.19740 to 0.18568, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1246 - val_loss: 0.1857 - lr: 0.0010\n",
            "Epoch 116/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0894\n",
            "Epoch 116: val_loss did not improve from 0.18568\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0957 - val_loss: 0.2346 - lr: 0.0010\n",
            "Epoch 117/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1388\n",
            "Epoch 117: val_loss improved from 0.18568 to 0.17845, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1171 - val_loss: 0.1784 - lr: 0.0010\n",
            "Epoch 118/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0855\n",
            "Epoch 118: val_loss did not improve from 0.17845\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0984 - val_loss: 0.2143 - lr: 0.0010\n",
            "Epoch 119/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0957\n",
            "Epoch 119: val_loss did not improve from 0.17845\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0944 - val_loss: 0.2175 - lr: 0.0010\n",
            "Epoch 120/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1297\n",
            "Epoch 120: val_loss did not improve from 0.17845\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1129 - val_loss: 0.1926 - lr: 0.0010\n",
            "Epoch 121/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1026\n",
            "Epoch 121: val_loss did not improve from 0.17845\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0970 - val_loss: 0.1958 - lr: 0.0010\n",
            "Epoch 122/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0703\n",
            "Epoch 122: val_loss improved from 0.17845 to 0.17072, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0925 - val_loss: 0.1707 - lr: 0.0010\n",
            "Epoch 123/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1084\n",
            "Epoch 123: val_loss did not improve from 0.17072\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0903 - val_loss: 0.1895 - lr: 0.0010\n",
            "Epoch 124/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0931\n",
            "Epoch 124: val_loss improved from 0.17072 to 0.17014, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0936 - val_loss: 0.1701 - lr: 0.0010\n",
            "Epoch 125/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0695\n",
            "Epoch 125: val_loss did not improve from 0.17014\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0784 - val_loss: 0.1754 - lr: 0.0010\n",
            "Epoch 126/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0904\n",
            "Epoch 126: val_loss did not improve from 0.17014\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0762 - val_loss: 0.1840 - lr: 0.0010\n",
            "Epoch 127/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0856\n",
            "Epoch 127: val_loss did not improve from 0.17014\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0810 - val_loss: 0.1782 - lr: 0.0010\n",
            "Epoch 128/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0866\n",
            "Epoch 128: val_loss did not improve from 0.17014\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0782 - val_loss: 0.1762 - lr: 0.0010\n",
            "Epoch 129/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0783\n",
            "Epoch 129: val_loss did not improve from 0.17014\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0816 - val_loss: 0.1966 - lr: 0.0010\n",
            "Epoch 130/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0849\n",
            "Epoch 130: val_loss improved from 0.17014 to 0.16090, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0922 - val_loss: 0.1609 - lr: 0.0010\n",
            "Epoch 131/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0731\n",
            "Epoch 131: val_loss improved from 0.16090 to 0.15820, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0839 - val_loss: 0.1582 - lr: 0.0010\n",
            "Epoch 132/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0767\n",
            "Epoch 132: val_loss did not improve from 0.15820\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0703 - val_loss: 0.1794 - lr: 0.0010\n",
            "Epoch 133/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0934\n",
            "Epoch 133: val_loss did not improve from 0.15820\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0795 - val_loss: 0.1762 - lr: 0.0010\n",
            "Epoch 134/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0950\n",
            "Epoch 134: val_loss improved from 0.15820 to 0.15383, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0738 - val_loss: 0.1538 - lr: 0.0010\n",
            "Epoch 135/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0646\n",
            "Epoch 135: val_loss did not improve from 0.15383\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0651 - val_loss: 0.1545 - lr: 0.0010\n",
            "Epoch 136/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0648\n",
            "Epoch 136: val_loss did not improve from 0.15383\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0644 - val_loss: 0.1567 - lr: 0.0010\n",
            "Epoch 137/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0604\n",
            "Epoch 137: val_loss did not improve from 0.15383\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0771 - val_loss: 0.1691 - lr: 0.0010\n",
            "Epoch 138/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0722\n",
            "Epoch 138: val_loss improved from 0.15383 to 0.14991, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0843 - val_loss: 0.1499 - lr: 0.0010\n",
            "Epoch 139/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0379\n",
            "Epoch 139: val_loss did not improve from 0.14991\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0697 - val_loss: 0.1551 - lr: 0.0010\n",
            "Epoch 140/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0758\n",
            "Epoch 140: val_loss did not improve from 0.14991\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0763 - val_loss: 0.1834 - lr: 0.0010\n",
            "Epoch 141/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1056\n",
            "Epoch 141: val_loss did not improve from 0.14991\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1085 - val_loss: 0.1531 - lr: 0.0010\n",
            "Epoch 142/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0804\n",
            "Epoch 142: val_loss did not improve from 0.14991\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1111 - val_loss: 0.1690 - lr: 0.0010\n",
            "Epoch 143/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0735\n",
            "Epoch 143: val_loss did not improve from 0.14991\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0928 - val_loss: 0.2168 - lr: 0.0010\n",
            "Epoch 144/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1221\n",
            "Epoch 144: val_loss did not improve from 0.14991\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1088 - val_loss: 0.2148 - lr: 0.0010\n",
            "Epoch 145/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1162\n",
            "Epoch 145: val_loss did not improve from 0.14991\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1071 - val_loss: 0.1959 - lr: 0.0010\n",
            "Epoch 146/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0804\n",
            "Epoch 146: val_loss did not improve from 0.14991\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0941 - val_loss: 0.2232 - lr: 0.0010\n",
            "Epoch 147/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1157\n",
            "Epoch 147: val_loss did not improve from 0.14991\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1108 - val_loss: 0.2111 - lr: 0.0010\n",
            "Epoch 148/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1568\n",
            "Epoch 148: val_loss did not improve from 0.14991\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1083 - val_loss: 0.1549 - lr: 0.0010\n",
            "Epoch 149/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0741\n",
            "Epoch 149: val_loss did not improve from 0.14991\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0759 - val_loss: 0.1738 - lr: 0.0010\n",
            "Epoch 150/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0647\n",
            "Epoch 150: val_loss improved from 0.14991 to 0.14793, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0846 - val_loss: 0.1479 - lr: 0.0010\n",
            "Epoch 151/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0604\n",
            "Epoch 151: val_loss did not improve from 0.14793\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0823 - val_loss: 0.1966 - lr: 0.0010\n",
            "Epoch 152/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1015\n",
            "Epoch 152: val_loss did not improve from 0.14793\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0925 - val_loss: 0.2142 - lr: 0.0010\n",
            "Epoch 153/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1068\n",
            "Epoch 153: val_loss did not improve from 0.14793\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1354 - val_loss: 0.2092 - lr: 0.0010\n",
            "Epoch 154/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1111\n",
            "Epoch 154: val_loss did not improve from 0.14793\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1201 - val_loss: 0.1579 - lr: 0.0010\n",
            "Epoch 155/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0667\n",
            "Epoch 155: val_loss did not improve from 0.14793\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0828 - val_loss: 0.1748 - lr: 0.0010\n",
            "Epoch 156/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1030\n",
            "Epoch 156: val_loss did not improve from 0.14793\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0806 - val_loss: 0.1699 - lr: 0.0010\n",
            "Epoch 157/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0553\n",
            "Epoch 157: val_loss did not improve from 0.14793\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0665 - val_loss: 0.2115 - lr: 0.0010\n",
            "Epoch 158/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1134\n",
            "Epoch 158: val_loss did not improve from 0.14793\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0867 - val_loss: 0.1695 - lr: 0.0010\n",
            "Epoch 159/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0685\n",
            "Epoch 159: val_loss did not improve from 0.14793\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0743 - val_loss: 0.1683 - lr: 0.0010\n",
            "Epoch 160/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1141\n",
            "Epoch 160: val_loss did not improve from 0.14793\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0957 - val_loss: 0.1679 - lr: 0.0010\n",
            "Epoch 161/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0815\n",
            "Epoch 161: val_loss did not improve from 0.14793\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0798 - val_loss: 0.1519 - lr: 0.0010\n",
            "Epoch 162/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0644\n",
            "Epoch 162: val_loss improved from 0.14793 to 0.13678, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0645 - val_loss: 0.1368 - lr: 0.0010\n",
            "Epoch 163/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0637\n",
            "Epoch 163: val_loss did not improve from 0.13678\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0703 - val_loss: 0.1542 - lr: 0.0010\n",
            "Epoch 164/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0667\n",
            "Epoch 164: val_loss improved from 0.13678 to 0.13183, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0714 - val_loss: 0.1318 - lr: 0.0010\n",
            "Epoch 165/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0617\n",
            "Epoch 165: val_loss did not improve from 0.13183\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0563 - val_loss: 0.1404 - lr: 0.0010\n",
            "Epoch 166/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0536\n",
            "Epoch 166: val_loss did not improve from 0.13183\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0632 - val_loss: 0.1411 - lr: 0.0010\n",
            "Epoch 167/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0653\n",
            "Epoch 167: val_loss did not improve from 0.13183\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0676 - val_loss: 0.1719 - lr: 0.0010\n",
            "Epoch 168/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0810\n",
            "Epoch 168: val_loss improved from 0.13183 to 0.12960, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0765 - val_loss: 0.1296 - lr: 0.0010\n",
            "Epoch 169/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0658\n",
            "Epoch 169: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0687 - val_loss: 0.1356 - lr: 0.0010\n",
            "Epoch 170/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0572\n",
            "Epoch 170: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0649 - val_loss: 0.1533 - lr: 0.0010\n",
            "Epoch 171/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0848\n",
            "Epoch 171: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0666 - val_loss: 0.1706 - lr: 0.0010\n",
            "Epoch 172/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0752\n",
            "Epoch 172: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0824 - val_loss: 0.1419 - lr: 0.0010\n",
            "Epoch 173/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0546\n",
            "Epoch 173: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0703 - val_loss: 0.1711 - lr: 0.0010\n",
            "Epoch 174/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0807\n",
            "Epoch 174: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0784 - val_loss: 0.2222 - lr: 0.0010\n",
            "Epoch 175/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1172\n",
            "Epoch 175: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0949 - val_loss: 0.1653 - lr: 0.0010\n",
            "Epoch 176/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0635\n",
            "Epoch 176: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0728 - val_loss: 0.1630 - lr: 0.0010\n",
            "Epoch 177/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0706\n",
            "Epoch 177: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0863 - val_loss: 0.1421 - lr: 0.0010\n",
            "Epoch 178/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0610\n",
            "Epoch 178: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0746 - val_loss: 0.1749 - lr: 0.0010\n",
            "Epoch 179/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0893\n",
            "Epoch 179: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0850 - val_loss: 0.1533 - lr: 0.0010\n",
            "Epoch 180/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0850\n",
            "Epoch 180: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0784 - val_loss: 0.1623 - lr: 0.0010\n",
            "Epoch 181/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0639\n",
            "Epoch 181: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0668 - val_loss: 0.1628 - lr: 0.0010\n",
            "Epoch 182/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0826\n",
            "Epoch 182: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0749 - val_loss: 0.1455 - lr: 0.0010\n",
            "Epoch 183/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0697\n",
            "Epoch 183: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0553 - val_loss: 0.1383 - lr: 0.0010\n",
            "Epoch 184/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0499\n",
            "Epoch 184: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0595 - val_loss: 0.1462 - lr: 0.0010\n",
            "Epoch 185/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0651\n",
            "Epoch 185: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1046 - val_loss: 0.2119 - lr: 0.0010\n",
            "Epoch 186/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1268\n",
            "Epoch 186: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1256 - val_loss: 0.1667 - lr: 0.0010\n",
            "Epoch 187/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1070\n",
            "Epoch 187: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1073 - val_loss: 0.1695 - lr: 0.0010\n",
            "Epoch 188/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0804\n",
            "Epoch 188: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0976 - val_loss: 0.2086 - lr: 0.0010\n",
            "Epoch 189/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1264\n",
            "Epoch 189: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0884 - val_loss: 0.1500 - lr: 0.0010\n",
            "Epoch 190/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0429\n",
            "Epoch 190: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0592 - val_loss: 0.1506 - lr: 0.0010\n",
            "Epoch 191/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0820\n",
            "Epoch 191: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0674 - val_loss: 0.1370 - lr: 0.0010\n",
            "Epoch 192/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0581\n",
            "Epoch 192: val_loss did not improve from 0.12960\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0548 - val_loss: 0.1374 - lr: 0.0010\n",
            "Epoch 193/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0704\n",
            "Epoch 193: val_loss improved from 0.12960 to 0.12861, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0551 - val_loss: 0.1286 - lr: 0.0010\n",
            "Epoch 194/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0386\n",
            "Epoch 194: val_loss did not improve from 0.12861\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0472 - val_loss: 0.1314 - lr: 0.0010\n",
            "Epoch 195/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0363\n",
            "Epoch 195: val_loss did not improve from 0.12861\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0451 - val_loss: 0.1436 - lr: 0.0010\n",
            "Epoch 196/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0680\n",
            "Epoch 196: val_loss did not improve from 0.12861\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0529 - val_loss: 0.1351 - lr: 0.0010\n",
            "Epoch 197/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0530\n",
            "Epoch 197: val_loss did not improve from 0.12861\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0549 - val_loss: 0.1434 - lr: 0.0010\n",
            "Epoch 198/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0568\n",
            "Epoch 198: val_loss improved from 0.12861 to 0.12391, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0552 - val_loss: 0.1239 - lr: 0.0010\n",
            "Epoch 199/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0569\n",
            "Epoch 199: val_loss did not improve from 0.12391\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0495 - val_loss: 0.1524 - lr: 0.0010\n",
            "Epoch 200/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0635\n",
            "Epoch 200: val_loss did not improve from 0.12391\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0677 - val_loss: 0.1279 - lr: 0.0010\n",
            "Epoch 201/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0415\n",
            "Epoch 201: val_loss did not improve from 0.12391\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0508 - val_loss: 0.1320 - lr: 0.0010\n",
            "Epoch 202/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0480\n",
            "Epoch 202: val_loss did not improve from 0.12391\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0437 - val_loss: 0.1394 - lr: 0.0010\n",
            "Epoch 203/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0573\n",
            "Epoch 203: val_loss improved from 0.12391 to 0.12137, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0519 - val_loss: 0.1214 - lr: 0.0010\n",
            "Epoch 204/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0446\n",
            "Epoch 204: val_loss did not improve from 0.12137\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0417 - val_loss: 0.1251 - lr: 0.0010\n",
            "Epoch 205/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0419\n",
            "Epoch 205: val_loss did not improve from 0.12137\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0512 - val_loss: 0.1262 - lr: 0.0010\n",
            "Epoch 206/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0571\n",
            "Epoch 206: val_loss improved from 0.12137 to 0.11561, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0475 - val_loss: 0.1156 - lr: 0.0010\n",
            "Epoch 207/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0456\n",
            "Epoch 207: val_loss improved from 0.11561 to 0.11491, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0465 - val_loss: 0.1149 - lr: 0.0010\n",
            "Epoch 208/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0462\n",
            "Epoch 208: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0411 - val_loss: 0.1291 - lr: 0.0010\n",
            "Epoch 209/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0539\n",
            "Epoch 209: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0430 - val_loss: 0.1159 - lr: 0.0010\n",
            "Epoch 210/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0335\n",
            "Epoch 210: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0411 - val_loss: 0.1256 - lr: 0.0010\n",
            "Epoch 211/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0442\n",
            "Epoch 211: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0457 - val_loss: 0.1157 - lr: 0.0010\n",
            "Epoch 212/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0503\n",
            "Epoch 212: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0490 - val_loss: 0.1262 - lr: 0.0010\n",
            "Epoch 213/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0468\n",
            "Epoch 213: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0470 - val_loss: 0.1598 - lr: 0.0010\n",
            "Epoch 214/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0770\n",
            "Epoch 214: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0690 - val_loss: 0.1590 - lr: 0.0010\n",
            "Epoch 215/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0750\n",
            "Epoch 215: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0626 - val_loss: 0.1670 - lr: 0.0010\n",
            "Epoch 216/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0740\n",
            "Epoch 216: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0763 - val_loss: 0.1405 - lr: 0.0010\n",
            "Epoch 217/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0565\n",
            "Epoch 217: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0662 - val_loss: 0.1431 - lr: 0.0010\n",
            "Epoch 218/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0648\n",
            "Epoch 218: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0709 - val_loss: 0.1897 - lr: 0.0010\n",
            "Epoch 219/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1005\n",
            "Epoch 219: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0968 - val_loss: 0.1709 - lr: 0.0010\n",
            "Epoch 220/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0893\n",
            "Epoch 220: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0992 - val_loss: 0.1318 - lr: 0.0010\n",
            "Epoch 221/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0615\n",
            "Epoch 221: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0799 - val_loss: 0.1378 - lr: 0.0010\n",
            "Epoch 222/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0697\n",
            "Epoch 222: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0826 - val_loss: 0.1454 - lr: 0.0010\n",
            "Epoch 223/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0583\n",
            "Epoch 223: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0815 - val_loss: 0.1388 - lr: 0.0010\n",
            "Epoch 224/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0729\n",
            "Epoch 224: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0845 - val_loss: 0.1314 - lr: 0.0010\n",
            "Epoch 225/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0641\n",
            "Epoch 225: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0713 - val_loss: 0.2127 - lr: 0.0010\n",
            "Epoch 226/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1389\n",
            "Epoch 226: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0892 - val_loss: 0.1320 - lr: 0.0010\n",
            "Epoch 227/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0574\n",
            "Epoch 227: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0560 - val_loss: 0.1681 - lr: 0.0010\n",
            "Epoch 228/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0868\n",
            "Epoch 228: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0773 - val_loss: 0.1700 - lr: 0.0010\n",
            "Epoch 229/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0637\n",
            "Epoch 229: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0627 - val_loss: 0.1959 - lr: 0.0010\n",
            "Epoch 230/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1169\n",
            "Epoch 230: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0823 - val_loss: 0.2091 - lr: 0.0010\n",
            "Epoch 231/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1254\n",
            "Epoch 231: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0791 - val_loss: 0.1363 - lr: 0.0010\n",
            "Epoch 232/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0616\n",
            "Epoch 232: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0640 - val_loss: 0.1150 - lr: 0.0010\n",
            "Epoch 233/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0442\n",
            "Epoch 233: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0602 - val_loss: 0.1355 - lr: 0.0010\n",
            "Epoch 234/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0567\n",
            "Epoch 234: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0632 - val_loss: 0.1475 - lr: 0.0010\n",
            "Epoch 235/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0825\n",
            "Epoch 235: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0688 - val_loss: 0.1346 - lr: 0.0010\n",
            "Epoch 236/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0517\n",
            "Epoch 236: val_loss did not improve from 0.11491\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0470 - val_loss: 0.1161 - lr: 0.0010\n",
            "Epoch 237/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0473\n",
            "Epoch 237: val_loss improved from 0.11491 to 0.11149, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0430 - val_loss: 0.1115 - lr: 0.0010\n",
            "Epoch 238/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0456\n",
            "Epoch 238: val_loss did not improve from 0.11149\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0473 - val_loss: 0.1164 - lr: 0.0010\n",
            "Epoch 239/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0544\n",
            "Epoch 239: val_loss improved from 0.11149 to 0.10552, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0516 - val_loss: 0.1055 - lr: 0.0010\n",
            "Epoch 240/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0286\n",
            "Epoch 240: val_loss improved from 0.10552 to 0.09877, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0425 - val_loss: 0.0988 - lr: 0.0010\n",
            "Epoch 241/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0300\n",
            "Epoch 241: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0466 - val_loss: 0.1070 - lr: 0.0010\n",
            "Epoch 242/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0475\n",
            "Epoch 242: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0470 - val_loss: 0.1278 - lr: 0.0010\n",
            "Epoch 243/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0601\n",
            "Epoch 243: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0595 - val_loss: 0.1114 - lr: 0.0010\n",
            "Epoch 244/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0420\n",
            "Epoch 244: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0452 - val_loss: 0.1734 - lr: 0.0010\n",
            "Epoch 245/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1095\n",
            "Epoch 245: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0639 - val_loss: 0.1596 - lr: 0.0010\n",
            "Epoch 246/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0926\n",
            "Epoch 246: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0688 - val_loss: 0.1193 - lr: 0.0010\n",
            "Epoch 247/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0468\n",
            "Epoch 247: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0411 - val_loss: 0.1125 - lr: 0.0010\n",
            "Epoch 248/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0404\n",
            "Epoch 248: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0385 - val_loss: 0.1260 - lr: 0.0010\n",
            "Epoch 249/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0469\n",
            "Epoch 249: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0474 - val_loss: 0.1132 - lr: 0.0010\n",
            "Epoch 250/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0372\n",
            "Epoch 250: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0465 - val_loss: 0.1305 - lr: 0.0010\n",
            "Epoch 251/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0558\n",
            "Epoch 251: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0522 - val_loss: 0.1044 - lr: 0.0010\n",
            "Epoch 252/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0354\n",
            "Epoch 252: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0410 - val_loss: 0.1045 - lr: 0.0010\n",
            "Epoch 253/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0361\n",
            "Epoch 253: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0459 - val_loss: 0.1191 - lr: 0.0010\n",
            "Epoch 254/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0545\n",
            "Epoch 254: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0473 - val_loss: 0.1098 - lr: 0.0010\n",
            "Epoch 255/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0418\n",
            "Epoch 255: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0480 - val_loss: 0.1735 - lr: 0.0010\n",
            "Epoch 256/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0997\n",
            "Epoch 256: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0766 - val_loss: 0.2433 - lr: 0.0010\n",
            "Epoch 257/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1716\n",
            "Epoch 257: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1179 - val_loss: 0.1639 - lr: 0.0010\n",
            "Epoch 258/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0983\n",
            "Epoch 258: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0785 - val_loss: 0.1352 - lr: 0.0010\n",
            "Epoch 259/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0747\n",
            "Epoch 259: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0672 - val_loss: 0.1653 - lr: 0.0010\n",
            "Epoch 260/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0883\n",
            "Epoch 260: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0733 - val_loss: 0.1374 - lr: 0.0010\n",
            "Epoch 261/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0767\n",
            "Epoch 261: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0696 - val_loss: 0.1565 - lr: 0.0010\n",
            "Epoch 262/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0769\n",
            "Epoch 262: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0694 - val_loss: 0.1979 - lr: 0.0010\n",
            "Epoch 263/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1277\n",
            "Epoch 263: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0772 - val_loss: 0.1789 - lr: 0.0010\n",
            "Epoch 264/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0866\n",
            "Epoch 264: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0856 - val_loss: 0.2377 - lr: 0.0010\n",
            "Epoch 265/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1657\n",
            "Epoch 265: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1202 - val_loss: 0.1156 - lr: 0.0010\n",
            "Epoch 266/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0503\n",
            "Epoch 266: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0624 - val_loss: 0.1842 - lr: 0.0010\n",
            "Epoch 267/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1096\n",
            "Epoch 267: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0925 - val_loss: 0.1904 - lr: 0.0010\n",
            "Epoch 268/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1294\n",
            "Epoch 268: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0897 - val_loss: 0.1224 - lr: 0.0010\n",
            "Epoch 269/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0524\n",
            "Epoch 269: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0426 - val_loss: 0.1043 - lr: 0.0010\n",
            "Epoch 270/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0411\n",
            "Epoch 270: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0385 - val_loss: 0.1200 - lr: 0.0010\n",
            "Epoch 271/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0368\n",
            "Epoch 271: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0398 - val_loss: 0.1218 - lr: 0.0010\n",
            "Epoch 272/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0433\n",
            "Epoch 272: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0446 - val_loss: 0.1151 - lr: 0.0010\n",
            "Epoch 273/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0332\n",
            "Epoch 273: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0380 - val_loss: 0.1129 - lr: 0.0010\n",
            "Epoch 274/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0565\n",
            "Epoch 274: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0455 - val_loss: 0.1367 - lr: 0.0010\n",
            "Epoch 275/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0603\n",
            "Epoch 275: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0607 - val_loss: 0.1087 - lr: 0.0010\n",
            "Epoch 276/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0457\n",
            "Epoch 276: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0542 - val_loss: 0.1738 - lr: 0.0010\n",
            "Epoch 277/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1017\n",
            "Epoch 277: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0754 - val_loss: 0.1202 - lr: 0.0010\n",
            "Epoch 278/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0473\n",
            "Epoch 278: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0439 - val_loss: 0.1158 - lr: 0.0010\n",
            "Epoch 279/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0478\n",
            "Epoch 279: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0418 - val_loss: 0.1001 - lr: 0.0010\n",
            "Epoch 280/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0360\n",
            "Epoch 280: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0396 - val_loss: 0.1252 - lr: 0.0010\n",
            "Epoch 281/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0418\n",
            "Epoch 281: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0408 - val_loss: 0.1136 - lr: 0.0010\n",
            "Epoch 282/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0524\n",
            "Epoch 282: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0483 - val_loss: 0.1525 - lr: 0.0010\n",
            "Epoch 283/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0670\n",
            "Epoch 283: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0542 - val_loss: 0.1345 - lr: 0.0010\n",
            "Epoch 284/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0870\n",
            "Epoch 284: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0650 - val_loss: 0.1798 - lr: 0.0010\n",
            "Epoch 285/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1048\n",
            "Epoch 285: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0638 - val_loss: 0.1249 - lr: 0.0010\n",
            "Epoch 286/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0697\n",
            "Epoch 286: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0589 - val_loss: 0.1498 - lr: 0.0010\n",
            "Epoch 287/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0842\n",
            "Epoch 287: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0676 - val_loss: 0.1702 - lr: 0.0010\n",
            "Epoch 288/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1179\n",
            "Epoch 288: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0948 - val_loss: 0.3034 - lr: 0.0010\n",
            "Epoch 289/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2100\n",
            "Epoch 289: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1700 - val_loss: 0.1504 - lr: 0.0010\n",
            "Epoch 290/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1017\n",
            "Epoch 290: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1030 - val_loss: 0.1725 - lr: 0.0010\n",
            "Epoch 291/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0936\n",
            "Epoch 291: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1026 - val_loss: 0.2425 - lr: 0.0010\n",
            "Epoch 292/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1486\n",
            "Epoch 292: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1167 - val_loss: 0.1640 - lr: 0.0010\n",
            "Epoch 293/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0887\n",
            "Epoch 293: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0771 - val_loss: 0.1275 - lr: 0.0010\n",
            "Epoch 294/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0491\n",
            "Epoch 294: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0436 - val_loss: 0.1052 - lr: 0.0010\n",
            "Epoch 295/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0459\n",
            "Epoch 295: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0405 - val_loss: 0.1397 - lr: 0.0010\n",
            "Epoch 296/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0744\n",
            "Epoch 296: val_loss did not improve from 0.09877\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0579 - val_loss: 0.1243 - lr: 0.0010\n",
            "Epoch 297/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0685\n",
            "Epoch 297: val_loss improved from 0.09877 to 0.09864, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0472 - val_loss: 0.0986 - lr: 0.0010\n",
            "Epoch 298/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0347\n",
            "Epoch 298: val_loss did not improve from 0.09864\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0376 - val_loss: 0.1009 - lr: 0.0010\n",
            "Epoch 299/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0325\n",
            "Epoch 299: val_loss did not improve from 0.09864\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0362 - val_loss: 0.0988 - lr: 0.0010\n",
            "Epoch 300/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0316\n",
            "Epoch 300: val_loss improved from 0.09864 to 0.09291, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0374 - val_loss: 0.0929 - lr: 0.0010\n",
            "Epoch 301/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0284\n",
            "Epoch 301: val_loss improved from 0.09291 to 0.08796, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0284 - val_loss: 0.0880 - lr: 0.0010\n",
            "Epoch 302/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0288\n",
            "Epoch 302: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0346 - val_loss: 0.1100 - lr: 0.0010\n",
            "Epoch 303/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0382\n",
            "Epoch 303: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0455 - val_loss: 0.1019 - lr: 0.0010\n",
            "Epoch 304/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0500\n",
            "Epoch 304: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0460 - val_loss: 0.1242 - lr: 0.0010\n",
            "Epoch 305/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0487\n",
            "Epoch 305: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0434 - val_loss: 0.1637 - lr: 0.0010\n",
            "Epoch 306/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1080\n",
            "Epoch 306: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0829 - val_loss: 0.1177 - lr: 0.0010\n",
            "Epoch 307/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0453\n",
            "Epoch 307: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0502 - val_loss: 0.1326 - lr: 0.0010\n",
            "Epoch 308/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0638\n",
            "Epoch 308: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0510 - val_loss: 0.1321 - lr: 0.0010\n",
            "Epoch 309/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0715\n",
            "Epoch 309: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0531 - val_loss: 0.1125 - lr: 0.0010\n",
            "Epoch 310/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0494\n",
            "Epoch 310: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0417 - val_loss: 0.1186 - lr: 0.0010\n",
            "Epoch 311/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0512\n",
            "Epoch 311: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0454 - val_loss: 0.0999 - lr: 0.0010\n",
            "Epoch 312/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0245\n",
            "Epoch 312: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0340 - val_loss: 0.1242 - lr: 0.0010\n",
            "Epoch 313/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0625\n",
            "Epoch 313: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0598 - val_loss: 0.1451 - lr: 0.0010\n",
            "Epoch 314/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0959\n",
            "Epoch 314: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0690 - val_loss: 0.1360 - lr: 0.0010\n",
            "Epoch 315/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0715\n",
            "Epoch 315: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0653 - val_loss: 0.2249 - lr: 0.0010\n",
            "Epoch 316/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1634\n",
            "Epoch 316: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1138 - val_loss: 0.2266 - lr: 0.0010\n",
            "Epoch 317/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1640\n",
            "Epoch 317: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1080 - val_loss: 0.1329 - lr: 0.0010\n",
            "Epoch 318/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0822\n",
            "Epoch 318: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0949 - val_loss: 0.1065 - lr: 0.0010\n",
            "Epoch 319/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0431\n",
            "Epoch 319: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0775 - val_loss: 0.1459 - lr: 0.0010\n",
            "Epoch 320/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0661\n",
            "Epoch 320: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0930 - val_loss: 0.1624 - lr: 0.0010\n",
            "Epoch 321/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1064\n",
            "Epoch 321: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0908 - val_loss: 0.1622 - lr: 0.0010\n",
            "Epoch 322/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0874\n",
            "Epoch 322: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0678 - val_loss: 0.1184 - lr: 0.0010\n",
            "Epoch 323/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0554\n",
            "Epoch 323: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0589 - val_loss: 0.1144 - lr: 0.0010\n",
            "Epoch 324/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0426\n",
            "Epoch 324: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0635 - val_loss: 0.0932 - lr: 0.0010\n",
            "Epoch 325/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0358\n",
            "Epoch 325: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0499 - val_loss: 0.0992 - lr: 0.0010\n",
            "Epoch 326/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0272\n",
            "Epoch 326: val_loss did not improve from 0.08796\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0391 - val_loss: 0.1004 - lr: 0.0010\n",
            "Epoch 327/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0461\n",
            "Epoch 327: val_loss improved from 0.08796 to 0.08661, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0393 - val_loss: 0.0866 - lr: 0.0010\n",
            "Epoch 328/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0383\n",
            "Epoch 328: val_loss improved from 0.08661 to 0.08557, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0337 - val_loss: 0.0856 - lr: 0.0010\n",
            "Epoch 329/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0296\n",
            "Epoch 329: val_loss improved from 0.08557 to 0.08117, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0257 - val_loss: 0.0812 - lr: 0.0010\n",
            "Epoch 330/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0226\n",
            "Epoch 330: val_loss did not improve from 0.08117\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0261 - val_loss: 0.0868 - lr: 0.0010\n",
            "Epoch 331/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0339\n",
            "Epoch 331: val_loss did not improve from 0.08117\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0293 - val_loss: 0.0824 - lr: 0.0010\n",
            "Epoch 332/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0318\n",
            "Epoch 332: val_loss did not improve from 0.08117\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0291 - val_loss: 0.0838 - lr: 0.0010\n",
            "Epoch 333/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0279\n",
            "Epoch 333: val_loss did not improve from 0.08117\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0256 - val_loss: 0.0840 - lr: 0.0010\n",
            "Epoch 334/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0228\n",
            "Epoch 334: val_loss did not improve from 0.08117\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0292 - val_loss: 0.0830 - lr: 0.0010\n",
            "Epoch 335/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0211\n",
            "Epoch 335: val_loss improved from 0.08117 to 0.07986, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0249 - val_loss: 0.0799 - lr: 0.0010\n",
            "Epoch 336/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0208\n",
            "Epoch 336: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0233 - val_loss: 0.0809 - lr: 0.0010\n",
            "Epoch 337/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0228\n",
            "Epoch 337: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0251 - val_loss: 0.0849 - lr: 0.0010\n",
            "Epoch 338/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0286\n",
            "Epoch 338: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0249 - val_loss: 0.0818 - lr: 0.0010\n",
            "Epoch 339/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0322\n",
            "Epoch 339: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0244 - val_loss: 0.0949 - lr: 0.0010\n",
            "Epoch 340/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0383\n",
            "Epoch 340: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0283 - val_loss: 0.0929 - lr: 0.0010\n",
            "Epoch 341/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0415\n",
            "Epoch 341: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0284 - val_loss: 0.0830 - lr: 0.0010\n",
            "Epoch 342/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0248\n",
            "Epoch 342: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0248 - val_loss: 0.0868 - lr: 0.0010\n",
            "Epoch 343/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0335\n",
            "Epoch 343: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0258 - val_loss: 0.1142 - lr: 0.0010\n",
            "Epoch 344/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0543\n",
            "Epoch 344: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0448 - val_loss: 0.1178 - lr: 0.0010\n",
            "Epoch 345/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0525\n",
            "Epoch 345: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0375 - val_loss: 0.0968 - lr: 0.0010\n",
            "Epoch 346/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0419\n",
            "Epoch 346: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0411 - val_loss: 0.1113 - lr: 0.0010\n",
            "Epoch 347/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0456\n",
            "Epoch 347: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0353 - val_loss: 0.1069 - lr: 0.0010\n",
            "Epoch 348/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0589\n",
            "Epoch 348: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0488 - val_loss: 0.1191 - lr: 0.0010\n",
            "Epoch 349/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0522\n",
            "Epoch 349: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0476 - val_loss: 0.1022 - lr: 0.0010\n",
            "Epoch 350/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0439\n",
            "Epoch 350: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0437 - val_loss: 0.1058 - lr: 0.0010\n",
            "Epoch 351/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0518\n",
            "Epoch 351: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0448 - val_loss: 0.0813 - lr: 1.0000e-04\n",
            "Epoch 352/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0272\n",
            "Epoch 352: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0240 - val_loss: 0.0950 - lr: 1.0000e-04\n",
            "Epoch 353/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0241\n",
            "Epoch 353: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0303 - val_loss: 0.0825 - lr: 1.0000e-04\n",
            "Epoch 354/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0183\n",
            "Epoch 354: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0244 - val_loss: 0.0874 - lr: 1.0000e-04\n",
            "Epoch 355/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0327\n",
            "Epoch 355: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0253 - val_loss: 0.0813 - lr: 1.0000e-04\n",
            "Epoch 356/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0166\n",
            "Epoch 356: val_loss did not improve from 0.07986\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0220 - val_loss: 0.0829 - lr: 1.0000e-04\n",
            "Epoch 357/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0291\n",
            "Epoch 357: val_loss improved from 0.07986 to 0.07865, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0227 - val_loss: 0.0786 - lr: 1.0000e-04\n",
            "Epoch 358/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0191\n",
            "Epoch 358: val_loss did not improve from 0.07865\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0218 - val_loss: 0.0796 - lr: 1.0000e-04\n",
            "Epoch 359/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 359: val_loss did not improve from 0.07865\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0208 - val_loss: 0.0807 - lr: 1.0000e-04\n",
            "Epoch 360/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0206\n",
            "Epoch 360: val_loss did not improve from 0.07865\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0208 - val_loss: 0.0791 - lr: 1.0000e-04\n",
            "Epoch 361/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0204\n",
            "Epoch 361: val_loss improved from 0.07865 to 0.07757, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0203 - val_loss: 0.0776 - lr: 1.0000e-04\n",
            "Epoch 362/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0309\n",
            "Epoch 362: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0205 - val_loss: 0.0780 - lr: 1.0000e-04\n",
            "Epoch 363/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0219\n",
            "Epoch 363: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0200 - val_loss: 0.0811 - lr: 1.0000e-04\n",
            "Epoch 364/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0182\n",
            "Epoch 364: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0216 - val_loss: 0.0796 - lr: 1.0000e-04\n",
            "Epoch 365/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0221\n",
            "Epoch 365: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0203 - val_loss: 0.0788 - lr: 1.0000e-04\n",
            "Epoch 366/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0206\n",
            "Epoch 366: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0207 - val_loss: 0.0781 - lr: 1.0000e-04\n",
            "Epoch 367/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0219\n",
            "Epoch 367: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0205 - val_loss: 0.0788 - lr: 1.0000e-04\n",
            "Epoch 368/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0238\n",
            "Epoch 368: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0201 - val_loss: 0.0788 - lr: 1.0000e-04\n",
            "Epoch 369/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0186\n",
            "Epoch 369: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0202 - val_loss: 0.0792 - lr: 1.0000e-04\n",
            "Epoch 370/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0141\n",
            "Epoch 370: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0200 - val_loss: 0.0796 - lr: 1.0000e-04\n",
            "Epoch 371/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0226\n",
            "Epoch 371: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0203 - val_loss: 0.0795 - lr: 1.0000e-04\n",
            "Epoch 372/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0225\n",
            "Epoch 372: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0204 - val_loss: 0.0782 - lr: 1.0000e-04\n",
            "Epoch 373/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0223\n",
            "Epoch 373: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0202 - val_loss: 0.0777 - lr: 1.0000e-04\n",
            "Epoch 374/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0209\n",
            "Epoch 374: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0202 - val_loss: 0.0778 - lr: 1.0000e-04\n",
            "Epoch 375/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0266\n",
            "Epoch 375: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0196 - val_loss: 0.0783 - lr: 1.0000e-04\n",
            "Epoch 376/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0176\n",
            "Epoch 376: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0198 - val_loss: 0.0782 - lr: 1.0000e-04\n",
            "Epoch 377/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0155\n",
            "Epoch 377: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0196 - val_loss: 0.0789 - lr: 1.0000e-04\n",
            "Epoch 378/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0199\n",
            "Epoch 378: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0200 - val_loss: 0.0793 - lr: 1.0000e-04\n",
            "Epoch 379/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0183\n",
            "Epoch 379: val_loss did not improve from 0.07757\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0199 - val_loss: 0.0778 - lr: 1.0000e-04\n",
            "Epoch 380/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0201\n",
            "Epoch 380: val_loss improved from 0.07757 to 0.07685, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0197 - val_loss: 0.0769 - lr: 1.0000e-04\n",
            "Epoch 381/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0189\n",
            "Epoch 381: val_loss did not improve from 0.07685\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0204 - val_loss: 0.0772 - lr: 1.0000e-04\n",
            "Epoch 382/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0155\n",
            "Epoch 382: val_loss did not improve from 0.07685\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0202 - val_loss: 0.0779 - lr: 1.0000e-04\n",
            "Epoch 383/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 383: val_loss did not improve from 0.07685\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0200 - val_loss: 0.0785 - lr: 1.0000e-04\n",
            "Epoch 384/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0143\n",
            "Epoch 384: val_loss did not improve from 0.07685\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0198 - val_loss: 0.0785 - lr: 1.0000e-04\n",
            "Epoch 385/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0205\n",
            "Epoch 385: val_loss did not improve from 0.07685\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0201 - val_loss: 0.0784 - lr: 1.0000e-04\n",
            "Epoch 386/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0197\n",
            "Epoch 386: val_loss did not improve from 0.07685\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0212 - val_loss: 0.0784 - lr: 1.0000e-04\n",
            "Epoch 387/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0205\n",
            "Epoch 387: val_loss improved from 0.07685 to 0.07654, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0202 - val_loss: 0.0765 - lr: 1.0000e-04\n",
            "Epoch 388/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0210\n",
            "Epoch 388: val_loss improved from 0.07654 to 0.07630, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0200 - val_loss: 0.0763 - lr: 1.0000e-04\n",
            "Epoch 389/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0183\n",
            "Epoch 389: val_loss did not improve from 0.07630\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0195 - val_loss: 0.0776 - lr: 1.0000e-04\n",
            "Epoch 390/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0235\n",
            "Epoch 390: val_loss improved from 0.07630 to 0.07604, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0198 - val_loss: 0.0760 - lr: 1.0000e-04\n",
            "Epoch 391/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0216\n",
            "Epoch 391: val_loss improved from 0.07604 to 0.07594, saving model to access_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0197 - val_loss: 0.0759 - lr: 1.0000e-04\n",
            "Epoch 392/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0216\n",
            "Epoch 392: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0196 - val_loss: 0.0766 - lr: 1.0000e-04\n",
            "Epoch 393/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0200\n",
            "Epoch 393: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0193 - val_loss: 0.0766 - lr: 1.0000e-04\n",
            "Epoch 394/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0178\n",
            "Epoch 394: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0194 - val_loss: 0.0769 - lr: 1.0000e-04\n",
            "Epoch 395/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0200\n",
            "Epoch 395: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0192 - val_loss: 0.0774 - lr: 1.0000e-04\n",
            "Epoch 396/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0222\n",
            "Epoch 396: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0192 - val_loss: 0.0767 - lr: 1.0000e-04\n",
            "Epoch 397/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0148\n",
            "Epoch 397: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0194 - val_loss: 0.0765 - lr: 1.0000e-04\n",
            "Epoch 398/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0171\n",
            "Epoch 398: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0192 - val_loss: 0.0782 - lr: 1.0000e-04\n",
            "Epoch 399/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0189\n",
            "Epoch 399: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0196 - val_loss: 0.0779 - lr: 1.0000e-04\n",
            "Epoch 400/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0174\n",
            "Epoch 400: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0197 - val_loss: 0.0772 - lr: 1.0000e-04\n",
            "Epoch 401/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0232\n",
            "Epoch 401: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0192 - val_loss: 0.0771 - lr: 1.0000e-05\n",
            "Epoch 402/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0168\n",
            "Epoch 402: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0192 - val_loss: 0.0773 - lr: 1.0000e-05\n",
            "Epoch 403/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0232\n",
            "Epoch 403: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0193 - val_loss: 0.0775 - lr: 1.0000e-05\n",
            "Epoch 404/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0175\n",
            "Epoch 404: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0193 - val_loss: 0.0773 - lr: 1.0000e-05\n",
            "Epoch 405/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0186\n",
            "Epoch 405: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0192 - val_loss: 0.0770 - lr: 1.0000e-05\n",
            "Epoch 406/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0198\n",
            "Epoch 406: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0191 - val_loss: 0.0769 - lr: 1.0000e-05\n",
            "Epoch 407/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0179\n",
            "Epoch 407: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0191 - val_loss: 0.0768 - lr: 1.0000e-05\n",
            "Epoch 408/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0176\n",
            "Epoch 408: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0191 - val_loss: 0.0767 - lr: 1.0000e-05\n",
            "Epoch 409/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0219\n",
            "Epoch 409: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0191 - val_loss: 0.0765 - lr: 1.0000e-05\n",
            "Epoch 410/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0248\n",
            "Epoch 410: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0191 - val_loss: 0.0764 - lr: 1.0000e-05\n",
            "Epoch 411/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0162\n",
            "Epoch 411: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0191 - val_loss: 0.0762 - lr: 1.0000e-05\n",
            "Epoch 412/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0189\n",
            "Epoch 412: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0191 - val_loss: 0.0762 - lr: 1.0000e-05\n",
            "Epoch 413/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0196\n",
            "Epoch 413: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0191 - val_loss: 0.0762 - lr: 1.0000e-05\n",
            "Epoch 414/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0151\n",
            "Epoch 414: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0191 - val_loss: 0.0762 - lr: 1.0000e-05\n",
            "Epoch 415/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0205\n",
            "Epoch 415: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0191 - val_loss: 0.0762 - lr: 1.0000e-05\n",
            "Epoch 416/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0178\n",
            "Epoch 416: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0191 - val_loss: 0.0762 - lr: 1.0000e-05\n",
            "Epoch 417/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0227\n",
            "Epoch 417: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0191 - val_loss: 0.0761 - lr: 1.0000e-05\n",
            "Epoch 418/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0251\n",
            "Epoch 418: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0191 - val_loss: 0.0761 - lr: 1.0000e-05\n",
            "Epoch 419/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0219\n",
            "Epoch 419: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0191 - val_loss: 0.0762 - lr: 1.0000e-05\n",
            "Epoch 420/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0243\n",
            "Epoch 420: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0762 - lr: 1.0000e-05\n",
            "Epoch 421/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0208\n",
            "Epoch 421: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.0763 - lr: 1.0000e-05\n",
            "Epoch 422/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0159\n",
            "Epoch 422: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0191 - val_loss: 0.0764 - lr: 1.0000e-05\n",
            "Epoch 423/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0210\n",
            "Epoch 423: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0191 - val_loss: 0.0764 - lr: 1.0000e-05\n",
            "Epoch 424/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0136\n",
            "Epoch 424: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0765 - lr: 1.0000e-05\n",
            "Epoch 425/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0159\n",
            "Epoch 425: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0765 - lr: 1.0000e-05\n",
            "Epoch 426/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0163\n",
            "Epoch 426: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.0766 - lr: 1.0000e-05\n",
            "Epoch 427/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0158\n",
            "Epoch 427: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0767 - lr: 1.0000e-05\n",
            "Epoch 428/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0161\n",
            "Epoch 428: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0767 - lr: 1.0000e-05\n",
            "Epoch 429/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0204\n",
            "Epoch 429: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0190 - val_loss: 0.0766 - lr: 1.0000e-05\n",
            "Epoch 430/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0248\n",
            "Epoch 430: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.0765 - lr: 1.0000e-05\n",
            "Epoch 431/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0173\n",
            "Epoch 431: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.0765 - lr: 1.0000e-05\n",
            "Epoch 432/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0152\n",
            "Epoch 432: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0766 - lr: 1.0000e-05\n",
            "Epoch 433/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0203\n",
            "Epoch 433: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0190 - val_loss: 0.0769 - lr: 1.0000e-05\n",
            "Epoch 434/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0206\n",
            "Epoch 434: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0771 - lr: 1.0000e-05\n",
            "Epoch 435/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 435: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0191 - val_loss: 0.0770 - lr: 1.0000e-05\n",
            "Epoch 436/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0154\n",
            "Epoch 436: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0191 - val_loss: 0.0769 - lr: 1.0000e-05\n",
            "Epoch 437/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0258\n",
            "Epoch 437: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0191 - val_loss: 0.0768 - lr: 1.0000e-05\n",
            "Epoch 438/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0139\n",
            "Epoch 438: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0192 - val_loss: 0.0767 - lr: 1.0000e-05\n",
            "Epoch 439/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0200\n",
            "Epoch 439: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0192 - val_loss: 0.0765 - lr: 1.0000e-05\n",
            "Epoch 440/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0211\n",
            "Epoch 440: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0191 - val_loss: 0.0764 - lr: 1.0000e-05\n",
            "Epoch 441/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0190\n",
            "Epoch 441: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.0765 - lr: 1.0000e-05\n",
            "Epoch 442/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0194\n",
            "Epoch 442: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0189 - val_loss: 0.0768 - lr: 1.0000e-05\n",
            "Epoch 443/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0193\n",
            "Epoch 443: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0191 - val_loss: 0.0770 - lr: 1.0000e-05\n",
            "Epoch 444/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0201\n",
            "Epoch 444: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0192 - val_loss: 0.0769 - lr: 1.0000e-05\n",
            "Epoch 445/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0177\n",
            "Epoch 445: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0191 - val_loss: 0.0766 - lr: 1.0000e-05\n",
            "Epoch 446/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0161\n",
            "Epoch 446: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0764 - lr: 1.0000e-05\n",
            "Epoch 447/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0227\n",
            "Epoch 447: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0762 - lr: 1.0000e-05\n",
            "Epoch 448/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0198\n",
            "Epoch 448: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.0760 - lr: 1.0000e-05\n",
            "Epoch 449/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0199\n",
            "Epoch 449: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0190 - val_loss: 0.0760 - lr: 1.0000e-05\n",
            "Epoch 450/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0258\n",
            "Epoch 450: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0190 - val_loss: 0.0760 - lr: 1.0000e-05\n",
            "Epoch 451/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0196\n",
            "Epoch 451: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0190 - val_loss: 0.0760 - lr: 1.0000e-06\n",
            "Epoch 452/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0225\n",
            "Epoch 452: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0760 - lr: 1.0000e-06\n",
            "Epoch 453/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0184\n",
            "Epoch 453: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0760 - lr: 1.0000e-06\n",
            "Epoch 454/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0216\n",
            "Epoch 454: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0190 - val_loss: 0.0760 - lr: 1.0000e-06\n",
            "Epoch 455/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0197\n",
            "Epoch 455: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0760 - lr: 1.0000e-06\n",
            "Epoch 456/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0184\n",
            "Epoch 456: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.0760 - lr: 1.0000e-06\n",
            "Epoch 457/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0188\n",
            "Epoch 457: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.0760 - lr: 1.0000e-06\n",
            "Epoch 458/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 458: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0190 - val_loss: 0.0760 - lr: 1.0000e-06\n",
            "Epoch 459/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0189\n",
            "Epoch 459: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 460/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 460: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0190 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 461/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0182\n",
            "Epoch 461: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 462/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0153\n",
            "Epoch 462: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 463/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0182\n",
            "Epoch 463: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 464/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0198\n",
            "Epoch 464: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 465/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0249\n",
            "Epoch 465: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 466/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0177\n",
            "Epoch 466: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 467/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0198\n",
            "Epoch 467: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 468/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0200\n",
            "Epoch 468: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 469/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0215\n",
            "Epoch 469: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 470/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0158\n",
            "Epoch 470: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 471/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0208\n",
            "Epoch 471: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 472/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0188\n",
            "Epoch 472: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 473/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0154\n",
            "Epoch 473: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 474/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0177\n",
            "Epoch 474: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 475/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0154\n",
            "Epoch 475: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 476/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0154\n",
            "Epoch 476: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 477/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0163\n",
            "Epoch 477: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 478/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0174\n",
            "Epoch 478: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 479/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0178\n",
            "Epoch 479: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 480/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0205\n",
            "Epoch 480: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 481/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0189\n",
            "Epoch 481: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 482/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0240\n",
            "Epoch 482: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 483/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0181\n",
            "Epoch 483: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0189 - val_loss: 0.0762 - lr: 1.0000e-06\n",
            "Epoch 484/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0191\n",
            "Epoch 484: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0189 - val_loss: 0.0762 - lr: 1.0000e-06\n",
            "Epoch 485/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0168\n",
            "Epoch 485: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0189 - val_loss: 0.0762 - lr: 1.0000e-06\n",
            "Epoch 486/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0157\n",
            "Epoch 486: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 487/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0229\n",
            "Epoch 487: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 488/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0284\n",
            "Epoch 488: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 489/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 489: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 490/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0175\n",
            "Epoch 490: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 491/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0271\n",
            "Epoch 491: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 492/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0230\n",
            "Epoch 492: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 493/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0224\n",
            "Epoch 493: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 494/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0182\n",
            "Epoch 494: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 495/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0222\n",
            "Epoch 495: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 496/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0170\n",
            "Epoch 496: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 497/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0169\n",
            "Epoch 497: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 498/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0191\n",
            "Epoch 498: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 499/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0190\n",
            "Epoch 499: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "Epoch 500/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0154\n",
            "Epoch 500: val_loss did not improve from 0.07594\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0189 - val_loss: 0.0761 - lr: 1.0000e-06\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0761\n",
            "Test Loss: 0.07610032707452774\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0759\n",
            "Test Loss (MSE) with Best Model: 0.07594054192304611\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "model_1 = Sequential([\n",
        "    Dense(4, activation='selu', kernel_regularizer=regularizers.l2(0.00001), input_shape=(4,)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(8, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(16, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(32, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(64, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(128, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(256, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(1)  # Single output neuron for regression\n",
        "])\n",
        "\n",
        "def learning_rate_schedule(epoch, initial_lr=0.001):\n",
        "    \"\"\"\n",
        "    Custom learning rate schedule. Adjust the learning rate based on the current epoch.\n",
        "    You can customize this function to fit your needs.\n",
        "    \"\"\"\n",
        "    if epoch < 300 or epoch == 300:\n",
        "        return initial_lr  # Keep the initial learning rate for the first 50 epochs\n",
        "    elif epoch > 300 and epoch%50 == 0:\n",
        "        initial_lr = initial_lr * 0.1  # Reduce the learning rate by a factor of 10 after epoch 50\n",
        "        return initial_lr\n",
        "    else:\n",
        "        return initial_lr\n",
        "\n",
        "# Create the Adam optimizer with the initial learning rate\n",
        "initial_learning_rate = 0.001\n",
        "adam_optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(optimizer=adam_optimizer, loss='mean_squared_error')\n",
        "\n",
        "checkpoint_1 = ModelCheckpoint('access_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "model_1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[checkpoint_1,lr_scheduler])\n",
        "\n",
        "loss_1 = model_1.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss_1)\n",
        "\n",
        "predictions_1 = model_1.predict(X_test)\n",
        "\n",
        "errors_1 = np.abs(predictions_1 - y_test)\n",
        "\n",
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model('access_model.h5')\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "test_loss = best_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss (MSE) with Best Model: {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THqrUDY99XSJ"
      },
      "source": [
        "Sensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crgAhWcD9Zqp",
        "outputId": "b79b7e81-5e25-4c66-bd80-833683e9f615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/4 [======>.......................] - ETA: 2s - loss: 26.0078\n",
            "Epoch 1: val_loss improved from inf to 7.84104, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 1s 58ms/step - loss: 18.8540 - val_loss: 7.8410 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 8.5114\n",
            "Epoch 2: val_loss improved from 7.84104 to 2.59352, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 6.2141 - val_loss: 2.5935 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 2.0825\n",
            "Epoch 3: val_loss improved from 2.59352 to 1.85022, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 2.0426 - val_loss: 1.8502 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.5663\n",
            "Epoch 4: val_loss did not improve from 1.85022\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.7632 - val_loss: 2.0421 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.5909\n",
            "Epoch 5: val_loss improved from 1.85022 to 1.40883, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 1.6920 - val_loss: 1.4088 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.1757\n",
            "Epoch 6: val_loss did not improve from 1.40883\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 1.1283 - val_loss: 1.5080 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.1548\n",
            "Epoch 7: val_loss improved from 1.40883 to 1.40804, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 1.2282 - val_loss: 1.4080 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.9756\n",
            "Epoch 8: val_loss improved from 1.40804 to 1.12149, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.9420 - val_loss: 1.1215 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.9116\n",
            "Epoch 9: val_loss improved from 1.12149 to 1.06575, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.7995 - val_loss: 1.0658 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.7542\n",
            "Epoch 10: val_loss improved from 1.06575 to 0.96195, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.7607 - val_loss: 0.9619 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6737\n",
            "Epoch 11: val_loss did not improve from 0.96195\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6897 - val_loss: 0.9632 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5685\n",
            "Epoch 12: val_loss improved from 0.96195 to 0.88282, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6303 - val_loss: 0.8828 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4915\n",
            "Epoch 13: val_loss improved from 0.88282 to 0.80906, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5897 - val_loss: 0.8091 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3415\n",
            "Epoch 14: val_loss improved from 0.80906 to 0.76480, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5361 - val_loss: 0.7648 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4050\n",
            "Epoch 15: val_loss improved from 0.76480 to 0.73540, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4908 - val_loss: 0.7354 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5611\n",
            "Epoch 16: val_loss improved from 0.73540 to 0.68978, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4824 - val_loss: 0.6898 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2722\n",
            "Epoch 17: val_loss improved from 0.68978 to 0.68624, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5026 - val_loss: 0.6862 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4644\n",
            "Epoch 18: val_loss did not improve from 0.68624\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4624 - val_loss: 0.7379 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4110\n",
            "Epoch 19: val_loss did not improve from 0.68624\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4379 - val_loss: 0.7059 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6225\n",
            "Epoch 20: val_loss improved from 0.68624 to 0.64811, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4451 - val_loss: 0.6481 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3259\n",
            "Epoch 21: val_loss improved from 0.64811 to 0.61159, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4277 - val_loss: 0.6116 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3501\n",
            "Epoch 22: val_loss did not improve from 0.61159\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3844 - val_loss: 0.6344 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3148\n",
            "Epoch 23: val_loss improved from 0.61159 to 0.61150, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3789 - val_loss: 0.6115 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4296\n",
            "Epoch 24: val_loss improved from 0.61150 to 0.58307, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3676 - val_loss: 0.5831 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3216\n",
            "Epoch 25: val_loss did not improve from 0.58307\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3373 - val_loss: 0.5929 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3453\n",
            "Epoch 26: val_loss improved from 0.58307 to 0.56861, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3394 - val_loss: 0.5686 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2899\n",
            "Epoch 27: val_loss improved from 0.56861 to 0.56515, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3247 - val_loss: 0.5651 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3361\n",
            "Epoch 28: val_loss improved from 0.56515 to 0.55270, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3239 - val_loss: 0.5527 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2651\n",
            "Epoch 29: val_loss improved from 0.55270 to 0.53501, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3277 - val_loss: 0.5350 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3942\n",
            "Epoch 30: val_loss improved from 0.53501 to 0.51842, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3216 - val_loss: 0.5184 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2748\n",
            "Epoch 31: val_loss did not improve from 0.51842\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3012 - val_loss: 0.6062 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3869\n",
            "Epoch 32: val_loss did not improve from 0.51842\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3386 - val_loss: 0.5224 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1732\n",
            "Epoch 33: val_loss did not improve from 0.51842\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2972 - val_loss: 0.5660 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3665\n",
            "Epoch 34: val_loss improved from 0.51842 to 0.49941, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3597 - val_loss: 0.4994 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1271\n",
            "Epoch 35: val_loss improved from 0.49941 to 0.48104, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.2724 - val_loss: 0.4810 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3334\n",
            "Epoch 36: val_loss improved from 0.48104 to 0.45340, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2661 - val_loss: 0.4534 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2439\n",
            "Epoch 37: val_loss improved from 0.45340 to 0.44789, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2562 - val_loss: 0.4479 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2960\n",
            "Epoch 38: val_loss did not improve from 0.44789\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2394 - val_loss: 0.4784 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2421\n",
            "Epoch 39: val_loss improved from 0.44789 to 0.42838, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2474 - val_loss: 0.4284 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2172\n",
            "Epoch 40: val_loss did not improve from 0.42838\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2289 - val_loss: 0.4336 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3011\n",
            "Epoch 41: val_loss improved from 0.42838 to 0.39599, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.2292 - val_loss: 0.3960 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2845\n",
            "Epoch 42: val_loss did not improve from 0.39599\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2342 - val_loss: 0.4057 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2277\n",
            "Epoch 43: val_loss improved from 0.39599 to 0.39029, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2004 - val_loss: 0.3903 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2354\n",
            "Epoch 44: val_loss improved from 0.39029 to 0.37995, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.2028 - val_loss: 0.3799 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1177\n",
            "Epoch 45: val_loss did not improve from 0.37995\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2246 - val_loss: 0.4435 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1986\n",
            "Epoch 46: val_loss improved from 0.37995 to 0.37656, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2050 - val_loss: 0.3766 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2289\n",
            "Epoch 47: val_loss improved from 0.37656 to 0.35398, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2127 - val_loss: 0.3540 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1386\n",
            "Epoch 48: val_loss did not improve from 0.35398\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1859 - val_loss: 0.3828 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2124\n",
            "Epoch 49: val_loss did not improve from 0.35398\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1856 - val_loss: 0.3567 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2594\n",
            "Epoch 50: val_loss improved from 0.35398 to 0.31570, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1953 - val_loss: 0.3157 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1249\n",
            "Epoch 51: val_loss improved from 0.31570 to 0.30850, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1774 - val_loss: 0.3085 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2029\n",
            "Epoch 52: val_loss did not improve from 0.30850\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1517 - val_loss: 0.3441 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2436\n",
            "Epoch 53: val_loss did not improve from 0.30850\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.1667 - val_loss: 0.3229 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1437\n",
            "Epoch 54: val_loss did not improve from 0.30850\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1876 - val_loss: 0.3441 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2036\n",
            "Epoch 55: val_loss improved from 0.30850 to 0.29351, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.1604 - val_loss: 0.2935 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1294\n",
            "Epoch 56: val_loss improved from 0.29351 to 0.26452, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1451 - val_loss: 0.2645 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1956\n",
            "Epoch 57: val_loss did not improve from 0.26452\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1361 - val_loss: 0.2653 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1105\n",
            "Epoch 58: val_loss improved from 0.26452 to 0.26183, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1203 - val_loss: 0.2618 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1702\n",
            "Epoch 59: val_loss improved from 0.26183 to 0.23564, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1343 - val_loss: 0.2356 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1242\n",
            "Epoch 60: val_loss improved from 0.23564 to 0.23432, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1234 - val_loss: 0.2343 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1097\n",
            "Epoch 61: val_loss improved from 0.23432 to 0.21753, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1191 - val_loss: 0.2175 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1146\n",
            "Epoch 62: val_loss improved from 0.21753 to 0.21305, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1024 - val_loss: 0.2131 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1379\n",
            "Epoch 63: val_loss improved from 0.21305 to 0.20327, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0997 - val_loss: 0.2033 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0919\n",
            "Epoch 64: val_loss improved from 0.20327 to 0.19425, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0921 - val_loss: 0.1942 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0875\n",
            "Epoch 65: val_loss improved from 0.19425 to 0.18272, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0916 - val_loss: 0.1827 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0815\n",
            "Epoch 66: val_loss did not improve from 0.18272\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0845 - val_loss: 0.1841 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0702\n",
            "Epoch 67: val_loss improved from 0.18272 to 0.16131, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1106 - val_loss: 0.1613 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0682\n",
            "Epoch 68: val_loss did not improve from 0.16131\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0905 - val_loss: 0.1629 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0655\n",
            "Epoch 69: val_loss improved from 0.16131 to 0.14917, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0758 - val_loss: 0.1492 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0788\n",
            "Epoch 70: val_loss improved from 0.14917 to 0.14159, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0716 - val_loss: 0.1416 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0631\n",
            "Epoch 71: val_loss did not improve from 0.14159\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0694 - val_loss: 0.1521 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0646\n",
            "Epoch 72: val_loss improved from 0.14159 to 0.13575, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0719 - val_loss: 0.1357 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0451\n",
            "Epoch 73: val_loss improved from 0.13575 to 0.12188, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0652 - val_loss: 0.1219 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0538\n",
            "Epoch 74: val_loss improved from 0.12188 to 0.11688, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0582 - val_loss: 0.1169 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0651\n",
            "Epoch 75: val_loss improved from 0.11688 to 0.11426, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0539 - val_loss: 0.1143 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0476\n",
            "Epoch 76: val_loss improved from 0.11426 to 0.10676, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0638 - val_loss: 0.1068 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0744\n",
            "Epoch 77: val_loss did not improve from 0.10676\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0567 - val_loss: 0.1085 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0524\n",
            "Epoch 78: val_loss improved from 0.10676 to 0.09992, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0580 - val_loss: 0.0999 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0556\n",
            "Epoch 79: val_loss improved from 0.09992 to 0.09386, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0519 - val_loss: 0.0939 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0464\n",
            "Epoch 80: val_loss did not improve from 0.09386\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0571 - val_loss: 0.0941 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0454\n",
            "Epoch 81: val_loss improved from 0.09386 to 0.09000, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0516 - val_loss: 0.0900 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0515\n",
            "Epoch 82: val_loss did not improve from 0.09000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0552 - val_loss: 0.1168 - lr: 0.0010\n",
            "Epoch 83/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0601\n",
            "Epoch 83: val_loss did not improve from 0.09000\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0708 - val_loss: 0.1172 - lr: 0.0010\n",
            "Epoch 84/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0681\n",
            "Epoch 84: val_loss did not improve from 0.09000\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1035 - val_loss: 0.0982 - lr: 0.0010\n",
            "Epoch 85/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0398\n",
            "Epoch 85: val_loss did not improve from 0.09000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0621 - val_loss: 0.1142 - lr: 0.0010\n",
            "Epoch 86/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0729\n",
            "Epoch 86: val_loss did not improve from 0.09000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0745 - val_loss: 0.1183 - lr: 0.0010\n",
            "Epoch 87/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0801\n",
            "Epoch 87: val_loss did not improve from 0.09000\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0676 - val_loss: 0.1392 - lr: 0.0010\n",
            "Epoch 88/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1075\n",
            "Epoch 88: val_loss did not improve from 0.09000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0758 - val_loss: 0.1050 - lr: 0.0010\n",
            "Epoch 89/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0557\n",
            "Epoch 89: val_loss improved from 0.09000 to 0.08473, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0540 - val_loss: 0.0847 - lr: 0.0010\n",
            "Epoch 90/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0453\n",
            "Epoch 90: val_loss improved from 0.08473 to 0.07406, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0476 - val_loss: 0.0741 - lr: 0.0010\n",
            "Epoch 91/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0301\n",
            "Epoch 91: val_loss improved from 0.07406 to 0.07276, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0424 - val_loss: 0.0728 - lr: 0.0010\n",
            "Epoch 92/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0343\n",
            "Epoch 92: val_loss improved from 0.07276 to 0.07021, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0471 - val_loss: 0.0702 - lr: 0.0010\n",
            "Epoch 93/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0229\n",
            "Epoch 93: val_loss did not improve from 0.07021\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0398 - val_loss: 0.1271 - lr: 0.0010\n",
            "Epoch 94/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1073\n",
            "Epoch 94: val_loss did not improve from 0.07021\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1072 - val_loss: 0.0933 - lr: 0.0010\n",
            "Epoch 95/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0423\n",
            "Epoch 95: val_loss did not improve from 0.07021\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0545 - val_loss: 0.0945 - lr: 0.0010\n",
            "Epoch 96/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0387\n",
            "Epoch 96: val_loss did not improve from 0.07021\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0451 - val_loss: 0.1461 - lr: 0.0010\n",
            "Epoch 97/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1096\n",
            "Epoch 97: val_loss did not improve from 0.07021\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0868 - val_loss: 0.0982 - lr: 0.0010\n",
            "Epoch 98/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0788\n",
            "Epoch 98: val_loss did not improve from 0.07021\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0523 - val_loss: 0.0985 - lr: 0.0010\n",
            "Epoch 99/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0477\n",
            "Epoch 99: val_loss did not improve from 0.07021\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0641 - val_loss: 0.0764 - lr: 0.0010\n",
            "Epoch 100/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0388\n",
            "Epoch 100: val_loss did not improve from 0.07021\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0542 - val_loss: 0.0809 - lr: 0.0010\n",
            "Epoch 101/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0416\n",
            "Epoch 101: val_loss improved from 0.07021 to 0.06757, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0635 - val_loss: 0.0676 - lr: 0.0010\n",
            "Epoch 102/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0351\n",
            "Epoch 102: val_loss did not improve from 0.06757\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0447 - val_loss: 0.0805 - lr: 0.0010\n",
            "Epoch 103/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0393\n",
            "Epoch 103: val_loss did not improve from 0.06757\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0563 - val_loss: 0.0776 - lr: 0.0010\n",
            "Epoch 104/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0440\n",
            "Epoch 104: val_loss did not improve from 0.06757\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0507 - val_loss: 0.0763 - lr: 0.0010\n",
            "Epoch 105/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0557\n",
            "Epoch 105: val_loss improved from 0.06757 to 0.06083, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0557 - val_loss: 0.0608 - lr: 0.0010\n",
            "Epoch 106/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0299\n",
            "Epoch 106: val_loss improved from 0.06083 to 0.05992, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0363 - val_loss: 0.0599 - lr: 0.0010\n",
            "Epoch 107/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0325\n",
            "Epoch 107: val_loss improved from 0.05992 to 0.05318, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0316 - val_loss: 0.0532 - lr: 0.0010\n",
            "Epoch 108/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0322\n",
            "Epoch 108: val_loss improved from 0.05318 to 0.04993, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0267 - val_loss: 0.0499 - lr: 0.0010\n",
            "Epoch 109/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0282\n",
            "Epoch 109: val_loss did not improve from 0.04993\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0224 - val_loss: 0.0513 - lr: 0.0010\n",
            "Epoch 110/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0238\n",
            "Epoch 110: val_loss did not improve from 0.04993\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0256 - val_loss: 0.0509 - lr: 0.0010\n",
            "Epoch 111/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0254\n",
            "Epoch 111: val_loss did not improve from 0.04993\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0303 - val_loss: 0.0513 - lr: 0.0010\n",
            "Epoch 112/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0265\n",
            "Epoch 112: val_loss improved from 0.04993 to 0.04892, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0250 - val_loss: 0.0489 - lr: 0.0010\n",
            "Epoch 113/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0162\n",
            "Epoch 113: val_loss did not improve from 0.04892\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0200 - val_loss: 0.0549 - lr: 0.0010\n",
            "Epoch 114/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0239\n",
            "Epoch 114: val_loss improved from 0.04892 to 0.04757, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0309 - val_loss: 0.0476 - lr: 0.0010\n",
            "Epoch 115/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0249\n",
            "Epoch 115: val_loss did not improve from 0.04757\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0238 - val_loss: 0.0481 - lr: 0.0010\n",
            "Epoch 116/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0238\n",
            "Epoch 116: val_loss improved from 0.04757 to 0.04635, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0239 - val_loss: 0.0464 - lr: 0.0010\n",
            "Epoch 117/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0232\n",
            "Epoch 117: val_loss did not improve from 0.04635\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0242 - val_loss: 0.0464 - lr: 0.0010\n",
            "Epoch 118/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0142\n",
            "Epoch 118: val_loss improved from 0.04635 to 0.04291, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0244 - val_loss: 0.0429 - lr: 0.0010\n",
            "Epoch 119/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0169\n",
            "Epoch 119: val_loss improved from 0.04291 to 0.03673, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0214 - val_loss: 0.0367 - lr: 0.0010\n",
            "Epoch 120/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0201\n",
            "Epoch 120: val_loss improved from 0.03673 to 0.03530, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0172 - val_loss: 0.0353 - lr: 0.0010\n",
            "Epoch 121/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0135\n",
            "Epoch 121: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.0410 - lr: 0.0010\n",
            "Epoch 122/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0174\n",
            "Epoch 122: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0259 - val_loss: 0.0423 - lr: 0.0010\n",
            "Epoch 123/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0197\n",
            "Epoch 123: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0231 - val_loss: 0.0378 - lr: 0.0010\n",
            "Epoch 124/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0195\n",
            "Epoch 124: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0271 - val_loss: 0.0383 - lr: 0.0010\n",
            "Epoch 125/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0182\n",
            "Epoch 125: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0273 - val_loss: 0.0464 - lr: 0.0010\n",
            "Epoch 126/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0191\n",
            "Epoch 126: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0288 - val_loss: 0.0492 - lr: 0.0010\n",
            "Epoch 127/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0206\n",
            "Epoch 127: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0265 - val_loss: 0.0482 - lr: 0.0010\n",
            "Epoch 128/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0357\n",
            "Epoch 128: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0238 - val_loss: 0.0361 - lr: 0.0010\n",
            "Epoch 129/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0183\n",
            "Epoch 129: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0204 - val_loss: 0.0427 - lr: 0.0010\n",
            "Epoch 130/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0204\n",
            "Epoch 130: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0193 - val_loss: 0.0551 - lr: 0.0010\n",
            "Epoch 131/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0286\n",
            "Epoch 131: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0390 - val_loss: 0.0987 - lr: 0.0010\n",
            "Epoch 132/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0922\n",
            "Epoch 132: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0564 - val_loss: 0.0710 - lr: 0.0010\n",
            "Epoch 133/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0489\n",
            "Epoch 133: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0409 - val_loss: 0.0940 - lr: 0.0010\n",
            "Epoch 134/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0751\n",
            "Epoch 134: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0409 - val_loss: 0.0784 - lr: 0.0010\n",
            "Epoch 135/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0485\n",
            "Epoch 135: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0372 - val_loss: 0.0589 - lr: 0.0010\n",
            "Epoch 136/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0331\n",
            "Epoch 136: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0246 - val_loss: 0.0456 - lr: 0.0010\n",
            "Epoch 137/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0302\n",
            "Epoch 137: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0221 - val_loss: 0.0424 - lr: 0.0010\n",
            "Epoch 138/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0267\n",
            "Epoch 138: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0204 - val_loss: 0.0427 - lr: 0.0010\n",
            "Epoch 139/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0223\n",
            "Epoch 139: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0218 - val_loss: 0.0381 - lr: 0.0010\n",
            "Epoch 140/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0170\n",
            "Epoch 140: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0212 - val_loss: 0.0416 - lr: 0.0010\n",
            "Epoch 141/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0205\n",
            "Epoch 141: val_loss did not improve from 0.03530\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0221 - val_loss: 0.0439 - lr: 0.0010\n",
            "Epoch 142/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0286\n",
            "Epoch 142: val_loss improved from 0.03530 to 0.02699, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0246 - val_loss: 0.0270 - lr: 0.0010\n",
            "Epoch 143/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0107\n",
            "Epoch 143: val_loss did not improve from 0.02699\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0146 - val_loss: 0.0321 - lr: 0.0010\n",
            "Epoch 144/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0146\n",
            "Epoch 144: val_loss did not improve from 0.02699\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0166 - val_loss: 0.0280 - lr: 0.0010\n",
            "Epoch 145/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0102\n",
            "Epoch 145: val_loss improved from 0.02699 to 0.02602, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0121 - val_loss: 0.0260 - lr: 0.0010\n",
            "Epoch 146/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0106\n",
            "Epoch 146: val_loss did not improve from 0.02602\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0116 - val_loss: 0.0260 - lr: 0.0010\n",
            "Epoch 147/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0130\n",
            "Epoch 147: val_loss improved from 0.02602 to 0.02467, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0117 - val_loss: 0.0247 - lr: 0.0010\n",
            "Epoch 148/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0105\n",
            "Epoch 148: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0115 - val_loss: 0.0277 - lr: 0.0010\n",
            "Epoch 149/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0094\n",
            "Epoch 149: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 0.0402 - lr: 0.0010\n",
            "Epoch 150/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0192\n",
            "Epoch 150: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0238 - val_loss: 0.0328 - lr: 0.0010\n",
            "Epoch 151/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 151: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0229 - val_loss: 0.0343 - lr: 0.0010\n",
            "Epoch 152/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0177\n",
            "Epoch 152: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0185 - val_loss: 0.0282 - lr: 0.0010\n",
            "Epoch 153/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0143\n",
            "Epoch 153: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0135 - val_loss: 0.0326 - lr: 0.0010\n",
            "Epoch 154/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0166\n",
            "Epoch 154: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0158 - val_loss: 0.0340 - lr: 0.0010\n",
            "Epoch 155/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0227\n",
            "Epoch 155: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0166 - val_loss: 0.0335 - lr: 0.0010\n",
            "Epoch 156/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0232\n",
            "Epoch 156: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0168 - val_loss: 0.0315 - lr: 0.0010\n",
            "Epoch 157/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0156\n",
            "Epoch 157: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0137 - val_loss: 0.0264 - lr: 0.0010\n",
            "Epoch 158/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0117\n",
            "Epoch 158: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0124 - val_loss: 0.0374 - lr: 0.0010\n",
            "Epoch 159/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0227\n",
            "Epoch 159: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0204 - val_loss: 0.0358 - lr: 0.0010\n",
            "Epoch 160/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0237\n",
            "Epoch 160: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0180 - val_loss: 0.0495 - lr: 0.0010\n",
            "Epoch 161/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0348\n",
            "Epoch 161: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0193 - val_loss: 0.0425 - lr: 0.0010\n",
            "Epoch 162/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0324\n",
            "Epoch 162: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0208 - val_loss: 0.1065 - lr: 0.0010\n",
            "Epoch 163/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0682\n",
            "Epoch 163: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1194 - val_loss: 0.1210 - lr: 0.0010\n",
            "Epoch 164/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0892\n",
            "Epoch 164: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0778 - val_loss: 0.1159 - lr: 0.0010\n",
            "Epoch 165/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0877\n",
            "Epoch 165: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0701 - val_loss: 0.0588 - lr: 0.0010\n",
            "Epoch 166/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0358\n",
            "Epoch 166: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0346 - val_loss: 0.0680 - lr: 0.0010\n",
            "Epoch 167/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0731\n",
            "Epoch 167: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0343 - val_loss: 0.0457 - lr: 0.0010\n",
            "Epoch 168/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0233\n",
            "Epoch 168: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0251 - val_loss: 0.0477 - lr: 0.0010\n",
            "Epoch 169/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0320\n",
            "Epoch 169: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0265 - val_loss: 0.0442 - lr: 0.0010\n",
            "Epoch 170/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0268\n",
            "Epoch 170: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0321 - val_loss: 0.0312 - lr: 0.0010\n",
            "Epoch 171/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0213\n",
            "Epoch 171: val_loss did not improve from 0.02467\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0197 - val_loss: 0.0276 - lr: 0.0010\n",
            "Epoch 172/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0117\n",
            "Epoch 172: val_loss improved from 0.02467 to 0.02178, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0177 - val_loss: 0.0218 - lr: 0.0010\n",
            "Epoch 173/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0085\n",
            "Epoch 173: val_loss did not improve from 0.02178\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0147 - val_loss: 0.0238 - lr: 0.0010\n",
            "Epoch 174/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0093\n",
            "Epoch 174: val_loss did not improve from 0.02178\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0145 - val_loss: 0.0229 - lr: 0.0010\n",
            "Epoch 175/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0103\n",
            "Epoch 175: val_loss did not improve from 0.02178\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0113 - val_loss: 0.0225 - lr: 0.0010\n",
            "Epoch 176/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0121\n",
            "Epoch 176: val_loss did not improve from 0.02178\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0114 - val_loss: 0.0274 - lr: 0.0010\n",
            "Epoch 177/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0152\n",
            "Epoch 177: val_loss did not improve from 0.02178\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0154 - val_loss: 0.0243 - lr: 0.0010\n",
            "Epoch 178/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0109\n",
            "Epoch 178: val_loss did not improve from 0.02178\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0128 - val_loss: 0.0246 - lr: 0.0010\n",
            "Epoch 179/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0136\n",
            "Epoch 179: val_loss did not improve from 0.02178\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0127 - val_loss: 0.0253 - lr: 0.0010\n",
            "Epoch 180/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0144\n",
            "Epoch 180: val_loss did not improve from 0.02178\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0137 - val_loss: 0.0269 - lr: 0.0010\n",
            "Epoch 181/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0193\n",
            "Epoch 181: val_loss improved from 0.02178 to 0.02155, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0167 - val_loss: 0.0215 - lr: 0.0010\n",
            "Epoch 182/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0085\n",
            "Epoch 182: val_loss did not improve from 0.02155\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0095 - val_loss: 0.0227 - lr: 0.0010\n",
            "Epoch 183/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0132\n",
            "Epoch 183: val_loss did not improve from 0.02155\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0240 - lr: 0.0010\n",
            "Epoch 184/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0136\n",
            "Epoch 184: val_loss improved from 0.02155 to 0.02121, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0117 - val_loss: 0.0212 - lr: 0.0010\n",
            "Epoch 185/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0086\n",
            "Epoch 185: val_loss improved from 0.02121 to 0.01885, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0100 - val_loss: 0.0188 - lr: 0.0010\n",
            "Epoch 186/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0095\n",
            "Epoch 186: val_loss did not improve from 0.01885\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0093 - val_loss: 0.0196 - lr: 0.0010\n",
            "Epoch 187/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0089\n",
            "Epoch 187: val_loss improved from 0.01885 to 0.01800, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0107 - val_loss: 0.0180 - lr: 0.0010\n",
            "Epoch 188/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0128\n",
            "Epoch 188: val_loss did not improve from 0.01800\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0101 - val_loss: 0.0180 - lr: 0.0010\n",
            "Epoch 189/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0118\n",
            "Epoch 189: val_loss improved from 0.01800 to 0.01575, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0105 - val_loss: 0.0157 - lr: 0.0010\n",
            "Epoch 190/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0082\n",
            "Epoch 190: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0100 - val_loss: 0.0378 - lr: 0.0010\n",
            "Epoch 191/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0176\n",
            "Epoch 191: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0407 - val_loss: 0.0914 - lr: 0.0010\n",
            "Epoch 192/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0762\n",
            "Epoch 192: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0449 - val_loss: 0.0903 - lr: 0.0010\n",
            "Epoch 193/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0764\n",
            "Epoch 193: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0428 - val_loss: 0.0690 - lr: 0.0010\n",
            "Epoch 194/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0448\n",
            "Epoch 194: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0352 - val_loss: 0.0863 - lr: 0.0010\n",
            "Epoch 195/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0700\n",
            "Epoch 195: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0535 - val_loss: 0.1562 - lr: 0.0010\n",
            "Epoch 196/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1066\n",
            "Epoch 196: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0782 - val_loss: 0.1174 - lr: 0.0010\n",
            "Epoch 197/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0914\n",
            "Epoch 197: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0808 - val_loss: 0.0898 - lr: 0.0010\n",
            "Epoch 198/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0495\n",
            "Epoch 198: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0589 - val_loss: 0.0492 - lr: 0.0010\n",
            "Epoch 199/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0329\n",
            "Epoch 199: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0534 - val_loss: 0.0545 - lr: 0.0010\n",
            "Epoch 200/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0450\n",
            "Epoch 200: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0357 - val_loss: 0.0216 - lr: 0.0010\n",
            "Epoch 201/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0136\n",
            "Epoch 201: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0183 - val_loss: 0.0241 - lr: 0.0010\n",
            "Epoch 202/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0139\n",
            "Epoch 202: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0164 - val_loss: 0.0279 - lr: 0.0010\n",
            "Epoch 203/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0091\n",
            "Epoch 203: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0132 - val_loss: 0.0273 - lr: 0.0010\n",
            "Epoch 204/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0156\n",
            "Epoch 204: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0155 - val_loss: 0.0277 - lr: 0.0010\n",
            "Epoch 205/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0130\n",
            "Epoch 205: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0128 - val_loss: 0.0237 - lr: 0.0010\n",
            "Epoch 206/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0157\n",
            "Epoch 206: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0152 - val_loss: 0.0212 - lr: 0.0010\n",
            "Epoch 207/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0129\n",
            "Epoch 207: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0109 - val_loss: 0.0199 - lr: 0.0010\n",
            "Epoch 208/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0073\n",
            "Epoch 208: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0086 - val_loss: 0.0171 - lr: 0.0010\n",
            "Epoch 209/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0077\n",
            "Epoch 209: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0083 - val_loss: 0.0184 - lr: 0.0010\n",
            "Epoch 210/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0064\n",
            "Epoch 210: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0083 - val_loss: 0.0178 - lr: 0.0010\n",
            "Epoch 211/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0095\n",
            "Epoch 211: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0083 - val_loss: 0.0182 - lr: 0.0010\n",
            "Epoch 212/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0069\n",
            "Epoch 212: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0081 - val_loss: 0.0180 - lr: 0.0010\n",
            "Epoch 213/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0090\n",
            "Epoch 213: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0087 - val_loss: 0.0208 - lr: 0.0010\n",
            "Epoch 214/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0111\n",
            "Epoch 214: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0097 - val_loss: 0.0239 - lr: 0.0010\n",
            "Epoch 215/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0130\n",
            "Epoch 215: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0127 - val_loss: 0.0203 - lr: 0.0010\n",
            "Epoch 216/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0082\n",
            "Epoch 216: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0115 - val_loss: 0.0225 - lr: 0.0010\n",
            "Epoch 217/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0146\n",
            "Epoch 217: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0114 - val_loss: 0.0215 - lr: 0.0010\n",
            "Epoch 218/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0123\n",
            "Epoch 218: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0116 - val_loss: 0.0171 - lr: 0.0010\n",
            "Epoch 219/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0106\n",
            "Epoch 219: val_loss did not improve from 0.01575\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0102 - val_loss: 0.0183 - lr: 0.0010\n",
            "Epoch 220/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0123\n",
            "Epoch 220: val_loss improved from 0.01575 to 0.01409, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0106 - val_loss: 0.0141 - lr: 0.0010\n",
            "Epoch 221/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0086\n",
            "Epoch 221: val_loss did not improve from 0.01409\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0103 - val_loss: 0.0165 - lr: 0.0010\n",
            "Epoch 222/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0091\n",
            "Epoch 222: val_loss did not improve from 0.01409\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0090 - val_loss: 0.0151 - lr: 0.0010\n",
            "Epoch 223/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0093\n",
            "Epoch 223: val_loss did not improve from 0.01409\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0092 - val_loss: 0.0145 - lr: 0.0010\n",
            "Epoch 224/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0064\n",
            "Epoch 224: val_loss improved from 0.01409 to 0.01401, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0092 - val_loss: 0.0140 - lr: 0.0010\n",
            "Epoch 225/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0065\n",
            "Epoch 225: val_loss improved from 0.01401 to 0.01369, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0075 - val_loss: 0.0137 - lr: 0.0010\n",
            "Epoch 226/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0066\n",
            "Epoch 226: val_loss improved from 0.01369 to 0.01303, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0064 - val_loss: 0.0130 - lr: 0.0010\n",
            "Epoch 227/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0061\n",
            "Epoch 227: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0065 - val_loss: 0.0272 - lr: 0.0010\n",
            "Epoch 228/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0172\n",
            "Epoch 228: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0184 - val_loss: 0.0184 - lr: 0.0010\n",
            "Epoch 229/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0127\n",
            "Epoch 229: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0264 - lr: 0.0010\n",
            "Epoch 230/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0186\n",
            "Epoch 230: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0201 - lr: 0.0010\n",
            "Epoch 231/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0144\n",
            "Epoch 231: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0140 - val_loss: 0.0224 - lr: 0.0010\n",
            "Epoch 232/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0109\n",
            "Epoch 232: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0128 - val_loss: 0.0180 - lr: 0.0010\n",
            "Epoch 233/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0122\n",
            "Epoch 233: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0119 - val_loss: 0.0151 - lr: 0.0010\n",
            "Epoch 234/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0077\n",
            "Epoch 234: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0074 - val_loss: 0.0158 - lr: 0.0010\n",
            "Epoch 235/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0106\n",
            "Epoch 235: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0081 - val_loss: 0.0179 - lr: 0.0010\n",
            "Epoch 236/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0085\n",
            "Epoch 236: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0083 - val_loss: 0.0194 - lr: 0.0010\n",
            "Epoch 237/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0104\n",
            "Epoch 237: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0116 - val_loss: 0.0349 - lr: 0.0010\n",
            "Epoch 238/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0252\n",
            "Epoch 238: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0176 - val_loss: 0.0246 - lr: 0.0010\n",
            "Epoch 239/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0192\n",
            "Epoch 239: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0149 - val_loss: 0.0326 - lr: 0.0010\n",
            "Epoch 240/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0174\n",
            "Epoch 240: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0166 - val_loss: 0.0282 - lr: 0.0010\n",
            "Epoch 241/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0215\n",
            "Epoch 241: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0200 - val_loss: 0.0312 - lr: 0.0010\n",
            "Epoch 242/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0242\n",
            "Epoch 242: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0144 - val_loss: 0.0187 - lr: 0.0010\n",
            "Epoch 243/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0148\n",
            "Epoch 243: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0106 - val_loss: 0.0210 - lr: 0.0010\n",
            "Epoch 244/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0124\n",
            "Epoch 244: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0088 - val_loss: 0.0182 - lr: 0.0010\n",
            "Epoch 245/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0114\n",
            "Epoch 245: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0102 - val_loss: 0.0213 - lr: 0.0010\n",
            "Epoch 246/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0109\n",
            "Epoch 246: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0091 - val_loss: 0.0142 - lr: 0.0010\n",
            "Epoch 247/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0080\n",
            "Epoch 247: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0068 - val_loss: 0.0187 - lr: 0.0010\n",
            "Epoch 248/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0073\n",
            "Epoch 248: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0080 - val_loss: 0.0157 - lr: 0.0010\n",
            "Epoch 249/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0092\n",
            "Epoch 249: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0077 - val_loss: 0.0201 - lr: 0.0010\n",
            "Epoch 250/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0120\n",
            "Epoch 250: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0091 - val_loss: 0.0132 - lr: 0.0010\n",
            "Epoch 251/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0071\n",
            "Epoch 251: val_loss did not improve from 0.01303\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0073 - val_loss: 0.0158 - lr: 0.0010\n",
            "Epoch 252/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0078\n",
            "Epoch 252: val_loss improved from 0.01303 to 0.01231, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0074 - val_loss: 0.0123 - lr: 0.0010\n",
            "Epoch 253/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0080\n",
            "Epoch 253: val_loss did not improve from 0.01231\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0076 - val_loss: 0.0137 - lr: 0.0010\n",
            "Epoch 254/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0063\n",
            "Epoch 254: val_loss improved from 0.01231 to 0.01194, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0071 - val_loss: 0.0119 - lr: 0.0010\n",
            "Epoch 255/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0063\n",
            "Epoch 255: val_loss did not improve from 0.01194\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0063 - val_loss: 0.0162 - lr: 0.0010\n",
            "Epoch 256/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0089\n",
            "Epoch 256: val_loss did not improve from 0.01194\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0072 - val_loss: 0.0121 - lr: 0.0010\n",
            "Epoch 257/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0055\n",
            "Epoch 257: val_loss improved from 0.01194 to 0.01042, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0062 - val_loss: 0.0104 - lr: 0.0010\n",
            "Epoch 258/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0059\n",
            "Epoch 258: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0066 - val_loss: 0.0249 - lr: 0.0010\n",
            "Epoch 259/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0147\n",
            "Epoch 259: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0170 - val_loss: 0.0339 - lr: 0.0010\n",
            "Epoch 260/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0224\n",
            "Epoch 260: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0283 - val_loss: 0.0612 - lr: 0.0010\n",
            "Epoch 261/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0481\n",
            "Epoch 261: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0288 - val_loss: 0.0524 - lr: 0.0010\n",
            "Epoch 262/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0438\n",
            "Epoch 262: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0439 - val_loss: 0.0389 - lr: 0.0010\n",
            "Epoch 263/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0307\n",
            "Epoch 263: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0340 - val_loss: 0.0416 - lr: 0.0010\n",
            "Epoch 264/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0301\n",
            "Epoch 264: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0239 - val_loss: 0.0596 - lr: 0.0010\n",
            "Epoch 265/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0518\n",
            "Epoch 265: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0345 - val_loss: 0.0385 - lr: 0.0010\n",
            "Epoch 266/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0312\n",
            "Epoch 266: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0286 - val_loss: 0.0354 - lr: 0.0010\n",
            "Epoch 267/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0249\n",
            "Epoch 267: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0271 - val_loss: 0.0263 - lr: 0.0010\n",
            "Epoch 268/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0221\n",
            "Epoch 268: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0210 - val_loss: 0.0333 - lr: 0.0010\n",
            "Epoch 269/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0175\n",
            "Epoch 269: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0222 - val_loss: 0.0455 - lr: 0.0010\n",
            "Epoch 270/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0403\n",
            "Epoch 270: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0245 - val_loss: 0.0483 - lr: 0.0010\n",
            "Epoch 271/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0320\n",
            "Epoch 271: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0237 - val_loss: 0.0407 - lr: 0.0010\n",
            "Epoch 272/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0326\n",
            "Epoch 272: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0206 - val_loss: 0.0425 - lr: 0.0010\n",
            "Epoch 273/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0292\n",
            "Epoch 273: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0217 - val_loss: 0.0328 - lr: 0.0010\n",
            "Epoch 274/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0219\n",
            "Epoch 274: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0189 - val_loss: 0.0452 - lr: 0.0010\n",
            "Epoch 275/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0294\n",
            "Epoch 275: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0218 - val_loss: 0.0255 - lr: 0.0010\n",
            "Epoch 276/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0199\n",
            "Epoch 276: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0180 - val_loss: 0.0341 - lr: 0.0010\n",
            "Epoch 277/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0239\n",
            "Epoch 277: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0176 - val_loss: 0.0301 - lr: 0.0010\n",
            "Epoch 278/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0228\n",
            "Epoch 278: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0207 - val_loss: 0.0380 - lr: 0.0010\n",
            "Epoch 279/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0151\n",
            "Epoch 279: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0268 - val_loss: 0.0408 - lr: 0.0010\n",
            "Epoch 280/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0381\n",
            "Epoch 280: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0280 - val_loss: 0.0458 - lr: 0.0010\n",
            "Epoch 281/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0290\n",
            "Epoch 281: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0268 - val_loss: 0.0303 - lr: 0.0010\n",
            "Epoch 282/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0200\n",
            "Epoch 282: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0237 - val_loss: 0.0316 - lr: 0.0010\n",
            "Epoch 283/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0127\n",
            "Epoch 283: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0186 - val_loss: 0.0185 - lr: 0.0010\n",
            "Epoch 284/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0115\n",
            "Epoch 284: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0128 - val_loss: 0.0229 - lr: 0.0010\n",
            "Epoch 285/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0141\n",
            "Epoch 285: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0224 - lr: 0.0010\n",
            "Epoch 286/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0170\n",
            "Epoch 286: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0153 - val_loss: 0.0245 - lr: 0.0010\n",
            "Epoch 287/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0143\n",
            "Epoch 287: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0188 - val_loss: 0.0216 - lr: 0.0010\n",
            "Epoch 288/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0190\n",
            "Epoch 288: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0179 - val_loss: 0.0222 - lr: 0.0010\n",
            "Epoch 289/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0141\n",
            "Epoch 289: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0152 - val_loss: 0.0132 - lr: 0.0010\n",
            "Epoch 290/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0081\n",
            "Epoch 290: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0101 - val_loss: 0.0159 - lr: 0.0010\n",
            "Epoch 291/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0093\n",
            "Epoch 291: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0105 - val_loss: 0.0132 - lr: 0.0010\n",
            "Epoch 292/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0079\n",
            "Epoch 292: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0103 - val_loss: 0.0217 - lr: 0.0010\n",
            "Epoch 293/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0130\n",
            "Epoch 293: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0152 - val_loss: 0.0146 - lr: 0.0010\n",
            "Epoch 294/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0102\n",
            "Epoch 294: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0107 - val_loss: 0.0168 - lr: 0.0010\n",
            "Epoch 295/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0094\n",
            "Epoch 295: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0099 - val_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 296/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0059\n",
            "Epoch 296: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0074 - val_loss: 0.0125 - lr: 0.0010\n",
            "Epoch 297/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0067\n",
            "Epoch 297: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0068 - val_loss: 0.0133 - lr: 0.0010\n",
            "Epoch 298/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0072\n",
            "Epoch 298: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0066 - val_loss: 0.0143 - lr: 0.0010\n",
            "Epoch 299/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0071\n",
            "Epoch 299: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0069 - val_loss: 0.0116 - lr: 0.0010\n",
            "Epoch 300/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0052\n",
            "Epoch 300: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0051 - val_loss: 0.0120 - lr: 0.0010\n",
            "Epoch 301/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0055\n",
            "Epoch 301: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 0.0120 - lr: 0.0010\n",
            "Epoch 302/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0067\n",
            "Epoch 302: val_loss did not improve from 0.01042\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0058 - val_loss: 0.0107 - lr: 0.0010\n",
            "Epoch 303/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0045\n",
            "Epoch 303: val_loss improved from 0.01042 to 0.01029, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0054 - val_loss: 0.0103 - lr: 0.0010\n",
            "Epoch 304/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0050\n",
            "Epoch 304: val_loss did not improve from 0.01029\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0049 - val_loss: 0.0105 - lr: 0.0010\n",
            "Epoch 305/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0048\n",
            "Epoch 305: val_loss did not improve from 0.01029\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0051 - val_loss: 0.0104 - lr: 0.0010\n",
            "Epoch 306/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0055\n",
            "Epoch 306: val_loss improved from 0.01029 to 0.00949, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0051 - val_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 307/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0049\n",
            "Epoch 307: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0047 - val_loss: 0.0099 - lr: 0.0010\n",
            "Epoch 308/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0051\n",
            "Epoch 308: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0049 - val_loss: 0.0102 - lr: 0.0010\n",
            "Epoch 309/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0063\n",
            "Epoch 309: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0053 - val_loss: 0.0110 - lr: 0.0010\n",
            "Epoch 310/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0062\n",
            "Epoch 310: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0055 - val_loss: 0.0152 - lr: 0.0010\n",
            "Epoch 311/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0103\n",
            "Epoch 311: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0092 - val_loss: 0.0121 - lr: 0.0010\n",
            "Epoch 312/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0081\n",
            "Epoch 312: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0083 - val_loss: 0.0179 - lr: 0.0010\n",
            "Epoch 313/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0146\n",
            "Epoch 313: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0122 - val_loss: 0.0190 - lr: 0.0010\n",
            "Epoch 314/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0125\n",
            "Epoch 314: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0105 - val_loss: 0.0132 - lr: 0.0010\n",
            "Epoch 315/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0083\n",
            "Epoch 315: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0086 - val_loss: 0.0156 - lr: 0.0010\n",
            "Epoch 316/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0114\n",
            "Epoch 316: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0093 - val_loss: 0.0142 - lr: 0.0010\n",
            "Epoch 317/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0090\n",
            "Epoch 317: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0085 - val_loss: 0.0115 - lr: 0.0010\n",
            "Epoch 318/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0082\n",
            "Epoch 318: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0071 - val_loss: 0.0144 - lr: 0.0010\n",
            "Epoch 319/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0101\n",
            "Epoch 319: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0077 - val_loss: 0.0172 - lr: 0.0010\n",
            "Epoch 320/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0105\n",
            "Epoch 320: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0113 - val_loss: 0.0143 - lr: 0.0010\n",
            "Epoch 321/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0093\n",
            "Epoch 321: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0094 - val_loss: 0.0224 - lr: 0.0010\n",
            "Epoch 322/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0144\n",
            "Epoch 322: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0103 - val_loss: 0.0206 - lr: 0.0010\n",
            "Epoch 323/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0176\n",
            "Epoch 323: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0113 - val_loss: 0.0131 - lr: 0.0010\n",
            "Epoch 324/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0069\n",
            "Epoch 324: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0085 - val_loss: 0.0129 - lr: 0.0010\n",
            "Epoch 325/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0080\n",
            "Epoch 325: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0065 - val_loss: 0.0143 - lr: 0.0010\n",
            "Epoch 326/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0079\n",
            "Epoch 326: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0073 - val_loss: 0.0107 - lr: 0.0010\n",
            "Epoch 327/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0059\n",
            "Epoch 327: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0070 - val_loss: 0.0109 - lr: 0.0010\n",
            "Epoch 328/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0069\n",
            "Epoch 328: val_loss did not improve from 0.00949\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0062 - val_loss: 0.0096 - lr: 0.0010\n",
            "Epoch 329/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0052\n",
            "Epoch 329: val_loss improved from 0.00949 to 0.00878, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0060 - val_loss: 0.0088 - lr: 0.0010\n",
            "Epoch 330/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0044\n",
            "Epoch 330: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0069 - val_loss: 0.0161 - lr: 0.0010\n",
            "Epoch 331/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0109\n",
            "Epoch 331: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0081 - val_loss: 0.0134 - lr: 0.0010\n",
            "Epoch 332/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0100\n",
            "Epoch 332: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0070 - val_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 333/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0062\n",
            "Epoch 333: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0076 - val_loss: 0.0110 - lr: 0.0010\n",
            "Epoch 334/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0048\n",
            "Epoch 334: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0055 - val_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 335/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0052\n",
            "Epoch 335: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0054 - val_loss: 0.0104 - lr: 0.0010\n",
            "Epoch 336/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0061\n",
            "Epoch 336: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0055 - val_loss: 0.0116 - lr: 0.0010\n",
            "Epoch 337/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0060\n",
            "Epoch 337: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0057 - val_loss: 0.0091 - lr: 0.0010\n",
            "Epoch 338/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0057\n",
            "Epoch 338: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0053 - val_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 339/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0054\n",
            "Epoch 339: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0049 - val_loss: 0.0089 - lr: 0.0010\n",
            "Epoch 340/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0048\n",
            "Epoch 340: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0049 - val_loss: 0.0103 - lr: 0.0010\n",
            "Epoch 341/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0048\n",
            "Epoch 341: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0048 - val_loss: 0.0095 - lr: 0.0010\n",
            "Epoch 342/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0053\n",
            "Epoch 342: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0049 - val_loss: 0.0105 - lr: 0.0010\n",
            "Epoch 343/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0046\n",
            "Epoch 343: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0058 - val_loss: 0.0097 - lr: 0.0010\n",
            "Epoch 344/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0049\n",
            "Epoch 344: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0060 - val_loss: 0.0103 - lr: 0.0010\n",
            "Epoch 345/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0058\n",
            "Epoch 345: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0067 - val_loss: 0.0115 - lr: 0.0010\n",
            "Epoch 346/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0063\n",
            "Epoch 346: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0059 - val_loss: 0.0093 - lr: 0.0010\n",
            "Epoch 347/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0046\n",
            "Epoch 347: val_loss did not improve from 0.00878\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0108 - lr: 0.0010\n",
            "Epoch 348/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0060\n",
            "Epoch 348: val_loss improved from 0.00878 to 0.00875, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0059 - val_loss: 0.0088 - lr: 0.0010\n",
            "Epoch 349/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0047\n",
            "Epoch 349: val_loss did not improve from 0.00875\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0052 - val_loss: 0.0089 - lr: 0.0010\n",
            "Epoch 350/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0046\n",
            "Epoch 350: val_loss did not improve from 0.00875\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0052 - val_loss: 0.0100 - lr: 0.0010\n",
            "Epoch 351/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0051\n",
            "Epoch 351: val_loss improved from 0.00875 to 0.00865, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0053 - val_loss: 0.0087 - lr: 1.0000e-04\n",
            "Epoch 352/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 352: val_loss improved from 0.00865 to 0.00802, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0043 - val_loss: 0.0080 - lr: 1.0000e-04\n",
            "Epoch 353/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0042\n",
            "Epoch 353: val_loss improved from 0.00802 to 0.00787, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0044 - val_loss: 0.0079 - lr: 1.0000e-04\n",
            "Epoch 354/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0043\n",
            "Epoch 354: val_loss improved from 0.00787 to 0.00760, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0042 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 355/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 355: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 356/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 356: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 0.0079 - lr: 1.0000e-04\n",
            "Epoch 357/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 357: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 0.0082 - lr: 1.0000e-04\n",
            "Epoch 358/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0045\n",
            "Epoch 358: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 0.0081 - lr: 1.0000e-04\n",
            "Epoch 359/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 359: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0041 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 360/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 360: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0040 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 361/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 361: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0040 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 362/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 362: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 363/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 363: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 364/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 364: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0040 - val_loss: 0.0079 - lr: 1.0000e-04\n",
            "Epoch 365/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 365: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 366/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 366: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 367/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 367: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 368/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 368: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 369/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 369: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0040 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 370/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0043\n",
            "Epoch 370: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 371/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 371: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 372/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 372: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 373/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 373: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 374/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 374: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 375/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 375: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0079 - lr: 1.0000e-04\n",
            "Epoch 376/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 376: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0040 - val_loss: 0.0080 - lr: 1.0000e-04\n",
            "Epoch 377/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 377: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0040 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 378/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 378: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 379/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 379: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 380/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 380: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0039 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 381/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0036\n",
            "Epoch 381: val_loss did not improve from 0.00760\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 382/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 382: val_loss improved from 0.00760 to 0.00755, saving model to sensors_model.h5\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 383/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 383: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 384/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 384: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 385/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 385: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 386/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 386: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 387/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 387: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 388/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 388: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 389/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 389: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 390/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 390: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 391/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 391: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 392/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 392: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 393/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0042\n",
            "Epoch 393: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0040 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 394/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 394: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 0.0079 - lr: 1.0000e-04\n",
            "Epoch 395/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0042\n",
            "Epoch 395: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.0079 - lr: 1.0000e-04\n",
            "Epoch 396/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 396: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0039 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 397/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 397: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 398/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 398: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0039 - val_loss: 0.0078 - lr: 1.0000e-04\n",
            "Epoch 399/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 399: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-04\n",
            "Epoch 400/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 400: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-04\n",
            "Epoch 401/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 401: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 402/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 402: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 403/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 403: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 404/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 404: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 405/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 405: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 406/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 406: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 407/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 407: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 408/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 408: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 409/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 409: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 410/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 410: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 411/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 411: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 412/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 412: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 413/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0036\n",
            "Epoch 413: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 414/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 414: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 415/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 415: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 416/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 416: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 417/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0036\n",
            "Epoch 417: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 418/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 418: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 419/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 419: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 420/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 420: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 421/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 421: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 422/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 422: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 423/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 423: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 424/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 424: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 425/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 425: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 426/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 426: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 427/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 427: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 428/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 428: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 429/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 429: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 430/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 430: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 431/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 431: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 432/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 432: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 433/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 433: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 434/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 434: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 435/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 435: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 436/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 436: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 437/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 437: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 438/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 438: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 439/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0042\n",
            "Epoch 439: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 440/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 440: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 441/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 441: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 442/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 442: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 443/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 443: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 444/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 444: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 445/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 445: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 446/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 446: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 447/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0036\n",
            "Epoch 447: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 448/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 448: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 449/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 449: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0039 - val_loss: 0.0077 - lr: 1.0000e-05\n",
            "Epoch 450/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 450: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-05\n",
            "Epoch 451/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 451: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 452/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 452: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 453/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 453: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 454/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 454: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 455/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 455: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 456/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 456: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 457/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 457: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 458/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 458: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 459/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 459: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 460/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 460: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 461/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 461: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 462/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 462: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 463/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 463: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 464/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 464: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 465/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 465: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 466/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 466: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 467/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 467: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 468/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 468: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 469/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 469: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 470/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 470: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 471/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 471: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 472/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 472: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 473/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 473: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 474/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 474: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 475/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 475: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 476/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 476: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 477/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0036\n",
            "Epoch 477: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 478/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 478: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 479/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 479: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 480/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 480: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 481/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0036\n",
            "Epoch 481: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 482/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 482: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 483/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 483: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 484/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 484: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 485/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 485: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 486/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 486: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 487/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 487: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 488/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0036\n",
            "Epoch 488: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 489/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 489: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 490/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 490: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 491/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 491: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 492/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 492: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 493/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 493: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 494/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0040\n",
            "Epoch 494: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 495/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041\n",
            "Epoch 495: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 496/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0037\n",
            "Epoch 496: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 497/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 497: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 498/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 498: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 499/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038\n",
            "Epoch 499: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "Epoch 500/500\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039\n",
            "Epoch 500: val_loss did not improve from 0.00755\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.0076 - lr: 1.0000e-06\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0076\n",
            "Test Loss: 0.007643550634384155\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0076\n",
            "Test Loss (MSE) with Best Model: 0.0075507559813559055\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "model_1 = Sequential([\n",
        "    Dense(16, activation='selu', kernel_regularizer=regularizers.l2(0.00001), input_shape=(4,)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(32, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(64, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(128, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(64, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(32, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(16, activation='selu', kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    #Dropout(0.01),\n",
        "    Dense(1)  # Single output neuron for regression\n",
        "])\n",
        "\n",
        "def learning_rate_schedule(epoch, initial_lr=0.001):\n",
        "    \"\"\"\n",
        "    Custom learning rate schedule. Adjust the learning rate based on the current epoch.\n",
        "    You can customize this function to fit your needs.\n",
        "    \"\"\"\n",
        "    if epoch < 300 or epoch == 300:\n",
        "        return initial_lr  # Keep the initial learning rate for the first 50 epochs\n",
        "    elif epoch > 300 and epoch%50 == 0:\n",
        "        initial_lr = initial_lr * 0.1  # Reduce the learning rate by a factor of 10 after epoch 50\n",
        "        return initial_lr\n",
        "    else:\n",
        "        return initial_lr\n",
        "\n",
        "# Create the Adam optimizer with the initial learning rate\n",
        "initial_learning_rate = 0.001\n",
        "adam_optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(optimizer=adam_optimizer, loss='mean_squared_error')\n",
        "\n",
        "checkpoint_1 = ModelCheckpoint('sensors_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "model_1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[checkpoint_1, lr_scheduler])\n",
        "\n",
        "loss_1 = model_1.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss_1)\n",
        "\n",
        "predictions_1 = model_1.predict(X_test)\n",
        "\n",
        "errors_1 = np.abs(predictions_1 - y_test)\n",
        "\n",
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model('sensors_model.h5')\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "test_loss = best_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss (MSE) with Best Model: {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI7Qtiyrrokz"
      },
      "source": [
        "# Add explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MuAu9derzm6",
        "outputId": "7fe117b2-764a-491c-9637-734e3e4a792d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/275.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.5)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=7d5b7281294a87a55b21361f339c8602328b6f986c434ecca99b809e1dc2ebb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHhMvsWwr16Y"
      },
      "outputs": [],
      "source": [
        "model_1 = tf.keras.models.load_model('best_model_1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcKrZ2F6r5rX",
        "outputId": "536108b8-60a8-4c8f-d90d-492ba78c6363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 1:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 2:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 3:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 4:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 5:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 6:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 7:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 8:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 9:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 10:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 11:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 12:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 13:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 14:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 15:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 16:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 17:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 18:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 19:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 20:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 21:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 22:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 23:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 24:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 25:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 26:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 27:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 28:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 29:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 30:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 31:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 32:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 33:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 34:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 35:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 36:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 37:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 38:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 39:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 40:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 41:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 42:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 43:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 44:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 45:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 46:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 47:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 48:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 49:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 50:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 51:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 52:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 53:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 54:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 55:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 56:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 57:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 58:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 59:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 60:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 61:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 62:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 63:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 64:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 65:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 66:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 67:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 68:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 69:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 70:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 71:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 72:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 73:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 74:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 75:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 76:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 77:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 78:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 79:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 80:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 81:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 82:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 83:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 84:\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "Analysis Details for Prediction 85:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 86:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 87:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 88:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 89:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 90:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 91:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 92:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 93:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 94:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 95:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 96:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 97:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 98:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 99:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 100:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 101:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 102:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 103:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 104:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 105:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 106:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 107:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 108:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 109:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 110:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 111:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 112:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 113:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 114:\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Analysis Details for Prediction 115:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 116:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 117:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 118:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 119:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 120:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 121:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 122:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 123:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 124:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 125:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 126:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 127:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 128:\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Analysis Details for Prediction 129:\n"
          ]
        }
      ],
      "source": [
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import csv\n",
        "\n",
        "# Create a LIME explainer\n",
        "explainer = LimeTabularExplainer(X_train, mode=\"regression\", training_labels=y_train, feature_names=['s21_mag', 's21_phase', 's22_mag', 's22_phase'])\n",
        "\n",
        "test_len = len(X_test)\n",
        "\n",
        "# Create a list to store explanation details for all test instances\n",
        "explanation_details_list = []\n",
        "\n",
        "# Define the output CSV file path\n",
        "output_csv_file = 'explanation_details.csv'\n",
        "\n",
        "with open(output_csv_file, mode='w', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write the header row with feature names\n",
        "    header_row = ['Prediction','Feature', 'Weight', 'local_exp', 'intercept', 'r score']\n",
        "    csv_writer.writerow(header_row)\n",
        "\n",
        "for i in range(test_len): #test_len\n",
        "\n",
        "    #explanation_details_list = []\n",
        "\n",
        "    # Explain an individual prediction\n",
        "    explanation = explainer.explain_instance(X_test[i], model_1.predict)\n",
        "\n",
        "    # Retrieve the explanation details as a list of tuples\n",
        "    explanation_details = explanation.as_list()\n",
        "\n",
        "    # Append the explanation details to the list\n",
        "    #explanation_details_list.append(explanation_details)\n",
        "\n",
        "    # Print the explanation details (optional)\n",
        "    print(f\"Analysis Details for Prediction {i + 1}:\")\n",
        "    for feature, weight in explanation_details:\n",
        "        # print(f\"Feature: {feature}, Weight: {weight}\")\n",
        "        continue\n",
        "\n",
        "    # print(explanation.local_exp)\n",
        "    # print(explanation.intercept)\n",
        "    # R2 score\n",
        "    # print(explanation.score)\n",
        "\n",
        "    # Visualize the explanation (optional)\n",
        "    #explanation.show_in_notebook()\n",
        "\n",
        "    # Write the explanation details to a CSV file row by row\n",
        "    with open(output_csv_file, mode='a', newline='') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "\n",
        "        #csv_writer.writerow([f'Prediction {i + 1}', explanation_details])\n",
        "\n",
        "        # Write explanation details for each test instance\n",
        "        #for i, explanation_details in enumerate(explanation_details_list):\n",
        "\n",
        "        for feature, weight in explanation_details:\n",
        "             csv_writer.writerow([f'Prediction {i + 1}', feature, weight, explanation.local_exp, explanation.intercept, explanation.score])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6poFVfKer8vS"
      },
      "source": [
        "SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLpfJjPrr-7K",
        "outputId": "8ba1f34a-78e3-4217-a0b5-dafb3ee78d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.43.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (532 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/532.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/532.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.56.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->shap) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.43.0 slicer-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "gg31A0KYsBq6",
        "outputId": "ca011d2c-271d-41be-b588-975e34db9eb8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x310 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import shap\n",
        "\n",
        "# Create an explainer\n",
        "explainer = shap.DeepExplainer(model_1, X_train)\n",
        "\n",
        "# Calculate SHAP values for a single prediction\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Plot SHAP values\n",
        "shap.summary_plot(shap_values, X_test, feature_names=['s21_mag', 's21_phase', 's22_mag', 's22_phase'], plot_type=\"bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3VgGw-GsYrc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}